{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eef8220c-db27-4be4-b3db-08ead651c81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>plate</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>price_Box-Cox</th>\n",
       "      <th>price_Yeo-Johnson</th>\n",
       "      <th>price_Quantile</th>\n",
       "      <th>price_log</th>\n",
       "      <th>plate_length</th>\n",
       "      <th>region</th>\n",
       "      <th>registration_code</th>\n",
       "      <th>...</th>\n",
       "      <th>series_part_2_YC</th>\n",
       "      <th>series_part_2_YE</th>\n",
       "      <th>series_part_2_YH</th>\n",
       "      <th>series_part_2_YK</th>\n",
       "      <th>series_part_2_YM</th>\n",
       "      <th>series_part_2_YO</th>\n",
       "      <th>series_part_2_YP</th>\n",
       "      <th>series_part_2_YT</th>\n",
       "      <th>series_part_2_YX</th>\n",
       "      <th>series_part_2_YY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>X059CP797</td>\n",
       "      <td>2024-12-26 00:00:00</td>\n",
       "      <td>65000</td>\n",
       "      <td>-0.903094</td>\n",
       "      <td>-0.903096</td>\n",
       "      <td>-0.817902</td>\n",
       "      <td>11.082158</td>\n",
       "      <td>9</td>\n",
       "      <td>797</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y800MH790</td>\n",
       "      <td>2024-07-12 21:31:37</td>\n",
       "      <td>100000</td>\n",
       "      <td>-0.440378</td>\n",
       "      <td>-0.440380</td>\n",
       "      <td>-0.370902</td>\n",
       "      <td>11.512935</td>\n",
       "      <td>9</td>\n",
       "      <td>790</td>\n",
       "      <td>800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A212TX77</td>\n",
       "      <td>2024-04-18 00:00:00</td>\n",
       "      <td>290000</td>\n",
       "      <td>0.532677</td>\n",
       "      <td>0.532678</td>\n",
       "      <td>0.468203</td>\n",
       "      <td>12.577640</td>\n",
       "      <td>8</td>\n",
       "      <td>77</td>\n",
       "      <td>212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P001AY199</td>\n",
       "      <td>2025-01-03 00:27:15</td>\n",
       "      <td>680000</td>\n",
       "      <td>1.196486</td>\n",
       "      <td>1.163831</td>\n",
       "      <td>1.149742</td>\n",
       "      <td>13.429850</td>\n",
       "      <td>9</td>\n",
       "      <td>199</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B400BB750</td>\n",
       "      <td>2022-04-09 00:00:00</td>\n",
       "      <td>50000</td>\n",
       "      <td>-1.207010</td>\n",
       "      <td>-1.207011</td>\n",
       "      <td>-1.184447</td>\n",
       "      <td>10.819798</td>\n",
       "      <td>9</td>\n",
       "      <td>750</td>\n",
       "      <td>400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       plate                 date   price  price_Box-Cox  price_Yeo-Johnson  \\\n",
       "0  X059CP797  2024-12-26 00:00:00   65000      -0.903094          -0.903096   \n",
       "1  Y800MH790  2024-07-12 21:31:37  100000      -0.440378          -0.440380   \n",
       "2   A212TX77  2024-04-18 00:00:00  290000       0.532677           0.532678   \n",
       "3  P001AY199  2025-01-03 00:27:15  680000       1.196486           1.163831   \n",
       "4  B400BB750  2022-04-09 00:00:00   50000      -1.207010          -1.207011   \n",
       "\n",
       "   price_Quantile  price_log  plate_length  region  registration_code  ...  \\\n",
       "0       -0.817902  11.082158             9     797                 59  ...   \n",
       "1       -0.370902  11.512935             9     790                800  ...   \n",
       "2        0.468203  12.577640             8      77                212  ...   \n",
       "3        1.149742  13.429850             9     199                  1  ...   \n",
       "4       -1.184447  10.819798             9     750                400  ...   \n",
       "\n",
       "  series_part_2_YC series_part_2_YE series_part_2_YH series_part_2_YK  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   series_part_2_YM  series_part_2_YO  series_part_2_YP  series_part_2_YT  \\\n",
       "0               0.0               0.0               0.0               0.0   \n",
       "1               0.0               0.0               0.0               0.0   \n",
       "2               0.0               0.0               0.0               0.0   \n",
       "3               0.0               0.0               0.0               0.0   \n",
       "4               0.0               0.0               0.0               0.0   \n",
       "\n",
       "   series_part_2_YX  series_part_2_YY  \n",
       "0               0.0               0.0  \n",
       "1               0.0               0.0  \n",
       "2               0.0               0.0  \n",
       "3               0.0               0.0  \n",
       "4               0.0               0.0  \n",
       "\n",
       "[5 rows x 194 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv('feature_extracted_plate.csv')\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2275dcbb-95ed-4ca8-a8db-902063844f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_series_sorted = sorted(df['series'].unique())\n",
    "series2idx = {series: idx for idx, series in enumerate(unique_series_sorted)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36f32687-f756-46e5-bba1-3e92451cccd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1728"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_series_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f73a1941-a117-4ff0-91ae-628540e50c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['plate', 'date', 'price', 'price_Box-Cox', 'price_Yeo-Johnson',\n",
       "       'price_Quantile', 'price_log', 'plate_length', 'region',\n",
       "       'registration_code', 'series_part_1', 'series_part_2', 'series',\n",
       "       'region_name', 'region_id_normal', 'region_id_Box-Cox',\n",
       "       'region_id_Yeo-Johnson', 'region_id_Quantile', 'region_id_log',\n",
       "       'region_avg_price', 'region_avg_Box-Cox', 'region_avg_Yeo-Johnson',\n",
       "       'region_avg_Quantile', 'region_avg_log', 'digit_symmetry',\n",
       "       'registration_symmetry', 'region_symmetry', 'digits_frequency',\n",
       "       'region_frequency', 'registration_frequency', 'series_symmetry', 'year',\n",
       "       'month', 'day', 'hour', 'day_of_week', 'is_prestigious_number',\n",
       "       'is_prestigious_letter', 'series_part_1_A', 'series_part_1_B',\n",
       "       'series_part_1_C', 'series_part_1_E', 'series_part_1_H',\n",
       "       'series_part_1_K', 'series_part_1_M', 'series_part_1_O',\n",
       "       'series_part_1_P', 'series_part_1_T'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[:48]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248eda02-9b76-499b-bcff-c073a0cd0040",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "getting distribution for each of the characters for each index.\n",
    "\n",
    "the problem is with different fold i have so much variance in the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "48b51602-135f-4106-bbe1-b901ccfcb2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.axis.XTick at 0x3219c0b30>,\n",
       " <matplotlib.axis.XTick at 0x3219c08c0>,\n",
       " <matplotlib.axis.XTick at 0x3219b0260>,\n",
       " <matplotlib.axis.XTick at 0x3219c0530>,\n",
       " <matplotlib.axis.XTick at 0x321a39400>,\n",
       " <matplotlib.axis.XTick at 0x321a397c0>,\n",
       " <matplotlib.axis.XTick at 0x321a39dc0>,\n",
       " <matplotlib.axis.XTick at 0x321a3a630>,\n",
       " <matplotlib.axis.XTick at 0x321a3ad20>,\n",
       " <matplotlib.axis.XTick at 0x321a3b4a0>,\n",
       " <matplotlib.axis.XTick at 0x321a3b530>,\n",
       " <matplotlib.axis.XTick at 0x321a3a2a0>]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "series_count_1 = df['series'].str[0].value_counts(normalize=True).sort_index()\n",
    "series_count_2 = df['series'].str[1].value_counts(normalize=True).sort_index()\n",
    "series_count_3 = df['series'].str[2].value_counts(normalize=True).sort_index()\n",
    "\n",
    "mean_count_char = pd.DataFrame({1: series_count_1, 2: series_count_2, 3: series_count_3}).T.mean().to_list()\n",
    "\n",
    "x = np.arange(12)\n",
    "width = 0.3\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "\n",
    "ax.bar(x-0.3, series_count_1, width)\n",
    "ax.plot(x, mean_count_char, 'ro--')\n",
    "ax.bar(x, series_count_2, width)\n",
    "ax.bar(x+0.3, series_count_3, width)\n",
    "\n",
    "ax.set_xticks(\n",
    "    ticks=x,\n",
    "    labels=['A', 'B', 'C', 'E', 'H', 'K', 'M', 'O', 'P', 'T', 'X', 'Y']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fea51c-83ef-4d69-8689-75d5eb55f9d2",
   "metadata": {},
   "source": [
    "# Embedding layer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d4d50fed-a1b8-47c8-aa9d-aab5bf4fd992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'B': 1,\n",
       " 'C': 2,\n",
       " 'E': 3,\n",
       " 'H': 4,\n",
       " 'K': 5,\n",
       " 'M': 6,\n",
       " 'O': 7,\n",
       " 'P': 8,\n",
       " 'T': 9,\n",
       " 'X': 10,\n",
       " 'Y': 11}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_plate = ['A', 'B', 'C', 'E', 'H', 'K', 'M', 'O', 'P', 'T', 'X', 'Y']\n",
    "char2idx = {char: idx for idx, char in enumerate(char_plate)}\n",
    "char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "43bb749d-068f-4559-bc4d-6b8062d46826",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 3),\n",
    "            nn.ReLU(),  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        return self.decoder(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2a262903-22ce-41ec-9c36-c691bc78da4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class SeriesDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor([char2idx[char] for char in self.data[idx]], dtype=torch.float32)\n",
    "\n",
    "\n",
    "dataset = SeriesDataset(unique_series_sorted)\n",
    "batch_size = 128\n",
    "k_folds = 12\n",
    "\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "713a1032-e6b8-471a-843e-bd837bb19d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n",
      "Epoch [1/500] Train Loss: 40.9978, Val Loss: 42.7865\n",
      "Epoch [2/500] Train Loss: 40.9595, Val Loss: 42.7468\n",
      "Epoch [3/500] Train Loss: 40.9214, Val Loss: 42.7056\n",
      "Epoch [4/500] Train Loss: 40.8809, Val Loss: 42.6607\n",
      "Epoch [5/500] Train Loss: 40.8356, Val Loss: 42.6080\n",
      "Epoch [6/500] Train Loss: 40.7814, Val Loss: 42.5456\n",
      "Epoch [7/500] Train Loss: 40.7122, Val Loss: 42.4435\n",
      "Epoch [8/500] Train Loss: 40.5798, Val Loss: 42.2659\n",
      "Epoch [9/500] Train Loss: 40.3791, Val Loss: 41.9642\n",
      "Epoch [10/500] Train Loss: 39.9590, Val Loss: 41.2500\n",
      "Epoch [11/500] Train Loss: 38.9307, Val Loss: 39.5338\n",
      "Epoch [12/500] Train Loss: 36.6527, Val Loss: 35.8486\n",
      "Epoch [13/500] Train Loss: 32.3928, Val Loss: 30.1150\n",
      "Epoch [14/500] Train Loss: 27.6495, Val Loss: 26.1733\n",
      "Epoch [15/500] Train Loss: 25.1278, Val Loss: 23.9509\n",
      "Epoch [16/500] Train Loss: 23.1602, Val Loss: 22.4162\n",
      "Epoch [17/500] Train Loss: 21.7811, Val Loss: 21.2337\n",
      "Epoch [18/500] Train Loss: 20.8511, Val Loss: 20.5656\n",
      "Epoch [19/500] Train Loss: 20.2712, Val Loss: 20.1861\n",
      "Epoch [20/500] Train Loss: 19.9343, Val Loss: 19.9768\n",
      "Epoch [21/500] Train Loss: 19.7175, Val Loss: 19.8178\n",
      "Epoch [22/500] Train Loss: 19.5524, Val Loss: 19.6822\n",
      "Epoch [23/500] Train Loss: 19.3975, Val Loss: 19.5540\n",
      "Epoch [24/500] Train Loss: 19.2554, Val Loss: 19.4379\n",
      "Epoch [25/500] Train Loss: 19.1220, Val Loss: 19.3220\n",
      "Epoch [26/500] Train Loss: 19.0008, Val Loss: 19.2288\n",
      "Epoch [27/500] Train Loss: 18.8926, Val Loss: 19.1429\n",
      "Epoch [28/500] Train Loss: 18.7943, Val Loss: 19.0679\n",
      "Epoch [29/500] Train Loss: 18.7048, Val Loss: 19.0056\n",
      "Epoch [30/500] Train Loss: 18.6234, Val Loss: 18.9459\n",
      "Epoch [31/500] Train Loss: 18.5536, Val Loss: 18.8952\n",
      "Epoch [32/500] Train Loss: 18.4919, Val Loss: 18.8632\n",
      "Epoch [33/500] Train Loss: 18.4392, Val Loss: 18.8242\n",
      "Epoch [34/500] Train Loss: 18.3918, Val Loss: 18.7916\n",
      "Epoch [35/500] Train Loss: 18.3510, Val Loss: 18.7664\n",
      "Epoch [36/500] Train Loss: 18.3158, Val Loss: 18.7401\n",
      "Epoch [37/500] Train Loss: 18.2841, Val Loss: 18.7220\n",
      "Epoch [38/500] Train Loss: 18.2571, Val Loss: 18.7012\n",
      "Epoch [39/500] Train Loss: 18.2318, Val Loss: 18.6790\n",
      "Epoch [40/500] Train Loss: 18.2089, Val Loss: 18.6688\n",
      "Epoch [41/500] Train Loss: 18.1879, Val Loss: 18.6526\n",
      "Epoch [42/500] Train Loss: 18.1682, Val Loss: 18.6444\n",
      "Epoch [43/500] Train Loss: 18.1527, Val Loss: 18.6389\n",
      "Epoch [44/500] Train Loss: 18.1354, Val Loss: 18.6265\n",
      "Epoch [45/500] Train Loss: 18.1216, Val Loss: 18.6161\n",
      "Epoch [46/500] Train Loss: 18.1094, Val Loss: 18.6092\n",
      "Epoch [47/500] Train Loss: 18.0991, Val Loss: 18.5994\n",
      "Epoch [48/500] Train Loss: 18.0909, Val Loss: 18.6004\n",
      "Epoch [49/500] Train Loss: 18.0826, Val Loss: 18.5944\n",
      "Epoch [50/500] Train Loss: 18.0764, Val Loss: 18.5843\n",
      "Epoch [51/500] Train Loss: 18.0696, Val Loss: 18.5900\n",
      "Epoch [52/500] Train Loss: 18.0630, Val Loss: 18.5904\n",
      "Epoch [53/500] Train Loss: 18.0580, Val Loss: 18.5783\n",
      "Epoch [54/500] Train Loss: 18.0542, Val Loss: 18.5766\n",
      "Epoch [55/500] Train Loss: 18.0500, Val Loss: 18.5762\n",
      "Epoch [56/500] Train Loss: 18.0473, Val Loss: 18.5756\n",
      "Epoch [57/500] Train Loss: 18.0439, Val Loss: 18.5796\n",
      "Epoch [58/500] Train Loss: 18.0408, Val Loss: 18.5736\n",
      "Epoch [59/500] Train Loss: 18.0397, Val Loss: 18.5694\n",
      "Epoch [60/500] Train Loss: 18.0367, Val Loss: 18.5714\n",
      "Epoch [61/500] Train Loss: 18.0350, Val Loss: 18.5746\n",
      "Epoch [62/500] Train Loss: 18.0330, Val Loss: 18.5736\n",
      "Epoch [63/500] Train Loss: 18.0301, Val Loss: 18.5743\n",
      "Epoch [64/500] Train Loss: 18.0293, Val Loss: 18.5712\n",
      "Epoch [65/500] Train Loss: 18.0292, Val Loss: 18.5720\n",
      "Epoch [66/500] Train Loss: 18.0253, Val Loss: 18.5622\n",
      "Epoch [67/500] Train Loss: 18.0240, Val Loss: 18.5638\n",
      "Epoch [68/500] Train Loss: 18.0223, Val Loss: 18.5671\n",
      "Epoch [69/500] Train Loss: 18.0216, Val Loss: 18.5654\n",
      "Epoch [70/500] Train Loss: 18.0205, Val Loss: 18.5665\n",
      "Epoch [71/500] Train Loss: 18.0196, Val Loss: 18.5645\n",
      "Epoch [72/500] Train Loss: 18.0165, Val Loss: 18.5580\n",
      "Epoch [73/500] Train Loss: 18.0154, Val Loss: 18.5618\n",
      "Epoch [74/500] Train Loss: 18.0146, Val Loss: 18.5634\n",
      "Epoch [75/500] Train Loss: 18.0138, Val Loss: 18.5575\n",
      "Epoch [76/500] Train Loss: 18.0117, Val Loss: 18.5586\n",
      "Epoch [77/500] Train Loss: 18.0104, Val Loss: 18.5593\n",
      "Epoch [78/500] Train Loss: 18.0086, Val Loss: 18.5565\n",
      "Epoch [79/500] Train Loss: 18.0086, Val Loss: 18.5496\n",
      "Epoch [80/500] Train Loss: 18.0069, Val Loss: 18.5536\n",
      "Epoch [81/500] Train Loss: 18.0043, Val Loss: 18.5516\n",
      "Epoch [82/500] Train Loss: 18.0052, Val Loss: 18.5563\n",
      "Epoch [83/500] Train Loss: 18.0020, Val Loss: 18.5514\n",
      "Epoch [84/500] Train Loss: 17.9992, Val Loss: 18.5468\n",
      "Epoch [85/500] Train Loss: 17.9978, Val Loss: 18.5438\n",
      "Epoch [86/500] Train Loss: 17.9963, Val Loss: 18.5470\n",
      "Epoch [87/500] Train Loss: 17.9929, Val Loss: 18.5314\n",
      "Epoch [88/500] Train Loss: 17.9954, Val Loss: 18.5420\n",
      "Epoch [89/500] Train Loss: 17.9869, Val Loss: 18.5283\n",
      "Epoch [90/500] Train Loss: 17.9822, Val Loss: 18.5261\n",
      "Epoch [91/500] Train Loss: 17.9767, Val Loss: 18.5333\n",
      "Epoch [92/500] Train Loss: 17.9728, Val Loss: 18.5308\n",
      "Epoch [93/500] Train Loss: 17.9664, Val Loss: 18.5103\n",
      "Epoch [94/500] Train Loss: 17.9591, Val Loss: 18.5023\n",
      "Epoch [95/500] Train Loss: 17.9522, Val Loss: 18.5015\n",
      "Epoch [96/500] Train Loss: 17.9391, Val Loss: 18.4811\n",
      "Epoch [97/500] Train Loss: 17.9267, Val Loss: 18.4629\n",
      "Epoch [98/500] Train Loss: 17.9079, Val Loss: 18.4514\n",
      "Epoch [99/500] Train Loss: 17.8890, Val Loss: 18.4348\n",
      "Epoch [100/500] Train Loss: 17.8567, Val Loss: 18.3884\n",
      "Epoch [101/500] Train Loss: 17.8182, Val Loss: 18.3548\n",
      "Epoch [102/500] Train Loss: 17.7650, Val Loss: 18.2809\n",
      "Epoch [103/500] Train Loss: 17.6938, Val Loss: 18.2046\n",
      "Epoch [104/500] Train Loss: 17.5952, Val Loss: 18.0881\n",
      "Epoch [105/500] Train Loss: 17.4576, Val Loss: 17.9011\n",
      "Epoch [106/500] Train Loss: 16.8821, Val Loss: 15.5415\n",
      "Epoch [107/500] Train Loss: 11.5576, Val Loss: 8.4623\n",
      "Epoch [108/500] Train Loss: 6.4644, Val Loss: 5.2139\n",
      "Epoch [109/500] Train Loss: 4.8341, Val Loss: 4.6324\n",
      "Epoch [110/500] Train Loss: 4.5284, Val Loss: 4.5034\n",
      "Epoch [111/500] Train Loss: 4.3641, Val Loss: 4.3840\n",
      "Epoch [112/500] Train Loss: 4.2690, Val Loss: 4.3201\n",
      "Epoch [113/500] Train Loss: 4.2038, Val Loss: 4.2495\n",
      "Epoch [114/500] Train Loss: 4.1591, Val Loss: 4.2161\n",
      "Epoch [115/500] Train Loss: 4.1291, Val Loss: 4.1940\n",
      "Epoch [116/500] Train Loss: 4.1061, Val Loss: 4.1772\n",
      "Epoch [117/500] Train Loss: 4.0897, Val Loss: 4.1648\n",
      "Epoch [118/500] Train Loss: 4.0810, Val Loss: 4.1503\n",
      "Epoch [119/500] Train Loss: 4.0617, Val Loss: 4.1492\n",
      "Epoch [120/500] Train Loss: 4.0558, Val Loss: 4.1330\n",
      "Epoch [121/500] Train Loss: 4.0488, Val Loss: 4.1202\n",
      "Epoch [122/500] Train Loss: 4.0425, Val Loss: 4.1175\n",
      "Epoch [123/500] Train Loss: 4.0415, Val Loss: 4.1150\n",
      "Epoch [124/500] Train Loss: 4.0389, Val Loss: 4.1008\n",
      "Epoch [125/500] Train Loss: 4.0273, Val Loss: 4.1104\n",
      "Epoch [126/500] Train Loss: 4.0212, Val Loss: 4.0951\n",
      "Epoch [127/500] Train Loss: 4.0185, Val Loss: 4.1022\n",
      "Epoch [128/500] Train Loss: 4.0157, Val Loss: 4.0950\n",
      "Epoch [129/500] Train Loss: 4.0161, Val Loss: 4.0916\n",
      "Epoch [130/500] Train Loss: 4.0074, Val Loss: 4.0889\n",
      "Epoch [131/500] Train Loss: 4.0045, Val Loss: 4.0868\n",
      "Epoch [132/500] Train Loss: 4.0032, Val Loss: 4.0855\n",
      "Epoch [133/500] Train Loss: 4.0035, Val Loss: 4.0764\n",
      "Epoch [134/500] Train Loss: 3.9995, Val Loss: 4.0755\n",
      "Epoch [135/500] Train Loss: 3.9964, Val Loss: 4.0781\n",
      "Epoch [136/500] Train Loss: 3.9958, Val Loss: 4.0775\n",
      "Epoch [137/500] Train Loss: 3.9957, Val Loss: 4.0719\n",
      "Epoch [138/500] Train Loss: 3.9934, Val Loss: 4.0762\n",
      "Epoch [139/500] Train Loss: 3.9905, Val Loss: 4.0667\n",
      "Epoch [140/500] Train Loss: 3.9951, Val Loss: 4.0801\n",
      "Epoch [141/500] Train Loss: 3.9867, Val Loss: 4.0678\n",
      "Epoch [142/500] Train Loss: 3.9862, Val Loss: 4.0695\n",
      "Epoch [143/500] Train Loss: 3.9856, Val Loss: 4.0676\n",
      "Epoch [144/500] Train Loss: 3.9840, Val Loss: 4.0720\n",
      "Epoch [145/500] Train Loss: 3.9820, Val Loss: 4.0656\n",
      "Epoch [146/500] Train Loss: 3.9804, Val Loss: 4.0704\n",
      "Epoch [147/500] Train Loss: 3.9799, Val Loss: 4.0652\n",
      "Epoch [148/500] Train Loss: 3.9786, Val Loss: 4.0635\n",
      "Epoch [149/500] Train Loss: 3.9778, Val Loss: 4.0636\n",
      "Epoch [150/500] Train Loss: 3.9755, Val Loss: 4.0643\n",
      "Epoch [151/500] Train Loss: 3.9765, Val Loss: 4.0610\n",
      "Epoch [152/500] Train Loss: 3.9737, Val Loss: 4.0666\n",
      "Epoch [153/500] Train Loss: 3.9733, Val Loss: 4.0621\n",
      "Epoch [154/500] Train Loss: 3.9761, Val Loss: 4.0588\n",
      "Epoch [155/500] Train Loss: 3.9733, Val Loss: 4.0704\n",
      "Epoch [156/500] Train Loss: 3.9719, Val Loss: 4.0592\n",
      "Epoch [157/500] Train Loss: 3.9701, Val Loss: 4.0587\n",
      "Epoch [158/500] Train Loss: 3.9698, Val Loss: 4.0543\n",
      "Epoch [159/500] Train Loss: 3.9684, Val Loss: 4.0563\n",
      "Epoch [160/500] Train Loss: 3.9733, Val Loss: 4.0563\n",
      "Epoch [161/500] Train Loss: 3.9646, Val Loss: 4.0618\n",
      "Epoch [162/500] Train Loss: 3.9650, Val Loss: 4.0599\n",
      "Epoch [163/500] Train Loss: 3.9645, Val Loss: 4.0593\n",
      "Epoch [164/500] Train Loss: 3.9689, Val Loss: 4.0558\n",
      "Epoch [165/500] Train Loss: 3.9651, Val Loss: 4.0574\n",
      "Epoch [166/500] Train Loss: 3.9616, Val Loss: 4.0544\n",
      "Epoch [167/500] Train Loss: 3.9621, Val Loss: 4.0552\n",
      "Epoch [168/500] Train Loss: 3.9618, Val Loss: 4.0565\n",
      "Epoch [169/500] Train Loss: 3.9600, Val Loss: 4.0493\n",
      "Epoch [170/500] Train Loss: 3.9644, Val Loss: 4.0510\n",
      "Epoch [171/500] Train Loss: 3.9571, Val Loss: 4.0629\n",
      "Epoch [172/500] Train Loss: 3.9579, Val Loss: 4.0527\n",
      "Epoch [173/500] Train Loss: 3.9581, Val Loss: 4.0485\n",
      "Epoch [174/500] Train Loss: 3.9572, Val Loss: 4.0506\n",
      "Epoch [175/500] Train Loss: 3.9545, Val Loss: 4.0505\n",
      "Epoch [176/500] Train Loss: 3.9552, Val Loss: 4.0529\n",
      "Epoch [177/500] Train Loss: 3.9556, Val Loss: 4.0466\n",
      "Epoch [178/500] Train Loss: 3.9540, Val Loss: 4.0533\n",
      "Epoch [179/500] Train Loss: 3.9578, Val Loss: 4.0468\n",
      "Epoch [180/500] Train Loss: 3.9629, Val Loss: 4.0627\n",
      "Epoch [181/500] Train Loss: 3.9534, Val Loss: 4.0466\n",
      "Epoch [182/500] Train Loss: 3.9520, Val Loss: 4.0575\n",
      "Epoch [183/500] Train Loss: 3.9531, Val Loss: 4.0538\n",
      "Epoch [184/500] Train Loss: 3.9522, Val Loss: 4.0427\n",
      "Epoch [185/500] Train Loss: 3.9489, Val Loss: 4.0507\n",
      "Epoch [186/500] Train Loss: 3.9484, Val Loss: 4.0472\n",
      "Epoch [187/500] Train Loss: 3.9471, Val Loss: 4.0568\n",
      "Epoch [188/500] Train Loss: 3.9488, Val Loss: 4.0506\n",
      "Epoch [189/500] Train Loss: 3.9466, Val Loss: 4.0474\n",
      "Epoch [190/500] Train Loss: 3.9455, Val Loss: 4.0478\n",
      "Epoch [191/500] Train Loss: 3.9472, Val Loss: 4.0402\n",
      "Epoch [192/500] Train Loss: 3.9484, Val Loss: 4.0511\n",
      "Epoch [193/500] Train Loss: 3.9443, Val Loss: 4.0411\n",
      "Epoch [194/500] Train Loss: 3.9442, Val Loss: 4.0417\n",
      "Epoch [195/500] Train Loss: 3.9425, Val Loss: 4.0509\n",
      "Epoch [196/500] Train Loss: 3.9427, Val Loss: 4.0488\n",
      "Epoch [197/500] Train Loss: 3.9429, Val Loss: 4.0407\n",
      "Epoch [198/500] Train Loss: 3.9420, Val Loss: 4.0454\n",
      "Epoch [199/500] Train Loss: 3.9403, Val Loss: 4.0471\n",
      "Epoch [200/500] Train Loss: 3.9442, Val Loss: 4.0417\n",
      "Epoch [201/500] Train Loss: 3.9394, Val Loss: 4.0501\n",
      "Epoch [202/500] Train Loss: 3.9399, Val Loss: 4.0461\n",
      "Epoch [203/500] Train Loss: 3.9365, Val Loss: 4.0373\n",
      "Epoch [204/500] Train Loss: 3.9362, Val Loss: 4.0442\n",
      "Epoch [205/500] Train Loss: 3.9361, Val Loss: 4.0407\n",
      "Epoch [206/500] Train Loss: 3.9458, Val Loss: 4.0315\n",
      "Epoch [207/500] Train Loss: 3.9546, Val Loss: 4.0503\n",
      "Epoch [208/500] Train Loss: 3.9325, Val Loss: 4.0315\n",
      "Epoch [209/500] Train Loss: 3.9372, Val Loss: 4.0332\n",
      "Epoch [210/500] Train Loss: 3.9331, Val Loss: 4.0372\n",
      "Epoch [211/500] Train Loss: 3.9388, Val Loss: 4.0424\n",
      "Epoch [212/500] Train Loss: 3.9345, Val Loss: 4.0374\n",
      "Epoch [213/500] Train Loss: 3.9310, Val Loss: 4.0332\n",
      "Epoch [214/500] Train Loss: 3.9297, Val Loss: 4.0435\n",
      "Epoch [215/500] Train Loss: 3.9297, Val Loss: 4.0417\n",
      "Epoch [216/500] Train Loss: 3.9288, Val Loss: 4.0377\n",
      "Epoch [217/500] Train Loss: 3.9272, Val Loss: 4.0351\n",
      "Epoch [218/500] Train Loss: 3.9288, Val Loss: 4.0403\n",
      "Epoch [219/500] Train Loss: 3.9268, Val Loss: 4.0409\n",
      "Epoch [220/500] Train Loss: 3.9280, Val Loss: 4.0429\n",
      "Epoch [221/500] Train Loss: 3.9298, Val Loss: 4.0341\n",
      "Epoch [222/500] Train Loss: 3.9253, Val Loss: 4.0395\n",
      "Epoch [223/500] Train Loss: 3.9240, Val Loss: 4.0338\n",
      "Epoch [224/500] Train Loss: 3.9234, Val Loss: 4.0360\n",
      "Epoch [225/500] Train Loss: 3.9215, Val Loss: 4.0327\n",
      "Epoch [226/500] Train Loss: 3.9222, Val Loss: 4.0323\n",
      "Epoch [227/500] Train Loss: 3.9249, Val Loss: 4.0503\n",
      "Epoch [228/500] Train Loss: 3.9242, Val Loss: 4.0301\n",
      "Epoch [229/500] Train Loss: 3.9242, Val Loss: 4.0396\n",
      "Epoch [230/500] Train Loss: 3.9206, Val Loss: 4.0307\n",
      "Epoch [231/500] Train Loss: 3.9173, Val Loss: 4.0321\n",
      "Epoch [232/500] Train Loss: 3.9179, Val Loss: 4.0339\n",
      "Epoch [233/500] Train Loss: 3.9165, Val Loss: 4.0323\n",
      "Epoch [234/500] Train Loss: 3.9199, Val Loss: 4.0388\n",
      "Epoch [235/500] Train Loss: 3.9131, Val Loss: 4.0237\n",
      "Epoch [236/500] Train Loss: 3.9134, Val Loss: 4.0266\n",
      "Epoch [237/500] Train Loss: 3.9116, Val Loss: 4.0346\n",
      "Epoch [238/500] Train Loss: 3.9145, Val Loss: 4.0321\n",
      "Epoch [239/500] Train Loss: 3.9107, Val Loss: 4.0283\n",
      "Epoch [240/500] Train Loss: 3.9077, Val Loss: 4.0292\n",
      "Epoch [241/500] Train Loss: 3.9088, Val Loss: 4.0308\n",
      "Epoch [242/500] Train Loss: 3.9056, Val Loss: 4.0251\n",
      "Epoch [243/500] Train Loss: 3.9075, Val Loss: 4.0238\n",
      "Epoch [244/500] Train Loss: 3.9033, Val Loss: 4.0398\n",
      "Epoch [245/500] Train Loss: 3.9038, Val Loss: 4.0273\n",
      "Epoch [246/500] Train Loss: 3.9050, Val Loss: 4.0275\n",
      "Epoch [247/500] Train Loss: 3.9044, Val Loss: 4.0232\n",
      "Epoch [248/500] Train Loss: 3.9048, Val Loss: 4.0325\n",
      "Epoch [249/500] Train Loss: 3.8987, Val Loss: 4.0224\n",
      "Epoch [250/500] Train Loss: 3.8984, Val Loss: 4.0172\n",
      "Epoch [251/500] Train Loss: 3.8958, Val Loss: 4.0274\n",
      "Epoch [252/500] Train Loss: 3.8968, Val Loss: 4.0223\n",
      "Epoch [253/500] Train Loss: 3.8954, Val Loss: 4.0166\n",
      "Epoch [254/500] Train Loss: 3.8970, Val Loss: 4.0237\n",
      "Epoch [255/500] Train Loss: 3.8970, Val Loss: 4.0232\n",
      "Epoch [256/500] Train Loss: 3.8924, Val Loss: 4.0137\n",
      "Epoch [257/500] Train Loss: 3.8884, Val Loss: 4.0328\n",
      "Epoch [258/500] Train Loss: 3.8925, Val Loss: 4.0227\n",
      "Epoch [259/500] Train Loss: 3.8892, Val Loss: 4.0273\n",
      "Epoch [260/500] Train Loss: 3.8931, Val Loss: 4.0153\n",
      "Epoch [261/500] Train Loss: 3.8845, Val Loss: 4.0337\n",
      "Epoch [262/500] Train Loss: 3.8837, Val Loss: 4.0184\n",
      "Epoch [263/500] Train Loss: 3.8850, Val Loss: 4.0183\n",
      "Epoch [264/500] Train Loss: 3.8847, Val Loss: 4.0327\n",
      "Epoch [265/500] Train Loss: 3.8817, Val Loss: 4.0239\n",
      "Epoch [266/500] Train Loss: 3.8931, Val Loss: 4.0136\n",
      "Epoch [267/500] Train Loss: 3.8812, Val Loss: 4.0284\n",
      "Epoch [268/500] Train Loss: 3.8780, Val Loss: 4.0251\n",
      "Epoch [269/500] Train Loss: 3.8803, Val Loss: 4.0153\n",
      "Epoch [270/500] Train Loss: 3.8771, Val Loss: 4.0204\n",
      "Epoch [271/500] Train Loss: 3.8730, Val Loss: 4.0066\n",
      "Epoch [272/500] Train Loss: 3.8727, Val Loss: 4.0157\n",
      "Epoch [273/500] Train Loss: 3.8708, Val Loss: 4.0244\n",
      "Epoch [274/500] Train Loss: 3.8686, Val Loss: 4.0113\n",
      "Epoch [275/500] Train Loss: 3.8719, Val Loss: 4.0188\n",
      "Epoch [276/500] Train Loss: 3.8676, Val Loss: 4.0126\n",
      "Epoch [277/500] Train Loss: 3.8676, Val Loss: 4.0097\n",
      "Epoch [278/500] Train Loss: 3.8668, Val Loss: 4.0167\n",
      "Epoch [279/500] Train Loss: 3.8633, Val Loss: 4.0036\n",
      "Epoch [280/500] Train Loss: 3.8590, Val Loss: 4.0172\n",
      "Epoch [281/500] Train Loss: 3.8618, Val Loss: 4.0190\n",
      "Epoch [282/500] Train Loss: 3.8626, Val Loss: 4.0094\n",
      "Epoch [283/500] Train Loss: 3.8575, Val Loss: 4.0119\n",
      "Epoch [284/500] Train Loss: 3.8563, Val Loss: 4.0092\n",
      "Epoch [285/500] Train Loss: 3.8560, Val Loss: 4.0143\n",
      "Epoch [286/500] Train Loss: 3.8598, Val Loss: 4.0074\n",
      "Epoch [287/500] Train Loss: 3.8542, Val Loss: 4.0063\n",
      "Epoch [288/500] Train Loss: 3.8522, Val Loss: 4.0136\n",
      "Epoch [289/500] Train Loss: 3.8495, Val Loss: 4.0036\n",
      "Epoch [290/500] Train Loss: 3.8483, Val Loss: 3.9996\n",
      "Epoch [291/500] Train Loss: 3.8507, Val Loss: 4.0213\n",
      "Epoch [292/500] Train Loss: 3.8491, Val Loss: 4.0033\n",
      "Epoch [293/500] Train Loss: 3.8453, Val Loss: 4.0183\n",
      "Epoch [294/500] Train Loss: 3.8539, Val Loss: 3.9970\n",
      "Epoch [295/500] Train Loss: 3.8454, Val Loss: 4.0178\n",
      "Epoch [296/500] Train Loss: 3.8462, Val Loss: 4.0041\n",
      "Epoch [297/500] Train Loss: 3.8423, Val Loss: 4.0243\n",
      "Epoch [298/500] Train Loss: 3.8517, Val Loss: 3.9988\n",
      "Epoch [299/500] Train Loss: 3.8507, Val Loss: 4.0082\n",
      "Epoch [300/500] Train Loss: 3.8385, Val Loss: 3.9978\n",
      "Epoch [301/500] Train Loss: 3.8319, Val Loss: 4.0123\n",
      "Epoch [302/500] Train Loss: 3.8335, Val Loss: 4.0172\n",
      "Epoch [303/500] Train Loss: 3.8298, Val Loss: 3.9934\n",
      "Epoch [304/500] Train Loss: 3.8275, Val Loss: 4.0098\n",
      "Epoch [305/500] Train Loss: 3.8320, Val Loss: 4.0003\n",
      "Epoch [306/500] Train Loss: 3.8283, Val Loss: 3.9970\n",
      "Epoch [307/500] Train Loss: 3.8275, Val Loss: 4.0085\n",
      "Epoch [308/500] Train Loss: 3.8259, Val Loss: 3.9882\n",
      "Epoch [309/500] Train Loss: 3.8194, Val Loss: 4.0011\n",
      "Epoch [310/500] Train Loss: 3.8197, Val Loss: 4.0013\n",
      "Epoch [311/500] Train Loss: 3.8205, Val Loss: 3.9982\n",
      "Epoch [312/500] Train Loss: 3.8198, Val Loss: 4.0090\n",
      "Epoch [313/500] Train Loss: 3.8134, Val Loss: 3.9890\n",
      "Epoch [314/500] Train Loss: 3.8153, Val Loss: 3.9888\n",
      "Epoch [315/500] Train Loss: 3.8081, Val Loss: 3.9933\n",
      "Epoch [316/500] Train Loss: 3.8085, Val Loss: 3.9924\n",
      "Epoch [317/500] Train Loss: 3.8056, Val Loss: 3.9951\n",
      "Epoch [318/500] Train Loss: 3.8045, Val Loss: 3.9801\n",
      "Epoch [319/500] Train Loss: 3.8061, Val Loss: 3.9913\n",
      "Epoch [320/500] Train Loss: 3.8021, Val Loss: 3.9756\n",
      "Epoch [321/500] Train Loss: 3.7970, Val Loss: 3.9894\n",
      "Epoch [322/500] Train Loss: 3.7976, Val Loss: 3.9750\n",
      "Epoch [323/500] Train Loss: 3.7892, Val Loss: 3.9910\n",
      "Epoch [324/500] Train Loss: 3.7996, Val Loss: 3.9759\n",
      "Epoch [325/500] Train Loss: 3.7985, Val Loss: 3.9724\n",
      "Epoch [326/500] Train Loss: 3.8222, Val Loss: 3.9905\n",
      "Epoch [327/500] Train Loss: 3.7939, Val Loss: 3.9781\n",
      "Epoch [328/500] Train Loss: 3.7925, Val Loss: 3.9867\n",
      "Epoch [329/500] Train Loss: 3.7884, Val Loss: 3.9714\n",
      "Epoch [330/500] Train Loss: 3.7799, Val Loss: 3.9729\n",
      "Epoch [331/500] Train Loss: 3.7785, Val Loss: 3.9798\n",
      "Epoch [332/500] Train Loss: 3.7769, Val Loss: 3.9589\n",
      "Epoch [333/500] Train Loss: 3.7844, Val Loss: 3.9732\n",
      "Epoch [334/500] Train Loss: 3.7713, Val Loss: 3.9708\n",
      "Epoch [335/500] Train Loss: 3.7685, Val Loss: 3.9704\n",
      "Epoch [336/500] Train Loss: 3.7720, Val Loss: 3.9600\n",
      "Epoch [337/500] Train Loss: 3.7652, Val Loss: 3.9718\n",
      "Epoch [338/500] Train Loss: 3.7615, Val Loss: 3.9595\n",
      "Epoch [339/500] Train Loss: 3.7618, Val Loss: 3.9670\n",
      "Epoch [340/500] Train Loss: 3.7575, Val Loss: 3.9653\n",
      "Epoch [341/500] Train Loss: 3.7517, Val Loss: 3.9574\n",
      "Epoch [342/500] Train Loss: 3.7529, Val Loss: 3.9499\n",
      "Epoch [343/500] Train Loss: 3.7524, Val Loss: 3.9603\n",
      "Epoch [344/500] Train Loss: 3.7481, Val Loss: 3.9472\n",
      "Epoch [345/500] Train Loss: 3.7426, Val Loss: 3.9479\n",
      "Epoch [346/500] Train Loss: 3.7437, Val Loss: 3.9622\n",
      "Epoch [347/500] Train Loss: 3.7375, Val Loss: 3.9412\n",
      "Epoch [348/500] Train Loss: 3.7364, Val Loss: 3.9407\n",
      "Epoch [349/500] Train Loss: 3.7454, Val Loss: 3.9529\n",
      "Epoch [350/500] Train Loss: 3.7292, Val Loss: 3.9487\n",
      "Epoch [351/500] Train Loss: 3.7332, Val Loss: 3.9340\n",
      "Epoch [352/500] Train Loss: 3.7244, Val Loss: 3.9416\n",
      "Epoch [353/500] Train Loss: 3.7176, Val Loss: 3.9277\n",
      "Epoch [354/500] Train Loss: 3.7230, Val Loss: 3.9162\n",
      "Epoch [355/500] Train Loss: 3.7201, Val Loss: 3.9389\n",
      "Epoch [356/500] Train Loss: 3.7114, Val Loss: 3.9343\n",
      "Epoch [357/500] Train Loss: 3.7103, Val Loss: 3.9240\n",
      "Epoch [358/500] Train Loss: 3.7028, Val Loss: 3.9179\n",
      "Epoch [359/500] Train Loss: 3.7008, Val Loss: 3.9423\n",
      "Epoch [360/500] Train Loss: 3.6979, Val Loss: 3.9238\n",
      "Epoch [361/500] Train Loss: 3.6943, Val Loss: 3.9139\n",
      "Epoch [362/500] Train Loss: 3.6897, Val Loss: 3.8974\n",
      "Epoch [363/500] Train Loss: 3.6884, Val Loss: 3.9238\n",
      "Epoch [364/500] Train Loss: 3.6774, Val Loss: 3.8939\n",
      "Epoch [365/500] Train Loss: 3.6762, Val Loss: 3.8945\n",
      "Epoch [366/500] Train Loss: 3.6716, Val Loss: 3.9095\n",
      "Epoch [367/500] Train Loss: 3.6733, Val Loss: 3.9041\n",
      "Epoch [368/500] Train Loss: 3.6642, Val Loss: 3.8839\n",
      "Epoch [369/500] Train Loss: 3.6660, Val Loss: 3.9007\n",
      "Epoch [370/500] Train Loss: 3.6547, Val Loss: 3.8782\n",
      "Epoch [371/500] Train Loss: 3.6530, Val Loss: 3.8791\n",
      "Epoch [372/500] Train Loss: 3.6734, Val Loss: 3.8854\n",
      "Epoch [373/500] Train Loss: 3.6558, Val Loss: 3.8724\n",
      "Epoch [374/500] Train Loss: 3.6547, Val Loss: 3.8714\n",
      "Epoch [375/500] Train Loss: 3.6358, Val Loss: 3.9025\n",
      "Epoch [376/500] Train Loss: 3.6354, Val Loss: 3.8585\n",
      "Epoch [377/500] Train Loss: 3.6203, Val Loss: 3.8626\n",
      "Epoch [378/500] Train Loss: 3.6159, Val Loss: 3.8538\n",
      "Epoch [379/500] Train Loss: 3.6084, Val Loss: 3.8581\n",
      "Epoch [380/500] Train Loss: 3.6077, Val Loss: 3.8326\n",
      "Epoch [381/500] Train Loss: 3.5986, Val Loss: 3.8493\n",
      "Epoch [382/500] Train Loss: 3.6090, Val Loss: 3.8440\n",
      "Epoch [383/500] Train Loss: 3.6060, Val Loss: 3.8605\n",
      "Epoch [384/500] Train Loss: 3.5899, Val Loss: 3.8430\n",
      "Epoch [385/500] Train Loss: 3.5773, Val Loss: 3.8218\n",
      "Epoch [386/500] Train Loss: 3.5713, Val Loss: 3.8051\n",
      "Epoch [387/500] Train Loss: 3.5695, Val Loss: 3.7843\n",
      "Epoch [388/500] Train Loss: 3.5514, Val Loss: 3.7829\n",
      "Epoch [389/500] Train Loss: 3.5458, Val Loss: 3.7879\n",
      "Epoch [390/500] Train Loss: 3.5428, Val Loss: 3.7655\n",
      "Epoch [391/500] Train Loss: 3.5301, Val Loss: 3.7591\n",
      "Epoch [392/500] Train Loss: 3.5183, Val Loss: 3.7398\n",
      "Epoch [393/500] Train Loss: 3.5113, Val Loss: 3.7533\n",
      "Epoch [394/500] Train Loss: 3.5004, Val Loss: 3.7358\n",
      "Epoch [395/500] Train Loss: 3.4884, Val Loss: 3.7110\n",
      "Epoch [396/500] Train Loss: 3.4800, Val Loss: 3.7220\n",
      "Epoch [397/500] Train Loss: 3.4791, Val Loss: 3.6846\n",
      "Epoch [398/500] Train Loss: 3.4624, Val Loss: 3.6863\n",
      "Epoch [399/500] Train Loss: 3.4488, Val Loss: 3.6985\n",
      "Epoch [400/500] Train Loss: 3.4341, Val Loss: 3.6498\n",
      "Epoch [401/500] Train Loss: 3.4244, Val Loss: 3.6574\n",
      "Epoch [402/500] Train Loss: 3.4125, Val Loss: 3.6307\n",
      "Epoch [403/500] Train Loss: 3.3969, Val Loss: 3.6073\n",
      "Epoch [404/500] Train Loss: 3.3828, Val Loss: 3.5804\n",
      "Epoch [405/500] Train Loss: 3.3654, Val Loss: 3.5865\n",
      "Epoch [406/500] Train Loss: 3.3533, Val Loss: 3.5492\n",
      "Epoch [407/500] Train Loss: 3.3355, Val Loss: 3.5211\n",
      "Epoch [408/500] Train Loss: 3.3256, Val Loss: 3.5328\n",
      "Epoch [409/500] Train Loss: 3.3228, Val Loss: 3.4811\n",
      "Epoch [410/500] Train Loss: 3.3023, Val Loss: 3.4392\n",
      "Epoch [411/500] Train Loss: 3.2783, Val Loss: 3.4752\n",
      "Epoch [412/500] Train Loss: 3.2565, Val Loss: 3.4069\n",
      "Epoch [413/500] Train Loss: 3.2434, Val Loss: 3.4334\n",
      "Epoch [414/500] Train Loss: 3.2329, Val Loss: 3.3665\n",
      "Epoch [415/500] Train Loss: 3.2131, Val Loss: 3.3305\n",
      "Epoch [416/500] Train Loss: 3.1917, Val Loss: 3.3184\n",
      "Epoch [417/500] Train Loss: 3.1581, Val Loss: 3.2884\n",
      "Epoch [418/500] Train Loss: 3.1279, Val Loss: 3.3195\n",
      "Epoch [419/500] Train Loss: 3.1091, Val Loss: 3.1899\n",
      "Epoch [420/500] Train Loss: 3.0827, Val Loss: 3.1850\n",
      "Epoch [421/500] Train Loss: 3.0463, Val Loss: 3.1475\n",
      "Epoch [422/500] Train Loss: 3.0029, Val Loss: 3.1715\n",
      "Epoch [423/500] Train Loss: 2.9961, Val Loss: 3.0573\n",
      "Epoch [424/500] Train Loss: 2.9264, Val Loss: 2.9878\n",
      "Epoch [425/500] Train Loss: 2.8723, Val Loss: 2.9316\n",
      "Epoch [426/500] Train Loss: 2.8059, Val Loss: 2.8320\n",
      "Epoch [427/500] Train Loss: 2.7292, Val Loss: 2.7371\n",
      "Epoch [428/500] Train Loss: 2.6599, Val Loss: 2.6091\n",
      "Epoch [429/500] Train Loss: 2.5561, Val Loss: 2.4905\n",
      "Epoch [430/500] Train Loss: 2.4221, Val Loss: 2.3547\n",
      "Epoch [431/500] Train Loss: 2.2655, Val Loss: 2.1253\n",
      "Epoch [432/500] Train Loss: 2.0493, Val Loss: 1.8709\n",
      "Epoch [433/500] Train Loss: 1.7824, Val Loss: 1.5426\n",
      "Epoch [434/500] Train Loss: 1.4497, Val Loss: 1.1563\n",
      "Epoch [435/500] Train Loss: 1.1034, Val Loss: 0.8029\n",
      "Epoch [436/500] Train Loss: 0.7745, Val Loss: 0.5211\n",
      "Epoch [437/500] Train Loss: 0.5377, Val Loss: 0.3511\n",
      "Epoch [438/500] Train Loss: 0.4490, Val Loss: 0.2934\n",
      "Epoch [439/500] Train Loss: 0.3887, Val Loss: 0.2556\n",
      "Epoch [440/500] Train Loss: 0.3377, Val Loss: 0.2354\n",
      "Epoch [441/500] Train Loss: 0.3084, Val Loss: 0.2072\n",
      "Epoch [442/500] Train Loss: 0.2806, Val Loss: 0.1805\n",
      "Epoch [443/500] Train Loss: 0.2592, Val Loss: 0.1615\n",
      "Epoch [444/500] Train Loss: 0.2418, Val Loss: 0.1461\n",
      "Epoch [445/500] Train Loss: 0.2247, Val Loss: 0.1366\n",
      "Epoch [446/500] Train Loss: 0.2136, Val Loss: 0.1243\n",
      "Epoch [447/500] Train Loss: 0.1978, Val Loss: 0.1206\n",
      "Epoch [448/500] Train Loss: 0.1883, Val Loss: 0.1149\n",
      "Epoch [449/500] Train Loss: 0.1861, Val Loss: 0.1166\n",
      "Epoch [450/500] Train Loss: 0.1728, Val Loss: 0.0977\n",
      "Epoch [451/500] Train Loss: 0.1608, Val Loss: 0.0920\n",
      "Epoch [452/500] Train Loss: 0.1497, Val Loss: 0.0851\n",
      "Epoch [453/500] Train Loss: 0.1432, Val Loss: 0.0813\n",
      "Epoch [454/500] Train Loss: 0.1371, Val Loss: 0.0951\n",
      "Epoch [455/500] Train Loss: 0.1334, Val Loss: 0.0801\n",
      "Epoch [456/500] Train Loss: 0.1291, Val Loss: 0.0696\n",
      "Epoch [457/500] Train Loss: 0.1244, Val Loss: 0.0725\n",
      "Epoch [458/500] Train Loss: 0.1168, Val Loss: 0.0652\n",
      "Epoch [459/500] Train Loss: 0.1098, Val Loss: 0.0639\n",
      "Epoch [460/500] Train Loss: 0.1038, Val Loss: 0.0601\n",
      "Epoch [461/500] Train Loss: 0.1004, Val Loss: 0.0675\n",
      "Epoch [462/500] Train Loss: 0.0989, Val Loss: 0.0570\n",
      "Epoch [463/500] Train Loss: 0.0924, Val Loss: 0.0573\n",
      "Epoch [464/500] Train Loss: 0.0899, Val Loss: 0.0544\n",
      "Epoch [465/500] Train Loss: 0.0844, Val Loss: 0.0519\n",
      "Epoch [466/500] Train Loss: 0.0810, Val Loss: 0.0521\n",
      "Epoch [467/500] Train Loss: 0.0773, Val Loss: 0.0479\n",
      "Epoch [468/500] Train Loss: 0.0760, Val Loss: 0.0531\n",
      "Epoch [469/500] Train Loss: 0.0739, Val Loss: 0.0465\n",
      "Epoch [470/500] Train Loss: 0.0702, Val Loss: 0.0449\n",
      "Epoch [471/500] Train Loss: 0.0663, Val Loss: 0.0448\n",
      "Epoch [472/500] Train Loss: 0.0644, Val Loss: 0.0444\n",
      "Epoch [473/500] Train Loss: 0.0621, Val Loss: 0.0426\n",
      "Epoch [474/500] Train Loss: 0.0611, Val Loss: 0.0465\n",
      "Epoch [475/500] Train Loss: 0.0583, Val Loss: 0.0427\n",
      "Epoch [476/500] Train Loss: 0.0565, Val Loss: 0.0447\n",
      "Epoch [477/500] Train Loss: 0.0544, Val Loss: 0.0432\n",
      "Epoch [478/500] Train Loss: 0.0520, Val Loss: 0.0401\n",
      "Epoch [479/500] Train Loss: 0.0501, Val Loss: 0.0382\n",
      "Epoch [480/500] Train Loss: 0.0489, Val Loss: 0.0364\n",
      "Epoch [481/500] Train Loss: 0.0495, Val Loss: 0.0421\n",
      "Epoch [482/500] Train Loss: 0.0463, Val Loss: 0.0365\n",
      "Epoch [483/500] Train Loss: 0.0461, Val Loss: 0.0530\n",
      "Epoch [484/500] Train Loss: 0.0454, Val Loss: 0.0355\n",
      "Epoch [485/500] Train Loss: 0.0418, Val Loss: 0.0373\n",
      "Epoch [486/500] Train Loss: 0.0409, Val Loss: 0.0339\n",
      "Epoch [487/500] Train Loss: 0.0383, Val Loss: 0.0366\n",
      "Epoch [488/500] Train Loss: 0.0381, Val Loss: 0.0330\n",
      "Epoch [489/500] Train Loss: 0.0367, Val Loss: 0.0328\n",
      "Epoch [490/500] Train Loss: 0.0357, Val Loss: 0.0339\n",
      "Epoch [491/500] Train Loss: 0.0367, Val Loss: 0.0317\n",
      "Epoch [492/500] Train Loss: 0.0369, Val Loss: 0.0324\n",
      "Epoch [493/500] Train Loss: 0.0350, Val Loss: 0.0338\n",
      "Epoch [494/500] Train Loss: 0.0339, Val Loss: 0.0298\n",
      "Epoch [495/500] Train Loss: 0.0323, Val Loss: 0.0307\n",
      "Epoch [496/500] Train Loss: 0.0317, Val Loss: 0.0302\n",
      "Epoch [497/500] Train Loss: 0.0314, Val Loss: 0.0339\n",
      "Epoch [498/500] Train Loss: 0.0314, Val Loss: 0.0293\n",
      "Epoch [499/500] Train Loss: 0.0310, Val Loss: 0.0303\n",
      "Epoch [500/500] Train Loss: 0.0305, Val Loss: 0.0286\n",
      "\n",
      "=== Fold 2 ===\n",
      "Epoch [1/500] Train Loss: 40.2434, Val Loss: 38.1571\n",
      "Epoch [2/500] Train Loss: 40.1963, Val Loss: 38.1101\n",
      "Epoch [3/500] Train Loss: 40.1471, Val Loss: 38.0606\n",
      "Epoch [4/500] Train Loss: 40.0966, Val Loss: 38.0110\n",
      "Epoch [5/500] Train Loss: 40.0427, Val Loss: 37.9512\n",
      "Epoch [6/500] Train Loss: 39.9708, Val Loss: 37.8640\n",
      "Epoch [7/500] Train Loss: 39.8654, Val Loss: 37.7326\n",
      "Epoch [8/500] Train Loss: 39.6871, Val Loss: 37.4863\n",
      "Epoch [9/500] Train Loss: 39.3273, Val Loss: 36.9703\n",
      "Epoch [10/500] Train Loss: 38.6770, Val Loss: 36.1543\n",
      "Epoch [11/500] Train Loss: 37.5326, Val Loss: 34.4848\n",
      "Epoch [12/500] Train Loss: 35.0865, Val Loss: 30.9164\n",
      "Epoch [13/500] Train Loss: 30.0966, Val Loss: 24.8947\n",
      "Epoch [14/500] Train Loss: 24.3148, Val Loss: 21.0261\n",
      "Epoch [15/500] Train Loss: 22.6544, Val Loss: 20.8324\n",
      "Epoch [16/500] Train Loss: 22.2604, Val Loss: 20.5012\n",
      "Epoch [17/500] Train Loss: 22.0042, Val Loss: 20.2577\n",
      "Epoch [18/500] Train Loss: 21.7945, Val Loss: 20.0666\n",
      "Epoch [19/500] Train Loss: 21.6278, Val Loss: 19.9018\n",
      "Epoch [20/500] Train Loss: 21.4801, Val Loss: 19.7530\n",
      "Epoch [21/500] Train Loss: 21.3432, Val Loss: 19.6186\n",
      "Epoch [22/500] Train Loss: 21.2196, Val Loss: 19.4928\n",
      "Epoch [23/500] Train Loss: 21.1058, Val Loss: 19.3821\n",
      "Epoch [24/500] Train Loss: 20.9979, Val Loss: 19.2688\n",
      "Epoch [25/500] Train Loss: 20.8985, Val Loss: 19.1663\n",
      "Epoch [26/500] Train Loss: 20.8035, Val Loss: 19.0752\n",
      "Epoch [27/500] Train Loss: 20.7163, Val Loss: 18.9841\n",
      "Epoch [28/500] Train Loss: 20.6284, Val Loss: 18.8933\n",
      "Epoch [29/500] Train Loss: 20.5520, Val Loss: 18.8146\n",
      "Epoch [30/500] Train Loss: 20.4740, Val Loss: 18.7409\n",
      "Epoch [31/500] Train Loss: 20.4031, Val Loss: 18.6660\n",
      "Epoch [32/500] Train Loss: 20.3368, Val Loss: 18.6009\n",
      "Epoch [33/500] Train Loss: 20.2682, Val Loss: 18.5301\n",
      "Epoch [34/500] Train Loss: 20.2051, Val Loss: 18.4686\n",
      "Epoch [35/500] Train Loss: 20.1426, Val Loss: 18.4003\n",
      "Epoch [36/500] Train Loss: 20.0875, Val Loss: 18.3410\n",
      "Epoch [37/500] Train Loss: 20.0275, Val Loss: 18.2787\n",
      "Epoch [38/500] Train Loss: 19.9747, Val Loss: 18.2212\n",
      "Epoch [39/500] Train Loss: 19.9150, Val Loss: 18.1581\n",
      "Epoch [40/500] Train Loss: 19.8598, Val Loss: 18.1031\n",
      "Epoch [41/500] Train Loss: 19.8068, Val Loss: 18.0403\n",
      "Epoch [42/500] Train Loss: 19.7563, Val Loss: 17.9811\n",
      "Epoch [43/500] Train Loss: 19.7006, Val Loss: 17.9229\n",
      "Epoch [44/500] Train Loss: 19.6486, Val Loss: 17.8677\n",
      "Epoch [45/500] Train Loss: 19.5974, Val Loss: 17.8092\n",
      "Epoch [46/500] Train Loss: 19.5447, Val Loss: 17.7533\n",
      "Epoch [47/500] Train Loss: 19.4941, Val Loss: 17.6970\n",
      "Epoch [48/500] Train Loss: 19.4450, Val Loss: 17.6420\n",
      "Epoch [49/500] Train Loss: 19.3929, Val Loss: 17.5897\n",
      "Epoch [50/500] Train Loss: 19.3441, Val Loss: 17.5344\n",
      "Epoch [51/500] Train Loss: 19.2981, Val Loss: 17.4824\n",
      "Epoch [52/500] Train Loss: 19.2514, Val Loss: 17.4304\n",
      "Epoch [53/500] Train Loss: 19.2010, Val Loss: 17.3803\n",
      "Epoch [54/500] Train Loss: 19.1554, Val Loss: 17.3303\n",
      "Epoch [55/500] Train Loss: 19.1112, Val Loss: 17.2813\n",
      "Epoch [56/500] Train Loss: 19.0665, Val Loss: 17.2346\n",
      "Epoch [57/500] Train Loss: 19.0232, Val Loss: 17.1902\n",
      "Epoch [58/500] Train Loss: 18.9830, Val Loss: 17.1471\n",
      "Epoch [59/500] Train Loss: 18.9405, Val Loss: 17.1026\n",
      "Epoch [60/500] Train Loss: 18.9016, Val Loss: 17.0629\n",
      "Epoch [61/500] Train Loss: 18.8601, Val Loss: 17.0256\n",
      "Epoch [62/500] Train Loss: 18.8222, Val Loss: 16.9853\n",
      "Epoch [63/500] Train Loss: 18.7845, Val Loss: 16.9511\n",
      "Epoch [64/500] Train Loss: 18.7503, Val Loss: 16.9167\n",
      "Epoch [65/500] Train Loss: 18.7157, Val Loss: 16.8865\n",
      "Epoch [66/500] Train Loss: 18.6847, Val Loss: 16.8562\n",
      "Epoch [67/500] Train Loss: 18.6527, Val Loss: 16.8278\n",
      "Epoch [68/500] Train Loss: 18.6239, Val Loss: 16.8014\n",
      "Epoch [69/500] Train Loss: 18.5951, Val Loss: 16.7769\n",
      "Epoch [70/500] Train Loss: 18.5690, Val Loss: 16.7524\n",
      "Epoch [71/500] Train Loss: 18.5435, Val Loss: 16.7288\n",
      "Epoch [72/500] Train Loss: 18.5199, Val Loss: 16.7065\n",
      "Epoch [73/500] Train Loss: 18.4972, Val Loss: 16.6877\n",
      "Epoch [74/500] Train Loss: 18.4758, Val Loss: 16.6667\n",
      "Epoch [75/500] Train Loss: 18.4544, Val Loss: 16.6484\n",
      "Epoch [76/500] Train Loss: 18.4331, Val Loss: 16.6265\n",
      "Epoch [77/500] Train Loss: 18.4124, Val Loss: 16.6072\n",
      "Epoch [78/500] Train Loss: 18.3936, Val Loss: 16.5936\n",
      "Epoch [79/500] Train Loss: 18.3792, Val Loss: 16.5773\n",
      "Epoch [80/500] Train Loss: 18.3650, Val Loss: 16.5644\n",
      "Epoch [81/500] Train Loss: 18.3533, Val Loss: 16.5541\n",
      "Epoch [82/500] Train Loss: 18.3427, Val Loss: 16.5406\n",
      "Epoch [83/500] Train Loss: 18.3326, Val Loss: 16.5294\n",
      "Epoch [84/500] Train Loss: 18.3242, Val Loss: 16.5209\n",
      "Epoch [85/500] Train Loss: 18.3154, Val Loss: 16.5126\n",
      "Epoch [86/500] Train Loss: 18.3091, Val Loss: 16.5029\n",
      "Epoch [87/500] Train Loss: 18.3011, Val Loss: 16.4947\n",
      "Epoch [88/500] Train Loss: 18.2945, Val Loss: 16.4876\n",
      "Epoch [89/500] Train Loss: 18.2892, Val Loss: 16.4809\n",
      "Epoch [90/500] Train Loss: 18.2837, Val Loss: 16.4745\n",
      "Epoch [91/500] Train Loss: 18.2788, Val Loss: 16.4684\n",
      "Epoch [92/500] Train Loss: 18.2738, Val Loss: 16.4628\n",
      "Epoch [93/500] Train Loss: 18.2697, Val Loss: 16.4581\n",
      "Epoch [94/500] Train Loss: 18.2648, Val Loss: 16.4521\n",
      "Epoch [95/500] Train Loss: 18.2604, Val Loss: 16.4462\n",
      "Epoch [96/500] Train Loss: 18.2568, Val Loss: 16.4407\n",
      "Epoch [97/500] Train Loss: 18.2512, Val Loss: 16.4347\n",
      "Epoch [98/500] Train Loss: 18.2458, Val Loss: 16.4291\n",
      "Epoch [99/500] Train Loss: 18.2412, Val Loss: 16.4252\n",
      "Epoch [100/500] Train Loss: 18.2361, Val Loss: 16.4195\n",
      "Epoch [101/500] Train Loss: 18.2288, Val Loss: 16.4134\n",
      "Epoch [102/500] Train Loss: 18.2219, Val Loss: 16.4074\n",
      "Epoch [103/500] Train Loss: 18.2145, Val Loss: 16.4005\n",
      "Epoch [104/500] Train Loss: 18.2045, Val Loss: 16.3893\n",
      "Epoch [105/500] Train Loss: 18.1891, Val Loss: 16.3720\n",
      "Epoch [106/500] Train Loss: 18.1605, Val Loss: 16.3221\n",
      "Epoch [107/500] Train Loss: 17.9589, Val Loss: 15.8142\n",
      "Epoch [108/500] Train Loss: 17.1901, Val Loss: 15.2135\n",
      "Epoch [109/500] Train Loss: 16.6277, Val Loss: 14.8292\n",
      "Epoch [110/500] Train Loss: 16.1991, Val Loss: 14.5018\n",
      "Epoch [111/500] Train Loss: 15.8336, Val Loss: 14.2124\n",
      "Epoch [112/500] Train Loss: 15.5088, Val Loss: 13.9572\n",
      "Epoch [113/500] Train Loss: 15.2134, Val Loss: 13.7240\n",
      "Epoch [114/500] Train Loss: 14.9436, Val Loss: 13.5071\n",
      "Epoch [115/500] Train Loss: 14.6906, Val Loss: 13.3059\n",
      "Epoch [116/500] Train Loss: 14.4520, Val Loss: 13.1128\n",
      "Epoch [117/500] Train Loss: 14.2216, Val Loss: 12.9295\n",
      "Epoch [118/500] Train Loss: 14.0019, Val Loss: 12.7535\n",
      "Epoch [119/500] Train Loss: 13.7897, Val Loss: 12.5845\n",
      "Epoch [120/500] Train Loss: 13.5847, Val Loss: 12.4199\n",
      "Epoch [121/500] Train Loss: 13.3825, Val Loss: 12.2624\n",
      "Epoch [122/500] Train Loss: 13.1867, Val Loss: 12.1068\n",
      "Epoch [123/500] Train Loss: 12.9948, Val Loss: 11.9538\n",
      "Epoch [124/500] Train Loss: 12.8056, Val Loss: 11.8018\n",
      "Epoch [125/500] Train Loss: 12.6165, Val Loss: 11.6563\n",
      "Epoch [126/500] Train Loss: 12.4343, Val Loss: 11.5153\n",
      "Epoch [127/500] Train Loss: 12.2559, Val Loss: 11.3774\n",
      "Epoch [128/500] Train Loss: 12.0788, Val Loss: 11.2429\n",
      "Epoch [129/500] Train Loss: 11.9091, Val Loss: 11.1093\n",
      "Epoch [130/500] Train Loss: 11.7363, Val Loss: 10.9787\n",
      "Epoch [131/500] Train Loss: 11.5687, Val Loss: 10.8531\n",
      "Epoch [132/500] Train Loss: 11.4054, Val Loss: 10.7317\n",
      "Epoch [133/500] Train Loss: 11.2455, Val Loss: 10.6113\n",
      "Epoch [134/500] Train Loss: 11.0895, Val Loss: 10.4940\n",
      "Epoch [135/500] Train Loss: 10.9303, Val Loss: 10.3804\n",
      "Epoch [136/500] Train Loss: 10.7805, Val Loss: 10.2694\n",
      "Epoch [137/500] Train Loss: 10.6326, Val Loss: 10.1620\n",
      "Epoch [138/500] Train Loss: 10.4882, Val Loss: 10.0592\n",
      "Epoch [139/500] Train Loss: 10.3491, Val Loss: 9.9603\n",
      "Epoch [140/500] Train Loss: 10.2125, Val Loss: 9.8654\n",
      "Epoch [141/500] Train Loss: 10.0783, Val Loss: 9.7741\n",
      "Epoch [142/500] Train Loss: 9.9507, Val Loss: 9.6851\n",
      "Epoch [143/500] Train Loss: 9.8256, Val Loss: 9.6018\n",
      "Epoch [144/500] Train Loss: 9.7061, Val Loss: 9.5203\n",
      "Epoch [145/500] Train Loss: 9.5905, Val Loss: 9.4449\n",
      "Epoch [146/500] Train Loss: 9.4791, Val Loss: 9.3746\n",
      "Epoch [147/500] Train Loss: 9.3737, Val Loss: 9.3049\n",
      "Epoch [148/500] Train Loss: 9.2722, Val Loss: 9.2411\n",
      "Epoch [149/500] Train Loss: 9.1765, Val Loss: 9.1799\n",
      "Epoch [150/500] Train Loss: 9.0813, Val Loss: 9.1228\n",
      "Epoch [151/500] Train Loss: 8.9914, Val Loss: 9.0713\n",
      "Epoch [152/500] Train Loss: 8.9064, Val Loss: 9.0201\n",
      "Epoch [153/500] Train Loss: 8.8212, Val Loss: 8.9727\n",
      "Epoch [154/500] Train Loss: 8.7388, Val Loss: 8.9275\n",
      "Epoch [155/500] Train Loss: 8.6591, Val Loss: 8.8836\n",
      "Epoch [156/500] Train Loss: 8.5785, Val Loss: 8.8435\n",
      "Epoch [157/500] Train Loss: 8.4977, Val Loss: 8.8031\n",
      "Epoch [158/500] Train Loss: 8.3408, Val Loss: 8.8094\n",
      "Epoch [159/500] Train Loss: 8.2075, Val Loss: 8.7460\n",
      "Epoch [160/500] Train Loss: 8.0915, Val Loss: 8.6808\n",
      "Epoch [161/500] Train Loss: 8.0047, Val Loss: 8.7304\n",
      "Epoch [162/500] Train Loss: 7.9669, Val Loss: 8.7171\n",
      "Epoch [163/500] Train Loss: 7.9379, Val Loss: 8.7012\n",
      "Epoch [164/500] Train Loss: 7.9277, Val Loss: 8.7018\n",
      "Epoch [165/500] Train Loss: 7.9121, Val Loss: 8.7299\n",
      "Epoch [166/500] Train Loss: 7.9072, Val Loss: 8.7297\n",
      "Epoch [167/500] Train Loss: 7.9047, Val Loss: 8.7104\n",
      "Epoch [168/500] Train Loss: 7.8976, Val Loss: 8.7353\n",
      "Epoch [169/500] Train Loss: 7.8927, Val Loss: 8.7108\n",
      "Epoch [170/500] Train Loss: 7.8889, Val Loss: 8.7121\n",
      "Epoch [171/500] Train Loss: 7.8858, Val Loss: 8.7150\n",
      "Epoch [172/500] Train Loss: 7.8835, Val Loss: 8.7238\n",
      "Epoch [173/500] Train Loss: 7.8780, Val Loss: 8.7199\n",
      "Epoch [174/500] Train Loss: 7.8736, Val Loss: 8.7028\n",
      "Epoch [175/500] Train Loss: 7.8704, Val Loss: 8.6975\n",
      "Epoch [176/500] Train Loss: 7.8628, Val Loss: 8.6872\n",
      "Epoch [177/500] Train Loss: 7.8548, Val Loss: 8.6923\n",
      "Epoch [178/500] Train Loss: 7.8419, Val Loss: 8.6610\n",
      "Epoch [179/500] Train Loss: 7.8287, Val Loss: 8.6402\n",
      "Epoch [180/500] Train Loss: 7.8136, Val Loss: 8.6308\n",
      "Epoch [181/500] Train Loss: 7.7929, Val Loss: 8.6350\n",
      "Epoch [182/500] Train Loss: 7.7662, Val Loss: 8.5825\n",
      "Epoch [183/500] Train Loss: 7.7245, Val Loss: 8.5247\n",
      "Epoch [184/500] Train Loss: 7.6631, Val Loss: 8.4460\n",
      "Epoch [185/500] Train Loss: 7.5727, Val Loss: 8.3066\n",
      "Epoch [186/500] Train Loss: 7.4265, Val Loss: 8.1405\n",
      "Epoch [187/500] Train Loss: 7.2036, Val Loss: 7.8207\n",
      "Epoch [188/500] Train Loss: 6.8565, Val Loss: 7.3769\n",
      "Epoch [189/500] Train Loss: 6.3746, Val Loss: 6.7804\n",
      "Epoch [190/500] Train Loss: 5.7672, Val Loss: 6.1496\n",
      "Epoch [191/500] Train Loss: 5.1523, Val Loss: 5.5894\n",
      "Epoch [192/500] Train Loss: 4.7839, Val Loss: 5.2687\n",
      "Epoch [193/500] Train Loss: 4.5648, Val Loss: 5.0717\n",
      "Epoch [194/500] Train Loss: 4.4090, Val Loss: 4.9351\n",
      "Epoch [195/500] Train Loss: 4.3081, Val Loss: 4.8469\n",
      "Epoch [196/500] Train Loss: 4.2338, Val Loss: 4.7747\n",
      "Epoch [197/500] Train Loss: 4.1731, Val Loss: 4.7282\n",
      "Epoch [198/500] Train Loss: 4.1212, Val Loss: 4.6840\n",
      "Epoch [199/500] Train Loss: 4.0807, Val Loss: 4.6563\n",
      "Epoch [200/500] Train Loss: 4.0433, Val Loss: 4.6264\n",
      "Epoch [201/500] Train Loss: 4.0118, Val Loss: 4.6022\n",
      "Epoch [202/500] Train Loss: 3.9869, Val Loss: 4.5639\n",
      "Epoch [203/500] Train Loss: 3.9511, Val Loss: 4.5245\n",
      "Epoch [204/500] Train Loss: 3.9214, Val Loss: 4.4794\n",
      "Epoch [205/500] Train Loss: 3.8910, Val Loss: 4.4473\n",
      "Epoch [206/500] Train Loss: 3.8632, Val Loss: 4.4282\n",
      "Epoch [207/500] Train Loss: 3.8391, Val Loss: 4.3991\n",
      "Epoch [208/500] Train Loss: 3.8119, Val Loss: 4.3606\n",
      "Epoch [209/500] Train Loss: 3.7857, Val Loss: 4.3348\n",
      "Epoch [210/500] Train Loss: 3.7588, Val Loss: 4.2812\n",
      "Epoch [211/500] Train Loss: 3.7221, Val Loss: 4.2526\n",
      "Epoch [212/500] Train Loss: 3.6858, Val Loss: 4.1989\n",
      "Epoch [213/500] Train Loss: 3.6582, Val Loss: 4.1417\n",
      "Epoch [214/500] Train Loss: 3.6143, Val Loss: 4.0787\n",
      "Epoch [215/500] Train Loss: 3.5825, Val Loss: 4.0322\n",
      "Epoch [216/500] Train Loss: 3.5466, Val Loss: 3.9756\n",
      "Epoch [217/500] Train Loss: 3.5137, Val Loss: 3.9363\n",
      "Epoch [218/500] Train Loss: 3.4846, Val Loss: 3.8860\n",
      "Epoch [219/500] Train Loss: 3.4496, Val Loss: 3.8353\n",
      "Epoch [220/500] Train Loss: 3.4168, Val Loss: 3.8275\n",
      "Epoch [221/500] Train Loss: 3.4048, Val Loss: 3.7590\n",
      "Epoch [222/500] Train Loss: 3.3743, Val Loss: 3.7348\n",
      "Epoch [223/500] Train Loss: 3.3499, Val Loss: 3.7270\n",
      "Epoch [224/500] Train Loss: 3.3476, Val Loss: 3.7267\n",
      "Epoch [225/500] Train Loss: 3.3240, Val Loss: 3.6820\n",
      "Epoch [226/500] Train Loss: 3.2979, Val Loss: 3.6803\n",
      "Epoch [227/500] Train Loss: 3.2845, Val Loss: 3.6586\n",
      "Epoch [228/500] Train Loss: 3.2746, Val Loss: 3.6426\n",
      "Epoch [229/500] Train Loss: 3.2624, Val Loss: 3.6256\n",
      "Epoch [230/500] Train Loss: 3.2519, Val Loss: 3.6262\n",
      "Epoch [231/500] Train Loss: 3.2489, Val Loss: 3.6091\n",
      "Epoch [232/500] Train Loss: 3.2520, Val Loss: 3.6188\n",
      "Epoch [233/500] Train Loss: 3.2299, Val Loss: 3.5869\n",
      "Epoch [234/500] Train Loss: 3.2114, Val Loss: 3.5820\n",
      "Epoch [235/500] Train Loss: 3.2007, Val Loss: 3.5768\n",
      "Epoch [236/500] Train Loss: 3.1901, Val Loss: 3.5748\n",
      "Epoch [237/500] Train Loss: 3.1842, Val Loss: 3.5743\n",
      "Epoch [238/500] Train Loss: 3.1721, Val Loss: 3.5697\n",
      "Epoch [239/500] Train Loss: 3.1634, Val Loss: 3.5868\n",
      "Epoch [240/500] Train Loss: 3.1613, Val Loss: 3.5698\n",
      "Epoch [241/500] Train Loss: 3.1476, Val Loss: 3.5784\n",
      "Epoch [242/500] Train Loss: 3.1491, Val Loss: 3.5653\n",
      "Epoch [243/500] Train Loss: 3.1325, Val Loss: 3.5622\n",
      "Epoch [244/500] Train Loss: 3.1228, Val Loss: 3.5542\n",
      "Epoch [245/500] Train Loss: 3.1192, Val Loss: 3.5652\n",
      "Epoch [246/500] Train Loss: 3.1250, Val Loss: 3.5433\n",
      "Epoch [247/500] Train Loss: 3.1032, Val Loss: 3.5097\n",
      "Epoch [248/500] Train Loss: 3.1102, Val Loss: 3.5188\n",
      "Epoch [249/500] Train Loss: 3.1119, Val Loss: 3.4722\n",
      "Epoch [250/500] Train Loss: 3.0849, Val Loss: 3.4788\n",
      "Epoch [251/500] Train Loss: 3.0725, Val Loss: 3.4725\n",
      "Epoch [252/500] Train Loss: 3.0670, Val Loss: 3.4735\n",
      "Epoch [253/500] Train Loss: 3.0717, Val Loss: 3.4613\n",
      "Epoch [254/500] Train Loss: 3.0662, Val Loss: 3.4391\n",
      "Epoch [255/500] Train Loss: 3.0603, Val Loss: 3.4418\n",
      "Epoch [256/500] Train Loss: 3.0503, Val Loss: 3.4236\n",
      "Epoch [257/500] Train Loss: 3.0416, Val Loss: 3.4141\n",
      "Epoch [258/500] Train Loss: 3.0389, Val Loss: 3.4171\n",
      "Epoch [259/500] Train Loss: 3.0405, Val Loss: 3.3961\n",
      "Epoch [260/500] Train Loss: 3.0433, Val Loss: 3.3973\n",
      "Epoch [261/500] Train Loss: 3.0330, Val Loss: 3.3770\n",
      "Epoch [262/500] Train Loss: 3.0197, Val Loss: 3.3873\n",
      "Epoch [263/500] Train Loss: 3.0115, Val Loss: 3.3743\n",
      "Epoch [264/500] Train Loss: 3.0075, Val Loss: 3.3583\n",
      "Epoch [265/500] Train Loss: 3.0027, Val Loss: 3.3655\n",
      "Epoch [266/500] Train Loss: 2.9934, Val Loss: 3.3457\n",
      "Epoch [267/500] Train Loss: 2.9867, Val Loss: 3.3349\n",
      "Epoch [268/500] Train Loss: 2.9846, Val Loss: 3.3359\n",
      "Epoch [269/500] Train Loss: 2.9926, Val Loss: 3.3343\n",
      "Epoch [270/500] Train Loss: 2.9918, Val Loss: 3.3160\n",
      "Epoch [271/500] Train Loss: 2.9743, Val Loss: 3.3170\n",
      "Epoch [272/500] Train Loss: 2.9670, Val Loss: 3.3285\n",
      "Epoch [273/500] Train Loss: 2.9538, Val Loss: 3.2798\n",
      "Epoch [274/500] Train Loss: 2.9479, Val Loss: 3.2842\n",
      "Epoch [275/500] Train Loss: 2.9417, Val Loss: 3.2718\n",
      "Epoch [276/500] Train Loss: 2.9533, Val Loss: 3.2983\n",
      "Epoch [277/500] Train Loss: 2.9404, Val Loss: 3.2502\n",
      "Epoch [278/500] Train Loss: 2.9299, Val Loss: 3.2499\n",
      "Epoch [279/500] Train Loss: 2.9227, Val Loss: 3.2208\n",
      "Epoch [280/500] Train Loss: 2.9075, Val Loss: 3.2428\n",
      "Epoch [281/500] Train Loss: 2.9001, Val Loss: 3.2215\n",
      "Epoch [282/500] Train Loss: 2.8925, Val Loss: 3.2188\n",
      "Epoch [283/500] Train Loss: 2.8867, Val Loss: 3.1994\n",
      "Epoch [284/500] Train Loss: 2.8828, Val Loss: 3.1975\n",
      "Epoch [285/500] Train Loss: 2.8810, Val Loss: 3.2125\n",
      "Epoch [286/500] Train Loss: 2.8854, Val Loss: 3.1725\n",
      "Epoch [287/500] Train Loss: 2.8645, Val Loss: 3.1773\n",
      "Epoch [288/500] Train Loss: 2.8537, Val Loss: 3.1696\n",
      "Epoch [289/500] Train Loss: 2.8465, Val Loss: 3.1692\n",
      "Epoch [290/500] Train Loss: 2.8439, Val Loss: 3.1627\n",
      "Epoch [291/500] Train Loss: 2.8333, Val Loss: 3.1450\n",
      "Epoch [292/500] Train Loss: 2.8298, Val Loss: 3.1325\n",
      "Epoch [293/500] Train Loss: 2.8374, Val Loss: 3.1425\n",
      "Epoch [294/500] Train Loss: 2.8155, Val Loss: 3.0975\n",
      "Epoch [295/500] Train Loss: 2.8098, Val Loss: 3.1036\n",
      "Epoch [296/500] Train Loss: 2.8037, Val Loss: 3.0823\n",
      "Epoch [297/500] Train Loss: 2.7968, Val Loss: 3.1021\n",
      "Epoch [298/500] Train Loss: 2.7993, Val Loss: 3.1046\n",
      "Epoch [299/500] Train Loss: 2.8027, Val Loss: 3.1154\n",
      "Epoch [300/500] Train Loss: 2.7827, Val Loss: 3.0869\n",
      "Epoch [301/500] Train Loss: 2.7777, Val Loss: 3.0903\n",
      "Epoch [302/500] Train Loss: 2.7673, Val Loss: 3.0873\n",
      "Epoch [303/500] Train Loss: 2.7548, Val Loss: 3.0728\n",
      "Epoch [304/500] Train Loss: 2.7480, Val Loss: 3.0632\n",
      "Epoch [305/500] Train Loss: 2.7539, Val Loss: 3.0984\n",
      "Epoch [306/500] Train Loss: 2.7452, Val Loss: 3.0337\n",
      "Epoch [307/500] Train Loss: 2.7416, Val Loss: 3.1211\n",
      "Epoch [308/500] Train Loss: 2.7408, Val Loss: 3.0082\n",
      "Epoch [309/500] Train Loss: 2.7229, Val Loss: 3.0987\n",
      "Epoch [310/500] Train Loss: 2.7230, Val Loss: 3.0236\n",
      "Epoch [311/500] Train Loss: 2.7220, Val Loss: 2.9883\n",
      "Epoch [312/500] Train Loss: 2.7082, Val Loss: 3.0077\n",
      "Epoch [313/500] Train Loss: 2.7016, Val Loss: 3.0122\n",
      "Epoch [314/500] Train Loss: 2.6989, Val Loss: 3.0174\n",
      "Epoch [315/500] Train Loss: 2.6908, Val Loss: 3.0014\n",
      "Epoch [316/500] Train Loss: 2.6835, Val Loss: 2.9748\n",
      "Epoch [317/500] Train Loss: 2.6867, Val Loss: 2.9931\n",
      "Epoch [318/500] Train Loss: 2.6786, Val Loss: 2.9777\n",
      "Epoch [319/500] Train Loss: 2.6639, Val Loss: 2.9722\n",
      "Epoch [320/500] Train Loss: 2.6642, Val Loss: 2.9408\n",
      "Epoch [321/500] Train Loss: 2.6761, Val Loss: 2.9283\n",
      "Epoch [322/500] Train Loss: 2.6519, Val Loss: 2.9408\n",
      "Epoch [323/500] Train Loss: 2.6416, Val Loss: 2.9563\n",
      "Epoch [324/500] Train Loss: 2.6379, Val Loss: 2.9346\n",
      "Epoch [325/500] Train Loss: 2.6286, Val Loss: 2.9481\n",
      "Epoch [326/500] Train Loss: 2.6327, Val Loss: 2.8984\n",
      "Epoch [327/500] Train Loss: 2.6112, Val Loss: 2.9261\n",
      "Epoch [328/500] Train Loss: 2.6097, Val Loss: 2.9215\n",
      "Epoch [329/500] Train Loss: 2.6037, Val Loss: 2.8738\n",
      "Epoch [330/500] Train Loss: 2.6024, Val Loss: 2.8856\n",
      "Epoch [331/500] Train Loss: 2.5910, Val Loss: 2.8619\n",
      "Epoch [332/500] Train Loss: 2.5942, Val Loss: 2.8726\n",
      "Epoch [333/500] Train Loss: 2.5809, Val Loss: 2.8708\n",
      "Epoch [334/500] Train Loss: 2.5750, Val Loss: 2.9037\n",
      "Epoch [335/500] Train Loss: 2.5742, Val Loss: 2.8579\n",
      "Epoch [336/500] Train Loss: 2.5710, Val Loss: 2.8576\n",
      "Epoch [337/500] Train Loss: 2.5688, Val Loss: 2.8029\n",
      "Epoch [338/500] Train Loss: 2.5485, Val Loss: 2.8628\n",
      "Epoch [339/500] Train Loss: 2.5458, Val Loss: 2.8819\n",
      "Epoch [340/500] Train Loss: 2.5445, Val Loss: 2.7850\n",
      "Epoch [341/500] Train Loss: 2.5405, Val Loss: 2.8004\n",
      "Epoch [342/500] Train Loss: 2.5310, Val Loss: 2.7705\n",
      "Epoch [343/500] Train Loss: 2.5269, Val Loss: 2.7774\n",
      "Epoch [344/500] Train Loss: 2.5229, Val Loss: 2.7665\n",
      "Epoch [345/500] Train Loss: 2.5239, Val Loss: 2.8213\n",
      "Epoch [346/500] Train Loss: 2.5242, Val Loss: 2.7552\n",
      "Epoch [347/500] Train Loss: 2.5295, Val Loss: 2.6709\n",
      "Epoch [348/500] Train Loss: 2.5082, Val Loss: 2.7781\n",
      "Epoch [349/500] Train Loss: 2.4995, Val Loss: 2.7822\n",
      "Epoch [350/500] Train Loss: 2.5004, Val Loss: 2.7217\n",
      "Epoch [351/500] Train Loss: 2.4920, Val Loss: 2.7461\n",
      "Epoch [352/500] Train Loss: 2.4964, Val Loss: 2.8077\n",
      "Epoch [353/500] Train Loss: 2.4825, Val Loss: 2.7416\n",
      "Epoch [354/500] Train Loss: 2.4838, Val Loss: 2.7434\n",
      "Epoch [355/500] Train Loss: 2.5066, Val Loss: 2.7461\n",
      "Epoch [356/500] Train Loss: 2.4963, Val Loss: 2.6235\n",
      "Epoch [357/500] Train Loss: 2.4716, Val Loss: 2.7252\n",
      "Epoch [358/500] Train Loss: 2.4643, Val Loss: 2.7262\n",
      "Epoch [359/500] Train Loss: 2.4578, Val Loss: 2.6920\n",
      "Epoch [360/500] Train Loss: 2.4536, Val Loss: 2.6984\n",
      "Epoch [361/500] Train Loss: 2.4537, Val Loss: 2.6904\n",
      "Epoch [362/500] Train Loss: 2.4479, Val Loss: 2.7943\n",
      "Epoch [363/500] Train Loss: 2.4627, Val Loss: 2.7230\n",
      "Epoch [364/500] Train Loss: 2.4456, Val Loss: 2.6932\n",
      "Epoch [365/500] Train Loss: 2.4445, Val Loss: 2.6588\n",
      "Epoch [366/500] Train Loss: 2.4372, Val Loss: 2.6415\n",
      "Epoch [367/500] Train Loss: 2.4320, Val Loss: 2.6958\n",
      "Epoch [368/500] Train Loss: 2.4361, Val Loss: 2.6187\n",
      "Epoch [369/500] Train Loss: 2.4439, Val Loss: 2.6859\n",
      "Epoch [370/500] Train Loss: 2.4271, Val Loss: 2.6060\n",
      "Epoch [371/500] Train Loss: 2.4228, Val Loss: 2.6792\n",
      "Epoch [372/500] Train Loss: 2.4153, Val Loss: 2.5626\n",
      "Epoch [373/500] Train Loss: 2.4155, Val Loss: 2.6292\n",
      "Epoch [374/500] Train Loss: 2.4161, Val Loss: 2.6447\n",
      "Epoch [375/500] Train Loss: 2.4119, Val Loss: 2.6710\n",
      "Epoch [376/500] Train Loss: 2.4165, Val Loss: 2.6278\n",
      "Epoch [377/500] Train Loss: 2.3985, Val Loss: 2.6267\n",
      "Epoch [378/500] Train Loss: 2.3984, Val Loss: 2.5981\n",
      "Epoch [379/500] Train Loss: 2.4043, Val Loss: 2.6210\n",
      "Epoch [380/500] Train Loss: 2.3926, Val Loss: 2.5290\n",
      "Epoch [381/500] Train Loss: 2.3888, Val Loss: 2.5789\n",
      "Epoch [382/500] Train Loss: 2.3818, Val Loss: 2.5493\n",
      "Epoch [383/500] Train Loss: 2.3870, Val Loss: 2.5657\n",
      "Epoch [384/500] Train Loss: 2.3812, Val Loss: 2.5700\n",
      "Epoch [385/500] Train Loss: 2.3779, Val Loss: 2.5541\n",
      "Epoch [386/500] Train Loss: 2.3713, Val Loss: 2.5657\n",
      "Epoch [387/500] Train Loss: 2.3739, Val Loss: 2.5474\n",
      "Epoch [388/500] Train Loss: 2.3656, Val Loss: 2.5372\n",
      "Epoch [389/500] Train Loss: 2.3632, Val Loss: 2.5358\n",
      "Epoch [390/500] Train Loss: 2.3668, Val Loss: 2.5672\n",
      "Epoch [391/500] Train Loss: 2.3563, Val Loss: 2.5524\n",
      "Epoch [392/500] Train Loss: 2.3643, Val Loss: 2.5206\n",
      "Epoch [393/500] Train Loss: 2.3528, Val Loss: 2.5272\n",
      "Epoch [394/500] Train Loss: 2.3468, Val Loss: 2.5017\n",
      "Epoch [395/500] Train Loss: 2.3428, Val Loss: 2.5480\n",
      "Epoch [396/500] Train Loss: 2.3484, Val Loss: 2.5092\n",
      "Epoch [397/500] Train Loss: 2.3489, Val Loss: 2.5804\n",
      "Epoch [398/500] Train Loss: 2.3621, Val Loss: 2.5159\n",
      "Epoch [399/500] Train Loss: 2.3353, Val Loss: 2.4659\n",
      "Epoch [400/500] Train Loss: 2.3342, Val Loss: 2.5280\n",
      "Epoch [401/500] Train Loss: 2.3399, Val Loss: 2.4941\n",
      "Epoch [402/500] Train Loss: 2.3442, Val Loss: 2.5207\n",
      "Epoch [403/500] Train Loss: 2.3299, Val Loss: 2.4319\n",
      "Epoch [404/500] Train Loss: 2.3206, Val Loss: 2.5217\n",
      "Epoch [405/500] Train Loss: 2.3200, Val Loss: 2.5629\n",
      "Epoch [406/500] Train Loss: 2.3291, Val Loss: 2.4903\n",
      "Epoch [407/500] Train Loss: 2.3193, Val Loss: 2.5183\n",
      "Epoch [408/500] Train Loss: 2.3121, Val Loss: 2.4535\n",
      "Epoch [409/500] Train Loss: 2.3095, Val Loss: 2.4827\n",
      "Epoch [410/500] Train Loss: 2.3068, Val Loss: 2.5175\n",
      "Epoch [411/500] Train Loss: 2.3081, Val Loss: 2.4352\n",
      "Epoch [412/500] Train Loss: 2.3105, Val Loss: 2.4275\n",
      "Epoch [413/500] Train Loss: 2.3021, Val Loss: 2.4466\n",
      "Epoch [414/500] Train Loss: 2.3061, Val Loss: 2.4480\n",
      "Epoch [415/500] Train Loss: 2.3037, Val Loss: 2.4325\n",
      "Epoch [416/500] Train Loss: 2.2965, Val Loss: 2.4901\n",
      "Epoch [417/500] Train Loss: 2.2992, Val Loss: 2.4835\n",
      "Epoch [418/500] Train Loss: 2.3148, Val Loss: 2.5176\n",
      "Epoch [419/500] Train Loss: 2.3013, Val Loss: 2.4928\n",
      "Epoch [420/500] Train Loss: 2.2877, Val Loss: 2.4716\n",
      "Epoch [421/500] Train Loss: 2.2885, Val Loss: 2.3932\n",
      "Epoch [422/500] Train Loss: 2.2758, Val Loss: 2.4590\n",
      "Epoch [423/500] Train Loss: 2.2750, Val Loss: 2.3852\n",
      "Epoch [424/500] Train Loss: 2.2780, Val Loss: 2.4225\n",
      "Epoch [425/500] Train Loss: 2.2710, Val Loss: 2.4016\n",
      "Epoch [426/500] Train Loss: 2.2681, Val Loss: 2.4154\n",
      "Epoch [427/500] Train Loss: 2.2654, Val Loss: 2.3744\n",
      "Epoch [428/500] Train Loss: 2.2639, Val Loss: 2.4061\n",
      "Epoch [429/500] Train Loss: 2.2643, Val Loss: 2.3690\n",
      "Epoch [430/500] Train Loss: 2.2610, Val Loss: 2.4270\n",
      "Epoch [431/500] Train Loss: 2.2580, Val Loss: 2.4077\n",
      "Epoch [432/500] Train Loss: 2.2604, Val Loss: 2.3455\n",
      "Epoch [433/500] Train Loss: 2.2542, Val Loss: 2.4781\n",
      "Epoch [434/500] Train Loss: 2.2691, Val Loss: 2.3833\n",
      "Epoch [435/500] Train Loss: 2.2570, Val Loss: 2.3855\n",
      "Epoch [436/500] Train Loss: 2.2563, Val Loss: 2.3586\n",
      "Epoch [437/500] Train Loss: 2.2464, Val Loss: 2.3870\n",
      "Epoch [438/500] Train Loss: 2.2543, Val Loss: 2.4464\n",
      "Epoch [439/500] Train Loss: 2.2738, Val Loss: 2.3420\n",
      "Epoch [440/500] Train Loss: 2.2653, Val Loss: 2.3894\n",
      "Epoch [441/500] Train Loss: 2.2696, Val Loss: 2.3855\n",
      "Epoch [442/500] Train Loss: 2.2572, Val Loss: 2.3413\n",
      "Epoch [443/500] Train Loss: 2.2529, Val Loss: 2.3773\n",
      "Epoch [444/500] Train Loss: 2.2378, Val Loss: 2.3525\n",
      "Epoch [445/500] Train Loss: 2.2407, Val Loss: 2.4167\n",
      "Epoch [446/500] Train Loss: 2.2478, Val Loss: 2.3875\n",
      "Epoch [447/500] Train Loss: 2.2480, Val Loss: 2.3924\n",
      "Epoch [448/500] Train Loss: 2.2497, Val Loss: 2.3418\n",
      "Epoch [449/500] Train Loss: 2.2261, Val Loss: 2.3433\n",
      "Epoch [450/500] Train Loss: 2.2338, Val Loss: 2.2716\n",
      "Epoch [451/500] Train Loss: 2.2416, Val Loss: 2.4429\n",
      "Epoch [452/500] Train Loss: 2.2254, Val Loss: 2.2737\n",
      "Epoch [453/500] Train Loss: 2.2374, Val Loss: 2.3156\n",
      "Epoch [454/500] Train Loss: 2.2095, Val Loss: 2.3272\n",
      "Epoch [455/500] Train Loss: 2.2110, Val Loss: 2.3258\n",
      "Epoch [456/500] Train Loss: 2.2146, Val Loss: 2.3331\n",
      "Epoch [457/500] Train Loss: 2.2143, Val Loss: 2.3440\n",
      "Epoch [458/500] Train Loss: 2.2202, Val Loss: 2.4069\n",
      "Epoch [459/500] Train Loss: 2.2136, Val Loss: 2.2453\n",
      "Epoch [460/500] Train Loss: 2.1966, Val Loss: 2.3480\n",
      "Epoch [461/500] Train Loss: 2.2076, Val Loss: 2.3599\n",
      "Epoch [462/500] Train Loss: 2.2011, Val Loss: 2.3445\n",
      "Epoch [463/500] Train Loss: 2.1999, Val Loss: 2.2931\n",
      "Epoch [464/500] Train Loss: 2.1911, Val Loss: 2.3512\n",
      "Epoch [465/500] Train Loss: 2.1906, Val Loss: 2.3220\n",
      "Epoch [466/500] Train Loss: 2.1889, Val Loss: 2.2686\n",
      "Epoch [467/500] Train Loss: 2.1864, Val Loss: 2.3054\n",
      "Epoch [468/500] Train Loss: 2.1909, Val Loss: 2.2683\n",
      "Epoch [469/500] Train Loss: 2.1839, Val Loss: 2.3229\n",
      "Epoch [470/500] Train Loss: 2.1807, Val Loss: 2.2747\n",
      "Epoch [471/500] Train Loss: 2.1804, Val Loss: 2.2515\n",
      "Epoch [472/500] Train Loss: 2.1790, Val Loss: 2.3403\n",
      "Epoch [473/500] Train Loss: 2.1798, Val Loss: 2.2358\n",
      "Epoch [474/500] Train Loss: 2.2123, Val Loss: 2.2599\n",
      "Epoch [475/500] Train Loss: 2.2024, Val Loss: 2.2985\n",
      "Epoch [476/500] Train Loss: 2.1803, Val Loss: 2.2892\n",
      "Epoch [477/500] Train Loss: 2.1696, Val Loss: 2.2458\n",
      "Epoch [478/500] Train Loss: 2.1696, Val Loss: 2.2964\n",
      "Epoch [479/500] Train Loss: 2.1734, Val Loss: 2.2150\n",
      "Epoch [480/500] Train Loss: 2.1756, Val Loss: 2.2326\n",
      "Epoch [481/500] Train Loss: 2.1607, Val Loss: 2.2350\n",
      "Epoch [482/500] Train Loss: 2.1656, Val Loss: 2.2258\n",
      "Epoch [483/500] Train Loss: 2.1635, Val Loss: 2.2012\n",
      "Epoch [484/500] Train Loss: 2.1637, Val Loss: 2.3147\n",
      "Epoch [485/500] Train Loss: 2.1764, Val Loss: 2.2304\n",
      "Epoch [486/500] Train Loss: 2.1540, Val Loss: 2.2306\n",
      "Epoch [487/500] Train Loss: 2.1512, Val Loss: 2.2536\n",
      "Epoch [488/500] Train Loss: 2.1498, Val Loss: 2.2038\n",
      "Epoch [489/500] Train Loss: 2.1448, Val Loss: 2.2552\n",
      "Epoch [490/500] Train Loss: 2.1583, Val Loss: 2.1967\n",
      "Epoch [491/500] Train Loss: 2.1487, Val Loss: 2.2292\n",
      "Epoch [492/500] Train Loss: 2.1433, Val Loss: 2.1902\n",
      "Epoch [493/500] Train Loss: 2.1385, Val Loss: 2.2189\n",
      "Epoch [494/500] Train Loss: 2.1390, Val Loss: 2.2200\n",
      "Epoch [495/500] Train Loss: 2.1337, Val Loss: 2.2284\n",
      "Epoch [496/500] Train Loss: 2.1325, Val Loss: 2.1953\n",
      "Epoch [497/500] Train Loss: 2.1348, Val Loss: 2.2147\n",
      "Epoch [498/500] Train Loss: 2.1355, Val Loss: 2.2383\n",
      "Epoch [499/500] Train Loss: 2.1359, Val Loss: 2.1971\n",
      "Epoch [500/500] Train Loss: 2.1294, Val Loss: 2.2197\n",
      "\n",
      "=== Fold 3 ===\n",
      "Epoch [1/500] Train Loss: 41.7718, Val Loss: 40.0691\n",
      "Epoch [2/500] Train Loss: 41.7594, Val Loss: 40.0559\n",
      "Epoch [3/500] Train Loss: 41.7463, Val Loss: 40.0414\n",
      "Epoch [4/500] Train Loss: 41.7314, Val Loss: 40.0235\n",
      "Epoch [5/500] Train Loss: 41.7085, Val Loss: 39.9879\n",
      "Epoch [6/500] Train Loss: 41.6595, Val Loss: 39.9113\n",
      "Epoch [7/500] Train Loss: 41.5479, Val Loss: 39.7360\n",
      "Epoch [8/500] Train Loss: 41.2693, Val Loss: 39.0936\n",
      "Epoch [9/500] Train Loss: 39.9087, Val Loss: 36.6872\n",
      "Epoch [10/500] Train Loss: 35.8555, Val Loss: 30.5456\n",
      "Epoch [11/500] Train Loss: 27.2412, Val Loss: 19.6957\n",
      "Epoch [12/500] Train Loss: 15.6742, Val Loss: 11.0209\n",
      "Epoch [13/500] Train Loss: 11.3783, Val Loss: 10.7815\n",
      "Epoch [14/500] Train Loss: 10.5537, Val Loss: 9.9398\n",
      "Epoch [15/500] Train Loss: 10.1334, Val Loss: 9.6848\n",
      "Epoch [16/500] Train Loss: 9.8748, Val Loss: 9.5194\n",
      "Epoch [17/500] Train Loss: 9.7288, Val Loss: 9.3356\n",
      "Epoch [18/500] Train Loss: 9.5968, Val Loss: 9.1990\n",
      "Epoch [19/500] Train Loss: 9.4829, Val Loss: 9.0748\n",
      "Epoch [20/500] Train Loss: 9.3815, Val Loss: 8.9630\n",
      "Epoch [21/500] Train Loss: 9.2919, Val Loss: 8.8619\n",
      "Epoch [22/500] Train Loss: 9.2077, Val Loss: 8.7772\n",
      "Epoch [23/500] Train Loss: 9.1344, Val Loss: 8.7033\n",
      "Epoch [24/500] Train Loss: 9.0596, Val Loss: 8.6219\n",
      "Epoch [25/500] Train Loss: 8.9912, Val Loss: 8.5559\n",
      "Epoch [26/500] Train Loss: 8.9344, Val Loss: 8.4951\n",
      "Epoch [27/500] Train Loss: 8.8639, Val Loss: 8.4197\n",
      "Epoch [28/500] Train Loss: 8.8041, Val Loss: 8.3692\n",
      "Epoch [29/500] Train Loss: 8.7415, Val Loss: 8.2984\n",
      "Epoch [30/500] Train Loss: 8.6874, Val Loss: 8.2402\n",
      "Epoch [31/500] Train Loss: 8.6294, Val Loss: 8.1846\n",
      "Epoch [32/500] Train Loss: 8.5753, Val Loss: 8.1357\n",
      "Epoch [33/500] Train Loss: 8.5246, Val Loss: 8.0842\n",
      "Epoch [34/500] Train Loss: 8.4765, Val Loss: 8.0514\n",
      "Epoch [35/500] Train Loss: 8.4327, Val Loss: 8.0041\n",
      "Epoch [36/500] Train Loss: 8.3916, Val Loss: 7.9623\n",
      "Epoch [37/500] Train Loss: 8.3524, Val Loss: 7.9286\n",
      "Epoch [38/500] Train Loss: 8.3173, Val Loss: 7.9063\n",
      "Epoch [39/500] Train Loss: 8.2822, Val Loss: 7.8631\n",
      "Epoch [40/500] Train Loss: 8.2507, Val Loss: 7.8378\n",
      "Epoch [41/500] Train Loss: 8.2224, Val Loss: 7.8115\n",
      "Epoch [42/500] Train Loss: 8.1971, Val Loss: 7.7831\n",
      "Epoch [43/500] Train Loss: 8.1760, Val Loss: 7.7635\n",
      "Epoch [44/500] Train Loss: 8.1565, Val Loss: 7.7574\n",
      "Epoch [45/500] Train Loss: 8.1375, Val Loss: 7.7399\n",
      "Epoch [46/500] Train Loss: 8.1213, Val Loss: 7.7208\n",
      "Epoch [47/500] Train Loss: 8.1083, Val Loss: 7.7033\n",
      "Epoch [48/500] Train Loss: 8.0961, Val Loss: 7.6980\n",
      "Epoch [49/500] Train Loss: 8.0864, Val Loss: 7.7020\n",
      "Epoch [50/500] Train Loss: 8.0793, Val Loss: 7.6965\n",
      "Epoch [51/500] Train Loss: 8.0735, Val Loss: 7.6854\n",
      "Epoch [52/500] Train Loss: 8.0681, Val Loss: 7.6860\n",
      "Epoch [53/500] Train Loss: 8.0643, Val Loss: 7.6718\n",
      "Epoch [54/500] Train Loss: 8.0593, Val Loss: 7.6766\n",
      "Epoch [55/500] Train Loss: 8.0549, Val Loss: 7.6717\n",
      "Epoch [56/500] Train Loss: 8.0535, Val Loss: 7.6786\n",
      "Epoch [57/500] Train Loss: 8.0517, Val Loss: 7.6600\n",
      "Epoch [58/500] Train Loss: 8.0457, Val Loss: 7.6659\n",
      "Epoch [59/500] Train Loss: 8.0391, Val Loss: 7.6591\n",
      "Epoch [60/500] Train Loss: 8.0348, Val Loss: 7.6540\n",
      "Epoch [61/500] Train Loss: 8.0311, Val Loss: 7.6533\n",
      "Epoch [62/500] Train Loss: 8.0280, Val Loss: 7.6417\n",
      "Epoch [63/500] Train Loss: 8.0260, Val Loss: 7.6468\n",
      "Epoch [64/500] Train Loss: 8.0225, Val Loss: 7.6377\n",
      "Epoch [65/500] Train Loss: 8.0207, Val Loss: 7.6363\n",
      "Epoch [66/500] Train Loss: 8.0187, Val Loss: 7.6342\n",
      "Epoch [67/500] Train Loss: 8.0158, Val Loss: 7.6269\n",
      "Epoch [68/500] Train Loss: 8.0156, Val Loss: 7.6304\n",
      "Epoch [69/500] Train Loss: 8.0129, Val Loss: 7.6341\n",
      "Epoch [70/500] Train Loss: 8.0114, Val Loss: 7.6345\n",
      "Epoch [71/500] Train Loss: 8.0111, Val Loss: 7.6290\n",
      "Epoch [72/500] Train Loss: 8.0086, Val Loss: 7.6238\n",
      "Epoch [73/500] Train Loss: 8.0070, Val Loss: 7.6258\n",
      "Epoch [74/500] Train Loss: 8.0063, Val Loss: 7.6241\n",
      "Epoch [75/500] Train Loss: 8.0046, Val Loss: 7.6271\n",
      "Epoch [76/500] Train Loss: 8.0042, Val Loss: 7.6354\n",
      "Epoch [77/500] Train Loss: 8.0045, Val Loss: 7.6212\n",
      "Epoch [78/500] Train Loss: 8.0013, Val Loss: 7.6230\n",
      "Epoch [79/500] Train Loss: 7.9998, Val Loss: 7.6175\n",
      "Epoch [80/500] Train Loss: 7.9988, Val Loss: 7.6148\n",
      "Epoch [81/500] Train Loss: 7.9991, Val Loss: 7.6216\n",
      "Epoch [82/500] Train Loss: 7.9973, Val Loss: 7.6133\n",
      "Epoch [83/500] Train Loss: 7.9973, Val Loss: 7.6192\n",
      "Epoch [84/500] Train Loss: 7.9951, Val Loss: 7.6192\n",
      "Epoch [85/500] Train Loss: 7.9942, Val Loss: 7.6151\n",
      "Epoch [86/500] Train Loss: 7.9963, Val Loss: 7.6082\n",
      "Epoch [87/500] Train Loss: 7.9942, Val Loss: 7.6029\n",
      "Epoch [88/500] Train Loss: 7.9919, Val Loss: 7.6079\n",
      "Epoch [89/500] Train Loss: 7.9912, Val Loss: 7.6116\n",
      "Epoch [90/500] Train Loss: 7.9902, Val Loss: 7.6145\n",
      "Epoch [91/500] Train Loss: 7.9891, Val Loss: 7.6132\n",
      "Epoch [92/500] Train Loss: 7.9878, Val Loss: 7.6089\n",
      "Epoch [93/500] Train Loss: 7.9885, Val Loss: 7.6089\n",
      "Epoch [94/500] Train Loss: 7.9864, Val Loss: 7.6097\n",
      "Epoch [95/500] Train Loss: 7.9854, Val Loss: 7.6127\n",
      "Epoch [96/500] Train Loss: 7.9842, Val Loss: 7.6078\n",
      "Epoch [97/500] Train Loss: 7.9829, Val Loss: 7.6097\n",
      "Epoch [98/500] Train Loss: 7.9826, Val Loss: 7.5938\n",
      "Epoch [99/500] Train Loss: 7.9796, Val Loss: 7.6028\n",
      "Epoch [100/500] Train Loss: 7.9803, Val Loss: 7.5929\n",
      "Epoch [101/500] Train Loss: 7.9769, Val Loss: 7.6013\n",
      "Epoch [102/500] Train Loss: 7.9745, Val Loss: 7.6002\n",
      "Epoch [103/500] Train Loss: 7.9724, Val Loss: 7.5871\n",
      "Epoch [104/500] Train Loss: 7.9701, Val Loss: 7.5847\n",
      "Epoch [105/500] Train Loss: 7.9664, Val Loss: 7.5825\n",
      "Epoch [106/500] Train Loss: 7.9642, Val Loss: 7.5923\n",
      "Epoch [107/500] Train Loss: 7.9601, Val Loss: 7.5858\n",
      "Epoch [108/500] Train Loss: 7.9556, Val Loss: 7.5749\n",
      "Epoch [109/500] Train Loss: 7.9510, Val Loss: 7.5710\n",
      "Epoch [110/500] Train Loss: 7.9451, Val Loss: 7.5561\n",
      "Epoch [111/500] Train Loss: 7.9378, Val Loss: 7.5534\n",
      "Epoch [112/500] Train Loss: 7.9308, Val Loss: 7.5464\n",
      "Epoch [113/500] Train Loss: 7.9204, Val Loss: 7.5295\n",
      "Epoch [114/500] Train Loss: 7.9097, Val Loss: 7.5155\n",
      "Epoch [115/500] Train Loss: 7.8935, Val Loss: 7.4995\n",
      "Epoch [116/500] Train Loss: 7.8779, Val Loss: 7.4613\n",
      "Epoch [117/500] Train Loss: 7.8500, Val Loss: 7.4439\n",
      "Epoch [118/500] Train Loss: 7.8150, Val Loss: 7.3936\n",
      "Epoch [119/500] Train Loss: 7.7684, Val Loss: 7.3399\n",
      "Epoch [120/500] Train Loss: 7.7019, Val Loss: 7.2356\n",
      "Epoch [121/500] Train Loss: 7.5770, Val Loss: 7.0037\n",
      "Epoch [122/500] Train Loss: 7.2937, Val Loss: 6.6791\n",
      "Epoch [123/500] Train Loss: 6.9140, Val Loss: 6.1113\n",
      "Epoch [124/500] Train Loss: 6.3234, Val Loss: 5.4421\n",
      "Epoch [125/500] Train Loss: 5.6817, Val Loss: 4.7007\n",
      "Epoch [126/500] Train Loss: 5.0246, Val Loss: 4.0780\n",
      "Epoch [127/500] Train Loss: 4.5581, Val Loss: 3.7133\n",
      "Epoch [128/500] Train Loss: 4.3505, Val Loss: 3.6042\n",
      "Epoch [129/500] Train Loss: 4.2596, Val Loss: 3.5434\n",
      "Epoch [130/500] Train Loss: 4.2008, Val Loss: 3.5229\n",
      "Epoch [131/500] Train Loss: 4.1752, Val Loss: 3.5075\n",
      "Epoch [132/500] Train Loss: 4.1582, Val Loss: 3.4926\n",
      "Epoch [133/500] Train Loss: 4.1343, Val Loss: 3.4438\n",
      "Epoch [134/500] Train Loss: 4.1196, Val Loss: 3.4402\n",
      "Epoch [135/500] Train Loss: 4.1010, Val Loss: 3.4197\n",
      "Epoch [136/500] Train Loss: 4.0867, Val Loss: 3.4081\n",
      "Epoch [137/500] Train Loss: 4.0739, Val Loss: 3.3955\n",
      "Epoch [138/500] Train Loss: 4.0647, Val Loss: 3.3831\n",
      "Epoch [139/500] Train Loss: 4.0578, Val Loss: 3.3533\n",
      "Epoch [140/500] Train Loss: 4.0464, Val Loss: 3.3375\n",
      "Epoch [141/500] Train Loss: 4.0392, Val Loss: 3.3153\n",
      "Epoch [142/500] Train Loss: 4.0275, Val Loss: 3.2930\n",
      "Epoch [143/500] Train Loss: 4.0177, Val Loss: 3.2779\n",
      "Epoch [144/500] Train Loss: 3.9924, Val Loss: 3.2530\n",
      "Epoch [145/500] Train Loss: 3.9849, Val Loss: 3.2300\n",
      "Epoch [146/500] Train Loss: 3.9738, Val Loss: 3.2153\n",
      "Epoch [147/500] Train Loss: 3.9647, Val Loss: 3.1916\n",
      "Epoch [148/500] Train Loss: 3.9473, Val Loss: 3.1661\n",
      "Epoch [149/500] Train Loss: 3.9305, Val Loss: 3.1428\n",
      "Epoch [150/500] Train Loss: 3.9334, Val Loss: 3.1078\n",
      "Epoch [151/500] Train Loss: 3.9224, Val Loss: 3.1142\n",
      "Epoch [152/500] Train Loss: 3.8858, Val Loss: 3.0801\n",
      "Epoch [153/500] Train Loss: 3.8676, Val Loss: 3.0626\n",
      "Epoch [154/500] Train Loss: 3.8486, Val Loss: 3.0429\n",
      "Epoch [155/500] Train Loss: 3.8316, Val Loss: 3.0133\n",
      "Epoch [156/500] Train Loss: 3.8093, Val Loss: 2.9911\n",
      "Epoch [157/500] Train Loss: 3.7871, Val Loss: 2.9744\n",
      "Epoch [158/500] Train Loss: 3.7703, Val Loss: 2.9455\n",
      "Epoch [159/500] Train Loss: 3.7476, Val Loss: 2.9234\n",
      "Epoch [160/500] Train Loss: 3.7190, Val Loss: 2.9143\n",
      "Epoch [161/500] Train Loss: 3.6959, Val Loss: 2.8959\n",
      "Epoch [162/500] Train Loss: 3.6761, Val Loss: 2.8673\n",
      "Epoch [163/500] Train Loss: 3.6411, Val Loss: 2.8353\n",
      "Epoch [164/500] Train Loss: 3.6240, Val Loss: 2.8637\n",
      "Epoch [165/500] Train Loss: 3.5697, Val Loss: 2.8016\n",
      "Epoch [166/500] Train Loss: 3.5339, Val Loss: 2.7810\n",
      "Epoch [167/500] Train Loss: 3.5168, Val Loss: 2.7779\n",
      "Epoch [168/500] Train Loss: 3.4600, Val Loss: 2.7328\n",
      "Epoch [169/500] Train Loss: 3.4161, Val Loss: 2.7274\n",
      "Epoch [170/500] Train Loss: 3.3926, Val Loss: 2.6957\n",
      "Epoch [171/500] Train Loss: 3.3461, Val Loss: 2.6800\n",
      "Epoch [172/500] Train Loss: 3.3170, Val Loss: 2.6932\n",
      "Epoch [173/500] Train Loss: 3.2749, Val Loss: 2.6529\n",
      "Epoch [174/500] Train Loss: 3.2424, Val Loss: 2.6346\n",
      "Epoch [175/500] Train Loss: 3.2092, Val Loss: 2.6355\n",
      "Epoch [176/500] Train Loss: 3.1693, Val Loss: 2.6219\n",
      "Epoch [177/500] Train Loss: 3.1566, Val Loss: 2.5925\n",
      "Epoch [178/500] Train Loss: 3.1235, Val Loss: 2.5877\n",
      "Epoch [179/500] Train Loss: 3.0844, Val Loss: 2.5522\n",
      "Epoch [180/500] Train Loss: 3.0482, Val Loss: 2.5405\n",
      "Epoch [181/500] Train Loss: 3.0188, Val Loss: 2.5118\n",
      "Epoch [182/500] Train Loss: 2.9771, Val Loss: 2.4924\n",
      "Epoch [183/500] Train Loss: 2.9544, Val Loss: 2.4533\n",
      "Epoch [184/500] Train Loss: 2.9261, Val Loss: 2.4194\n",
      "Epoch [185/500] Train Loss: 2.8701, Val Loss: 2.3688\n",
      "Epoch [186/500] Train Loss: 2.8018, Val Loss: 2.3394\n",
      "Epoch [187/500] Train Loss: 2.7200, Val Loss: 2.2577\n",
      "Epoch [188/500] Train Loss: 2.6030, Val Loss: 2.1456\n",
      "Epoch [189/500] Train Loss: 2.4475, Val Loss: 2.0038\n",
      "Epoch [190/500] Train Loss: 2.2267, Val Loss: 1.7788\n",
      "Epoch [191/500] Train Loss: 1.9474, Val Loss: 1.5824\n",
      "Epoch [192/500] Train Loss: 1.6889, Val Loss: 1.3512\n",
      "Epoch [193/500] Train Loss: 1.4369, Val Loss: 1.1790\n",
      "Epoch [194/500] Train Loss: 1.2339, Val Loss: 1.0292\n",
      "Epoch [195/500] Train Loss: 1.0643, Val Loss: 0.8595\n",
      "Epoch [196/500] Train Loss: 0.9192, Val Loss: 0.7101\n",
      "Epoch [197/500] Train Loss: 0.8348, Val Loss: 0.6374\n",
      "Epoch [198/500] Train Loss: 0.7651, Val Loss: 0.5838\n",
      "Epoch [199/500] Train Loss: 0.7067, Val Loss: 0.5164\n",
      "Epoch [200/500] Train Loss: 0.6338, Val Loss: 0.4554\n",
      "Epoch [201/500] Train Loss: 0.5860, Val Loss: 0.4124\n",
      "Epoch [202/500] Train Loss: 0.5344, Val Loss: 0.3742\n",
      "Epoch [203/500] Train Loss: 0.4813, Val Loss: 0.3346\n",
      "Epoch [204/500] Train Loss: 0.4415, Val Loss: 0.2995\n",
      "Epoch [205/500] Train Loss: 0.4052, Val Loss: 0.2684\n",
      "Epoch [206/500] Train Loss: 0.3696, Val Loss: 0.2497\n",
      "Epoch [207/500] Train Loss: 0.3416, Val Loss: 0.2290\n",
      "Epoch [208/500] Train Loss: 0.3182, Val Loss: 0.2024\n",
      "Epoch [209/500] Train Loss: 0.2970, Val Loss: 0.1935\n",
      "Epoch [210/500] Train Loss: 0.2723, Val Loss: 0.1748\n",
      "Epoch [211/500] Train Loss: 0.2547, Val Loss: 0.1584\n",
      "Epoch [212/500] Train Loss: 0.2340, Val Loss: 0.1466\n",
      "Epoch [213/500] Train Loss: 0.2205, Val Loss: 0.1375\n",
      "Epoch [214/500] Train Loss: 0.2104, Val Loss: 0.1272\n",
      "Epoch [215/500] Train Loss: 0.1933, Val Loss: 0.1199\n",
      "Epoch [216/500] Train Loss: 0.1832, Val Loss: 0.1122\n",
      "Epoch [217/500] Train Loss: 0.1716, Val Loss: 0.1025\n",
      "Epoch [218/500] Train Loss: 0.1625, Val Loss: 0.1043\n",
      "Epoch [219/500] Train Loss: 0.1583, Val Loss: 0.0933\n",
      "Epoch [220/500] Train Loss: 0.1485, Val Loss: 0.0844\n",
      "Epoch [221/500] Train Loss: 0.1412, Val Loss: 0.0837\n",
      "Epoch [222/500] Train Loss: 0.1334, Val Loss: 0.0767\n",
      "Epoch [223/500] Train Loss: 0.1263, Val Loss: 0.0756\n",
      "Epoch [224/500] Train Loss: 0.1232, Val Loss: 0.0692\n",
      "Epoch [225/500] Train Loss: 0.1177, Val Loss: 0.0659\n",
      "Epoch [226/500] Train Loss: 0.1116, Val Loss: 0.0750\n",
      "Epoch [227/500] Train Loss: 0.1129, Val Loss: 0.0597\n",
      "Epoch [228/500] Train Loss: 0.1048, Val Loss: 0.0607\n",
      "Epoch [229/500] Train Loss: 0.1024, Val Loss: 0.0535\n",
      "Epoch [230/500] Train Loss: 0.0963, Val Loss: 0.0531\n",
      "Epoch [231/500] Train Loss: 0.0923, Val Loss: 0.0483\n",
      "Epoch [232/500] Train Loss: 0.0895, Val Loss: 0.0465\n",
      "Epoch [233/500] Train Loss: 0.0868, Val Loss: 0.0465\n",
      "Epoch [234/500] Train Loss: 0.0841, Val Loss: 0.0483\n",
      "Epoch [235/500] Train Loss: 0.0813, Val Loss: 0.0419\n",
      "Epoch [236/500] Train Loss: 0.0782, Val Loss: 0.0413\n",
      "Epoch [237/500] Train Loss: 0.0765, Val Loss: 0.0424\n",
      "Epoch [238/500] Train Loss: 0.0756, Val Loss: 0.0416\n",
      "Epoch [239/500] Train Loss: 0.0733, Val Loss: 0.0366\n",
      "Epoch [240/500] Train Loss: 0.0693, Val Loss: 0.0372\n",
      "Epoch [241/500] Train Loss: 0.0682, Val Loss: 0.0349\n",
      "Epoch [242/500] Train Loss: 0.0668, Val Loss: 0.0336\n",
      "Epoch [243/500] Train Loss: 0.0651, Val Loss: 0.0352\n",
      "Epoch [244/500] Train Loss: 0.0634, Val Loss: 0.0331\n",
      "Epoch [245/500] Train Loss: 0.0621, Val Loss: 0.0330\n",
      "Epoch [246/500] Train Loss: 0.0597, Val Loss: 0.0303\n",
      "Epoch [247/500] Train Loss: 0.0576, Val Loss: 0.0297\n",
      "Epoch [248/500] Train Loss: 0.0559, Val Loss: 0.0294\n",
      "Epoch [249/500] Train Loss: 0.0564, Val Loss: 0.0282\n",
      "Epoch [250/500] Train Loss: 0.0538, Val Loss: 0.0327\n",
      "Epoch [251/500] Train Loss: 0.0537, Val Loss: 0.0269\n",
      "Epoch [252/500] Train Loss: 0.0522, Val Loss: 0.0285\n",
      "Epoch [253/500] Train Loss: 0.0520, Val Loss: 0.0259\n",
      "Epoch [254/500] Train Loss: 0.0485, Val Loss: 0.0261\n",
      "Epoch [255/500] Train Loss: 0.0475, Val Loss: 0.0252\n",
      "Epoch [256/500] Train Loss: 0.0483, Val Loss: 0.0261\n",
      "Epoch [257/500] Train Loss: 0.0464, Val Loss: 0.0246\n",
      "Epoch [258/500] Train Loss: 0.0458, Val Loss: 0.0247\n",
      "Epoch [259/500] Train Loss: 0.0448, Val Loss: 0.0230\n",
      "Epoch [260/500] Train Loss: 0.0426, Val Loss: 0.0236\n",
      "Epoch [261/500] Train Loss: 0.0426, Val Loss: 0.0238\n",
      "Epoch [262/500] Train Loss: 0.0414, Val Loss: 0.0217\n",
      "Epoch [263/500] Train Loss: 0.0400, Val Loss: 0.0212\n",
      "Epoch [264/500] Train Loss: 0.0394, Val Loss: 0.0221\n",
      "Epoch [265/500] Train Loss: 0.0386, Val Loss: 0.0209\n",
      "Epoch [266/500] Train Loss: 0.0372, Val Loss: 0.0217\n",
      "Epoch [267/500] Train Loss: 0.0371, Val Loss: 0.0204\n",
      "Epoch [268/500] Train Loss: 0.0362, Val Loss: 0.0195\n",
      "Epoch [269/500] Train Loss: 0.0356, Val Loss: 0.0192\n",
      "Epoch [270/500] Train Loss: 0.0344, Val Loss: 0.0187\n",
      "Epoch [271/500] Train Loss: 0.0336, Val Loss: 0.0196\n",
      "Epoch [272/500] Train Loss: 0.0333, Val Loss: 0.0187\n",
      "Epoch [273/500] Train Loss: 0.0330, Val Loss: 0.0202\n",
      "Epoch [274/500] Train Loss: 0.0329, Val Loss: 0.0195\n",
      "Epoch [275/500] Train Loss: 0.0315, Val Loss: 0.0183\n",
      "Epoch [276/500] Train Loss: 0.0306, Val Loss: 0.0173\n",
      "Epoch [277/500] Train Loss: 0.0305, Val Loss: 0.0190\n",
      "Epoch [278/500] Train Loss: 0.0317, Val Loss: 0.0194\n",
      "Epoch [279/500] Train Loss: 0.0299, Val Loss: 0.0189\n",
      "Epoch [280/500] Train Loss: 0.0310, Val Loss: 0.0255\n",
      "Epoch [281/500] Train Loss: 0.0324, Val Loss: 0.0167\n",
      "Epoch [282/500] Train Loss: 0.0286, Val Loss: 0.0157\n",
      "Epoch [283/500] Train Loss: 0.0273, Val Loss: 0.0176\n",
      "Epoch [284/500] Train Loss: 0.0267, Val Loss: 0.0151\n",
      "Epoch [285/500] Train Loss: 0.0261, Val Loss: 0.0157\n",
      "Epoch [286/500] Train Loss: 0.0259, Val Loss: 0.0148\n",
      "Epoch [287/500] Train Loss: 0.0253, Val Loss: 0.0146\n",
      "Epoch [288/500] Train Loss: 0.0247, Val Loss: 0.0140\n",
      "Epoch [289/500] Train Loss: 0.0256, Val Loss: 0.0150\n",
      "Epoch [290/500] Train Loss: 0.0245, Val Loss: 0.0137\n",
      "Epoch [291/500] Train Loss: 0.0238, Val Loss: 0.0139\n",
      "Epoch [292/500] Train Loss: 0.0240, Val Loss: 0.0157\n",
      "Epoch [293/500] Train Loss: 0.0237, Val Loss: 0.0132\n",
      "Epoch [294/500] Train Loss: 0.0224, Val Loss: 0.0126\n",
      "Epoch [295/500] Train Loss: 0.0221, Val Loss: 0.0127\n",
      "Epoch [296/500] Train Loss: 0.0216, Val Loss: 0.0123\n",
      "Epoch [297/500] Train Loss: 0.0220, Val Loss: 0.0130\n",
      "Epoch [298/500] Train Loss: 0.0214, Val Loss: 0.0138\n",
      "Epoch [299/500] Train Loss: 0.0219, Val Loss: 0.0141\n",
      "Epoch [300/500] Train Loss: 0.0217, Val Loss: 0.0164\n",
      "Epoch [301/500] Train Loss: 0.0208, Val Loss: 0.0123\n",
      "Epoch [302/500] Train Loss: 0.0202, Val Loss: 0.0117\n",
      "Epoch [303/500] Train Loss: 0.0195, Val Loss: 0.0124\n",
      "Epoch [304/500] Train Loss: 0.0195, Val Loss: 0.0121\n",
      "Epoch [305/500] Train Loss: 0.0193, Val Loss: 0.0142\n",
      "Epoch [306/500] Train Loss: 0.0193, Val Loss: 0.0125\n",
      "Epoch [307/500] Train Loss: 0.0186, Val Loss: 0.0113\n",
      "Epoch [308/500] Train Loss: 0.0184, Val Loss: 0.0111\n",
      "Epoch [309/500] Train Loss: 0.0183, Val Loss: 0.0108\n",
      "Epoch [310/500] Train Loss: 0.0175, Val Loss: 0.0106\n",
      "Epoch [311/500] Train Loss: 0.0173, Val Loss: 0.0106\n",
      "Epoch [312/500] Train Loss: 0.0177, Val Loss: 0.0099\n",
      "Epoch [313/500] Train Loss: 0.0176, Val Loss: 0.0099\n",
      "Epoch [314/500] Train Loss: 0.0168, Val Loss: 0.0104\n",
      "Epoch [315/500] Train Loss: 0.0163, Val Loss: 0.0095\n",
      "Epoch [316/500] Train Loss: 0.0163, Val Loss: 0.0106\n",
      "Epoch [317/500] Train Loss: 0.0163, Val Loss: 0.0097\n",
      "Epoch [318/500] Train Loss: 0.0159, Val Loss: 0.0092\n",
      "Epoch [319/500] Train Loss: 0.0156, Val Loss: 0.0102\n",
      "Epoch [320/500] Train Loss: 0.0153, Val Loss: 0.0098\n",
      "Epoch [321/500] Train Loss: 0.0155, Val Loss: 0.0094\n",
      "Epoch [322/500] Train Loss: 0.0150, Val Loss: 0.0098\n",
      "Epoch [323/500] Train Loss: 0.0148, Val Loss: 0.0095\n",
      "Epoch [324/500] Train Loss: 0.0147, Val Loss: 0.0097\n",
      "Epoch [325/500] Train Loss: 0.0144, Val Loss: 0.0092\n",
      "Epoch [326/500] Train Loss: 0.0141, Val Loss: 0.0091\n",
      "Epoch [327/500] Train Loss: 0.0139, Val Loss: 0.0093\n",
      "Epoch [328/500] Train Loss: 0.0142, Val Loss: 0.0089\n",
      "Epoch [329/500] Train Loss: 0.0138, Val Loss: 0.0091\n",
      "Epoch [330/500] Train Loss: 0.0138, Val Loss: 0.0090\n",
      "Epoch [331/500] Train Loss: 0.0139, Val Loss: 0.0086\n",
      "Epoch [332/500] Train Loss: 0.0133, Val Loss: 0.0086\n",
      "Epoch [333/500] Train Loss: 0.0131, Val Loss: 0.0089\n",
      "Epoch [334/500] Train Loss: 0.0126, Val Loss: 0.0084\n",
      "Epoch [335/500] Train Loss: 0.0132, Val Loss: 0.0093\n",
      "Epoch [336/500] Train Loss: 0.0125, Val Loss: 0.0081\n",
      "Epoch [337/500] Train Loss: 0.0127, Val Loss: 0.0088\n",
      "Epoch [338/500] Train Loss: 0.0127, Val Loss: 0.0084\n",
      "Epoch [339/500] Train Loss: 0.0124, Val Loss: 0.0080\n",
      "Epoch [340/500] Train Loss: 0.0121, Val Loss: 0.0105\n",
      "Epoch [341/500] Train Loss: 0.0124, Val Loss: 0.0082\n",
      "Epoch [342/500] Train Loss: 0.0124, Val Loss: 0.0104\n",
      "Epoch [343/500] Train Loss: 0.0123, Val Loss: 0.0090\n",
      "Epoch [344/500] Train Loss: 0.0120, Val Loss: 0.0075\n",
      "Epoch [345/500] Train Loss: 0.0110, Val Loss: 0.0081\n",
      "Epoch [346/500] Train Loss: 0.0112, Val Loss: 0.0081\n",
      "Epoch [347/500] Train Loss: 0.0110, Val Loss: 0.0077\n",
      "Epoch [348/500] Train Loss: 0.0107, Val Loss: 0.0075\n",
      "Epoch [349/500] Train Loss: 0.0107, Val Loss: 0.0079\n",
      "Epoch [350/500] Train Loss: 0.0110, Val Loss: 0.0074\n",
      "Epoch [351/500] Train Loss: 0.0106, Val Loss: 0.0077\n",
      "Epoch [352/500] Train Loss: 0.0110, Val Loss: 0.0089\n",
      "Epoch [353/500] Train Loss: 0.0107, Val Loss: 0.0074\n",
      "Epoch [354/500] Train Loss: 0.0101, Val Loss: 0.0078\n",
      "Epoch [355/500] Train Loss: 0.0102, Val Loss: 0.0072\n",
      "Epoch [356/500] Train Loss: 0.0101, Val Loss: 0.0068\n",
      "Epoch [357/500] Train Loss: 0.0102, Val Loss: 0.0067\n",
      "Epoch [358/500] Train Loss: 0.0095, Val Loss: 0.0071\n",
      "Epoch [359/500] Train Loss: 0.0094, Val Loss: 0.0065\n",
      "Epoch [360/500] Train Loss: 0.0095, Val Loss: 0.0069\n",
      "Epoch [361/500] Train Loss: 0.0094, Val Loss: 0.0076\n",
      "Epoch [362/500] Train Loss: 0.0093, Val Loss: 0.0075\n",
      "Epoch [363/500] Train Loss: 0.0091, Val Loss: 0.0083\n",
      "Epoch [364/500] Train Loss: 0.0104, Val Loss: 0.0067\n",
      "Epoch [365/500] Train Loss: 0.0093, Val Loss: 0.0068\n",
      "Epoch [366/500] Train Loss: 0.0089, Val Loss: 0.0062\n",
      "Epoch [367/500] Train Loss: 0.0086, Val Loss: 0.0061\n",
      "Epoch [368/500] Train Loss: 0.0085, Val Loss: 0.0068\n",
      "Epoch [369/500] Train Loss: 0.0085, Val Loss: 0.0064\n",
      "Epoch [370/500] Train Loss: 0.0084, Val Loss: 0.0063\n",
      "Epoch [371/500] Train Loss: 0.0083, Val Loss: 0.0058\n",
      "Epoch [372/500] Train Loss: 0.0081, Val Loss: 0.0059\n",
      "Epoch [373/500] Train Loss: 0.0089, Val Loss: 0.0057\n",
      "Epoch [374/500] Train Loss: 0.0084, Val Loss: 0.0065\n",
      "Epoch [375/500] Train Loss: 0.0085, Val Loss: 0.0061\n",
      "Epoch [376/500] Train Loss: 0.0082, Val Loss: 0.0055\n",
      "Epoch [377/500] Train Loss: 0.0079, Val Loss: 0.0056\n",
      "Epoch [378/500] Train Loss: 0.0077, Val Loss: 0.0056\n",
      "Epoch [379/500] Train Loss: 0.0077, Val Loss: 0.0063\n",
      "Epoch [380/500] Train Loss: 0.0079, Val Loss: 0.0055\n",
      "Epoch [381/500] Train Loss: 0.0079, Val Loss: 0.0051\n",
      "Epoch [382/500] Train Loss: 0.0074, Val Loss: 0.0051\n",
      "Epoch [383/500] Train Loss: 0.0078, Val Loss: 0.0059\n",
      "Epoch [384/500] Train Loss: 0.0076, Val Loss: 0.0053\n",
      "Epoch [385/500] Train Loss: 0.0074, Val Loss: 0.0065\n",
      "Epoch [386/500] Train Loss: 0.0077, Val Loss: 0.0056\n",
      "Epoch [387/500] Train Loss: 0.0074, Val Loss: 0.0047\n",
      "Epoch [388/500] Train Loss: 0.0073, Val Loss: 0.0058\n",
      "Epoch [389/500] Train Loss: 0.0079, Val Loss: 0.0048\n",
      "Epoch [390/500] Train Loss: 0.0071, Val Loss: 0.0047\n",
      "Epoch [391/500] Train Loss: 0.0066, Val Loss: 0.0050\n",
      "Epoch [392/500] Train Loss: 0.0065, Val Loss: 0.0047\n",
      "Epoch [393/500] Train Loss: 0.0066, Val Loss: 0.0045\n",
      "Epoch [394/500] Train Loss: 0.0065, Val Loss: 0.0046\n",
      "Epoch [395/500] Train Loss: 0.0065, Val Loss: 0.0045\n",
      "Epoch [396/500] Train Loss: 0.0064, Val Loss: 0.0047\n",
      "Epoch [397/500] Train Loss: 0.0067, Val Loss: 0.0046\n",
      "Epoch [398/500] Train Loss: 0.0064, Val Loss: 0.0042\n",
      "Epoch [399/500] Train Loss: 0.0061, Val Loss: 0.0047\n",
      "Epoch [400/500] Train Loss: 0.0065, Val Loss: 0.0046\n",
      "Epoch [401/500] Train Loss: 0.0065, Val Loss: 0.0051\n",
      "Epoch [402/500] Train Loss: 0.0061, Val Loss: 0.0039\n",
      "Epoch [403/500] Train Loss: 0.0059, Val Loss: 0.0049\n",
      "Epoch [404/500] Train Loss: 0.0061, Val Loss: 0.0051\n",
      "Epoch [405/500] Train Loss: 0.0063, Val Loss: 0.0051\n",
      "Epoch [406/500] Train Loss: 0.0061, Val Loss: 0.0048\n",
      "Epoch [407/500] Train Loss: 0.0058, Val Loss: 0.0043\n",
      "Epoch [408/500] Train Loss: 0.0057, Val Loss: 0.0042\n",
      "Epoch [409/500] Train Loss: 0.0058, Val Loss: 0.0041\n",
      "Epoch [410/500] Train Loss: 0.0063, Val Loss: 0.0044\n",
      "Epoch [411/500] Train Loss: 0.0058, Val Loss: 0.0037\n",
      "Epoch [412/500] Train Loss: 0.0054, Val Loss: 0.0038\n",
      "Epoch [413/500] Train Loss: 0.0057, Val Loss: 0.0034\n",
      "Epoch [414/500] Train Loss: 0.0053, Val Loss: 0.0039\n",
      "Epoch [415/500] Train Loss: 0.0056, Val Loss: 0.0039\n",
      "Epoch [416/500] Train Loss: 0.0055, Val Loss: 0.0043\n",
      "Epoch [417/500] Train Loss: 0.0055, Val Loss: 0.0036\n",
      "Epoch [418/500] Train Loss: 0.0051, Val Loss: 0.0033\n",
      "Epoch [419/500] Train Loss: 0.0048, Val Loss: 0.0033\n",
      "Epoch [420/500] Train Loss: 0.0049, Val Loss: 0.0034\n",
      "Epoch [421/500] Train Loss: 0.0049, Val Loss: 0.0037\n",
      "Epoch [422/500] Train Loss: 0.0055, Val Loss: 0.0034\n",
      "Epoch [423/500] Train Loss: 0.0061, Val Loss: 0.0040\n",
      "Epoch [424/500] Train Loss: 0.0055, Val Loss: 0.0032\n",
      "Epoch [425/500] Train Loss: 0.0055, Val Loss: 0.0039\n",
      "Epoch [426/500] Train Loss: 0.0050, Val Loss: 0.0030\n",
      "Epoch [427/500] Train Loss: 0.0046, Val Loss: 0.0031\n",
      "Epoch [428/500] Train Loss: 0.0046, Val Loss: 0.0030\n",
      "Epoch [429/500] Train Loss: 0.0045, Val Loss: 0.0030\n",
      "Epoch [430/500] Train Loss: 0.0044, Val Loss: 0.0034\n",
      "Epoch [431/500] Train Loss: 0.0045, Val Loss: 0.0031\n",
      "Epoch [432/500] Train Loss: 0.0046, Val Loss: 0.0034\n",
      "Epoch [433/500] Train Loss: 0.0044, Val Loss: 0.0031\n",
      "Epoch [434/500] Train Loss: 0.0048, Val Loss: 0.0037\n",
      "Epoch [435/500] Train Loss: 0.0048, Val Loss: 0.0035\n",
      "Epoch [436/500] Train Loss: 0.0046, Val Loss: 0.0028\n",
      "Epoch [437/500] Train Loss: 0.0044, Val Loss: 0.0030\n",
      "Epoch [438/500] Train Loss: 0.0044, Val Loss: 0.0033\n",
      "Epoch [439/500] Train Loss: 0.0043, Val Loss: 0.0028\n",
      "Epoch [440/500] Train Loss: 0.0043, Val Loss: 0.0030\n",
      "Epoch [441/500] Train Loss: 0.0042, Val Loss: 0.0031\n",
      "Epoch [442/500] Train Loss: 0.0041, Val Loss: 0.0027\n",
      "Epoch [443/500] Train Loss: 0.0041, Val Loss: 0.0028\n",
      "Epoch [444/500] Train Loss: 0.0041, Val Loss: 0.0029\n",
      "Epoch [445/500] Train Loss: 0.0040, Val Loss: 0.0030\n",
      "Epoch [446/500] Train Loss: 0.0043, Val Loss: 0.0034\n",
      "Epoch [447/500] Train Loss: 0.0043, Val Loss: 0.0027\n",
      "Epoch [448/500] Train Loss: 0.0039, Val Loss: 0.0027\n",
      "Epoch [449/500] Train Loss: 0.0038, Val Loss: 0.0025\n",
      "Epoch [450/500] Train Loss: 0.0037, Val Loss: 0.0024\n",
      "Epoch [451/500] Train Loss: 0.0038, Val Loss: 0.0033\n",
      "Epoch [452/500] Train Loss: 0.0041, Val Loss: 0.0030\n",
      "Epoch [453/500] Train Loss: 0.0038, Val Loss: 0.0025\n",
      "Epoch [454/500] Train Loss: 0.0037, Val Loss: 0.0024\n",
      "Epoch [455/500] Train Loss: 0.0039, Val Loss: 0.0044\n",
      "Epoch [456/500] Train Loss: 0.0043, Val Loss: 0.0037\n",
      "Epoch [457/500] Train Loss: 0.0039, Val Loss: 0.0023\n",
      "Epoch [458/500] Train Loss: 0.0040, Val Loss: 0.0027\n",
      "Epoch [459/500] Train Loss: 0.0040, Val Loss: 0.0046\n",
      "Epoch [460/500] Train Loss: 0.0045, Val Loss: 0.0026\n",
      "Epoch [461/500] Train Loss: 0.0037, Val Loss: 0.0026\n",
      "Epoch [462/500] Train Loss: 0.0035, Val Loss: 0.0025\n",
      "Epoch [463/500] Train Loss: 0.0034, Val Loss: 0.0025\n",
      "Epoch [464/500] Train Loss: 0.0034, Val Loss: 0.0025\n",
      "Epoch [465/500] Train Loss: 0.0034, Val Loss: 0.0026\n",
      "Epoch [466/500] Train Loss: 0.0034, Val Loss: 0.0021\n",
      "Epoch [467/500] Train Loss: 0.0034, Val Loss: 0.0025\n",
      "Epoch [468/500] Train Loss: 0.0034, Val Loss: 0.0026\n",
      "Epoch [469/500] Train Loss: 0.0033, Val Loss: 0.0021\n",
      "Epoch [470/500] Train Loss: 0.0034, Val Loss: 0.0026\n",
      "Epoch [471/500] Train Loss: 0.0033, Val Loss: 0.0024\n",
      "Epoch [472/500] Train Loss: 0.0033, Val Loss: 0.0023\n",
      "Epoch [473/500] Train Loss: 0.0036, Val Loss: 0.0024\n",
      "Epoch [474/500] Train Loss: 0.0032, Val Loss: 0.0025\n",
      "Epoch [475/500] Train Loss: 0.0032, Val Loss: 0.0020\n",
      "Epoch [476/500] Train Loss: 0.0031, Val Loss: 0.0022\n",
      "Epoch [477/500] Train Loss: 0.0033, Val Loss: 0.0028\n",
      "Epoch [478/500] Train Loss: 0.0035, Val Loss: 0.0026\n",
      "Epoch [479/500] Train Loss: 0.0031, Val Loss: 0.0022\n",
      "Epoch [480/500] Train Loss: 0.0031, Val Loss: 0.0024\n",
      "Epoch [481/500] Train Loss: 0.0032, Val Loss: 0.0023\n",
      "Epoch [482/500] Train Loss: 0.0030, Val Loss: 0.0033\n",
      "Epoch [483/500] Train Loss: 0.0035, Val Loss: 0.0022\n",
      "Epoch [484/500] Train Loss: 0.0031, Val Loss: 0.0024\n",
      "Epoch [485/500] Train Loss: 0.0031, Val Loss: 0.0024\n",
      "Epoch [486/500] Train Loss: 0.0031, Val Loss: 0.0021\n",
      "Epoch [487/500] Train Loss: 0.0031, Val Loss: 0.0027\n",
      "Epoch [488/500] Train Loss: 0.0032, Val Loss: 0.0031\n",
      "Epoch [489/500] Train Loss: 0.0031, Val Loss: 0.0036\n",
      "Epoch [490/500] Train Loss: 0.0032, Val Loss: 0.0029\n",
      "Epoch [491/500] Train Loss: 0.0029, Val Loss: 0.0024\n",
      "Epoch [492/500] Train Loss: 0.0030, Val Loss: 0.0033\n",
      "Epoch [493/500] Train Loss: 0.0037, Val Loss: 0.0028\n",
      "Epoch [494/500] Train Loss: 0.0032, Val Loss: 0.0021\n",
      "Epoch [495/500] Train Loss: 0.0027, Val Loss: 0.0020\n",
      "Epoch [496/500] Train Loss: 0.0028, Val Loss: 0.0027\n",
      "Epoch [497/500] Train Loss: 0.0031, Val Loss: 0.0031\n",
      "Epoch [498/500] Train Loss: 0.0032, Val Loss: 0.0026\n",
      "Epoch [499/500] Train Loss: 0.0029, Val Loss: 0.0021\n",
      "Epoch [500/500] Train Loss: 0.0032, Val Loss: 0.0030\n",
      "\n",
      "=== Fold 4 ===\n",
      "Epoch [1/500] Train Loss: 38.9284, Val Loss: 38.9317\n",
      "Epoch [2/500] Train Loss: 38.8869, Val Loss: 38.8918\n",
      "Epoch [3/500] Train Loss: 38.8477, Val Loss: 38.8531\n",
      "Epoch [4/500] Train Loss: 38.8080, Val Loss: 38.8114\n",
      "Epoch [5/500] Train Loss: 38.7636, Val Loss: 38.7634\n",
      "Epoch [6/500] Train Loss: 38.7128, Val Loss: 38.7074\n",
      "Epoch [7/500] Train Loss: 38.6424, Val Loss: 38.5982\n",
      "Epoch [8/500] Train Loss: 38.4772, Val Loss: 38.3327\n",
      "Epoch [9/500] Train Loss: 38.0811, Val Loss: 37.6980\n",
      "Epoch [10/500] Train Loss: 37.1931, Val Loss: 36.3060\n",
      "Epoch [11/500] Train Loss: 35.1514, Val Loss: 33.0893\n",
      "Epoch [12/500] Train Loss: 30.6981, Val Loss: 26.6016\n",
      "Epoch [13/500] Train Loss: 22.6533, Val Loss: 17.2587\n",
      "Epoch [14/500] Train Loss: 14.4465, Val Loss: 13.2778\n",
      "Epoch [15/500] Train Loss: 12.5098, Val Loss: 12.1302\n",
      "Epoch [16/500] Train Loss: 11.7807, Val Loss: 11.4369\n",
      "Epoch [17/500] Train Loss: 11.5824, Val Loss: 11.2746\n",
      "Epoch [18/500] Train Loss: 11.4345, Val Loss: 11.1929\n",
      "Epoch [19/500] Train Loss: 11.3259, Val Loss: 11.0946\n",
      "Epoch [20/500] Train Loss: 11.2229, Val Loss: 11.0335\n",
      "Epoch [21/500] Train Loss: 11.1318, Val Loss: 10.9170\n",
      "Epoch [22/500] Train Loss: 11.0426, Val Loss: 10.8490\n",
      "Epoch [23/500] Train Loss: 10.9658, Val Loss: 10.7879\n",
      "Epoch [24/500] Train Loss: 10.8844, Val Loss: 10.6924\n",
      "Epoch [25/500] Train Loss: 10.8025, Val Loss: 10.6445\n",
      "Epoch [26/500] Train Loss: 10.7319, Val Loss: 10.5773\n",
      "Epoch [27/500] Train Loss: 10.6626, Val Loss: 10.4859\n",
      "Epoch [28/500] Train Loss: 10.5846, Val Loss: 10.4476\n",
      "Epoch [29/500] Train Loss: 10.5216, Val Loss: 10.3867\n",
      "Epoch [30/500] Train Loss: 10.4518, Val Loss: 10.3287\n",
      "Epoch [31/500] Train Loss: 10.3875, Val Loss: 10.2681\n",
      "Epoch [32/500] Train Loss: 10.3250, Val Loss: 10.2135\n",
      "Epoch [33/500] Train Loss: 10.2493, Val Loss: 10.1423\n",
      "Epoch [34/500] Train Loss: 10.1822, Val Loss: 10.1088\n",
      "Epoch [35/500] Train Loss: 10.1088, Val Loss: 10.0423\n",
      "Epoch [36/500] Train Loss: 10.0327, Val Loss: 9.9659\n",
      "Epoch [37/500] Train Loss: 9.9513, Val Loss: 9.8929\n",
      "Epoch [38/500] Train Loss: 9.8647, Val Loss: 9.8289\n",
      "Epoch [39/500] Train Loss: 9.7708, Val Loss: 9.7570\n",
      "Epoch [40/500] Train Loss: 9.6700, Val Loss: 9.6728\n",
      "Epoch [41/500] Train Loss: 9.5699, Val Loss: 9.5957\n",
      "Epoch [42/500] Train Loss: 9.4644, Val Loss: 9.4909\n",
      "Epoch [43/500] Train Loss: 9.3613, Val Loss: 9.4105\n",
      "Epoch [44/500] Train Loss: 9.2609, Val Loss: 9.3297\n",
      "Epoch [45/500] Train Loss: 9.1586, Val Loss: 9.2557\n",
      "Epoch [46/500] Train Loss: 9.0645, Val Loss: 9.1840\n",
      "Epoch [47/500] Train Loss: 8.9777, Val Loss: 9.0988\n",
      "Epoch [48/500] Train Loss: 8.8931, Val Loss: 9.0353\n",
      "Epoch [49/500] Train Loss: 8.8173, Val Loss: 8.9994\n",
      "Epoch [50/500] Train Loss: 8.7450, Val Loss: 8.9198\n",
      "Epoch [51/500] Train Loss: 8.6777, Val Loss: 8.8565\n",
      "Epoch [52/500] Train Loss: 8.6196, Val Loss: 8.7992\n",
      "Epoch [53/500] Train Loss: 8.5652, Val Loss: 8.7689\n",
      "Epoch [54/500] Train Loss: 8.5115, Val Loss: 8.7168\n",
      "Epoch [55/500] Train Loss: 8.4634, Val Loss: 8.6762\n",
      "Epoch [56/500] Train Loss: 8.4229, Val Loss: 8.6568\n",
      "Epoch [57/500] Train Loss: 8.3808, Val Loss: 8.5956\n",
      "Epoch [58/500] Train Loss: 8.3426, Val Loss: 8.5549\n",
      "Epoch [59/500] Train Loss: 8.3086, Val Loss: 8.5270\n",
      "Epoch [60/500] Train Loss: 8.2782, Val Loss: 8.4767\n",
      "Epoch [61/500] Train Loss: 8.2495, Val Loss: 8.4592\n",
      "Epoch [62/500] Train Loss: 8.2221, Val Loss: 8.4401\n",
      "Epoch [63/500] Train Loss: 8.2005, Val Loss: 8.4233\n",
      "Epoch [64/500] Train Loss: 8.1794, Val Loss: 8.3904\n",
      "Epoch [65/500] Train Loss: 8.1602, Val Loss: 8.3641\n",
      "Epoch [66/500] Train Loss: 8.1447, Val Loss: 8.3427\n",
      "Epoch [67/500] Train Loss: 8.1289, Val Loss: 8.3383\n",
      "Epoch [68/500] Train Loss: 8.1162, Val Loss: 8.3229\n",
      "Epoch [69/500] Train Loss: 8.1057, Val Loss: 8.2935\n",
      "Epoch [70/500] Train Loss: 8.0964, Val Loss: 8.2800\n",
      "Epoch [71/500] Train Loss: 8.0879, Val Loss: 8.2787\n",
      "Epoch [72/500] Train Loss: 8.0792, Val Loss: 8.2644\n",
      "Epoch [73/500] Train Loss: 8.0707, Val Loss: 8.2613\n",
      "Epoch [74/500] Train Loss: 8.0648, Val Loss: 8.2566\n",
      "Epoch [75/500] Train Loss: 8.0596, Val Loss: 8.2472\n",
      "Epoch [76/500] Train Loss: 8.0555, Val Loss: 8.2164\n",
      "Epoch [77/500] Train Loss: 8.0488, Val Loss: 8.2349\n",
      "Epoch [78/500] Train Loss: 8.0455, Val Loss: 8.2277\n",
      "Epoch [79/500] Train Loss: 8.0421, Val Loss: 8.2305\n",
      "Epoch [80/500] Train Loss: 8.0403, Val Loss: 8.2046\n",
      "Epoch [81/500] Train Loss: 8.0346, Val Loss: 8.2117\n",
      "Epoch [82/500] Train Loss: 8.0311, Val Loss: 8.1968\n",
      "Epoch [83/500] Train Loss: 8.0281, Val Loss: 8.1918\n",
      "Epoch [84/500] Train Loss: 8.0254, Val Loss: 8.1822\n",
      "Epoch [85/500] Train Loss: 8.0222, Val Loss: 8.1745\n",
      "Epoch [86/500] Train Loss: 8.0181, Val Loss: 8.1821\n",
      "Epoch [87/500] Train Loss: 8.0151, Val Loss: 8.1733\n",
      "Epoch [88/500] Train Loss: 8.0137, Val Loss: 8.1670\n",
      "Epoch [89/500] Train Loss: 8.0128, Val Loss: 8.1737\n",
      "Epoch [90/500] Train Loss: 8.0086, Val Loss: 8.1662\n",
      "Epoch [91/500] Train Loss: 8.0068, Val Loss: 8.1597\n",
      "Epoch [92/500] Train Loss: 8.0049, Val Loss: 8.1687\n",
      "Epoch [93/500] Train Loss: 8.0037, Val Loss: 8.1656\n",
      "Epoch [94/500] Train Loss: 8.0014, Val Loss: 8.1528\n",
      "Epoch [95/500] Train Loss: 7.9987, Val Loss: 8.1564\n",
      "Epoch [96/500] Train Loss: 7.9985, Val Loss: 8.1625\n",
      "Epoch [97/500] Train Loss: 7.9959, Val Loss: 8.1573\n",
      "Epoch [98/500] Train Loss: 7.9956, Val Loss: 8.1473\n",
      "Epoch [99/500] Train Loss: 7.9914, Val Loss: 8.1604\n",
      "Epoch [100/500] Train Loss: 7.9916, Val Loss: 8.1539\n",
      "Epoch [101/500] Train Loss: 7.9893, Val Loss: 8.1385\n",
      "Epoch [102/500] Train Loss: 7.9883, Val Loss: 8.1405\n",
      "Epoch [103/500] Train Loss: 7.9873, Val Loss: 8.1468\n",
      "Epoch [104/500] Train Loss: 7.9856, Val Loss: 8.1367\n",
      "Epoch [105/500] Train Loss: 7.9845, Val Loss: 8.1405\n",
      "Epoch [106/500] Train Loss: 7.9837, Val Loss: 8.1678\n",
      "Epoch [107/500] Train Loss: 7.9837, Val Loss: 8.1457\n",
      "Epoch [108/500] Train Loss: 7.9810, Val Loss: 8.1168\n",
      "Epoch [109/500] Train Loss: 7.9804, Val Loss: 8.1384\n",
      "Epoch [110/500] Train Loss: 7.9815, Val Loss: 8.1277\n",
      "Epoch [111/500] Train Loss: 7.9803, Val Loss: 8.1482\n",
      "Epoch [112/500] Train Loss: 7.9763, Val Loss: 8.1327\n",
      "Epoch [113/500] Train Loss: 7.9776, Val Loss: 8.1202\n",
      "Epoch [114/500] Train Loss: 7.9771, Val Loss: 8.1415\n",
      "Epoch [115/500] Train Loss: 7.9765, Val Loss: 8.1263\n",
      "Epoch [116/500] Train Loss: 7.9752, Val Loss: 8.1357\n",
      "Epoch [117/500] Train Loss: 7.9724, Val Loss: 8.1273\n",
      "Epoch [118/500] Train Loss: 7.9717, Val Loss: 8.1204\n",
      "Epoch [119/500] Train Loss: 7.9711, Val Loss: 8.1297\n",
      "Epoch [120/500] Train Loss: 7.9700, Val Loss: 8.1267\n",
      "Epoch [121/500] Train Loss: 7.9697, Val Loss: 8.1233\n",
      "Epoch [122/500] Train Loss: 7.9687, Val Loss: 8.1329\n",
      "Epoch [123/500] Train Loss: 7.9697, Val Loss: 8.1230\n",
      "Epoch [124/500] Train Loss: 7.9677, Val Loss: 8.1162\n",
      "Epoch [125/500] Train Loss: 7.9694, Val Loss: 8.1402\n",
      "Epoch [126/500] Train Loss: 7.9667, Val Loss: 8.1336\n",
      "Epoch [127/500] Train Loss: 7.9659, Val Loss: 8.1161\n",
      "Epoch [128/500] Train Loss: 7.9654, Val Loss: 8.1268\n",
      "Epoch [129/500] Train Loss: 7.9644, Val Loss: 8.1219\n",
      "Epoch [130/500] Train Loss: 7.9646, Val Loss: 8.1285\n",
      "Epoch [131/500] Train Loss: 7.9640, Val Loss: 8.1195\n",
      "Epoch [132/500] Train Loss: 7.9636, Val Loss: 8.1220\n",
      "Epoch [133/500] Train Loss: 7.9630, Val Loss: 8.1307\n",
      "Epoch [134/500] Train Loss: 7.9641, Val Loss: 8.1078\n",
      "Epoch [135/500] Train Loss: 7.9612, Val Loss: 8.1332\n",
      "Epoch [136/500] Train Loss: 7.9612, Val Loss: 8.1256\n",
      "Epoch [137/500] Train Loss: 7.9616, Val Loss: 8.1213\n",
      "Epoch [138/500] Train Loss: 7.9641, Val Loss: 8.1063\n",
      "Epoch [139/500] Train Loss: 7.9595, Val Loss: 8.1243\n",
      "Epoch [140/500] Train Loss: 7.9597, Val Loss: 8.1250\n",
      "Epoch [141/500] Train Loss: 7.9611, Val Loss: 8.1157\n",
      "Epoch [142/500] Train Loss: 7.9598, Val Loss: 8.1107\n",
      "Epoch [143/500] Train Loss: 7.9578, Val Loss: 8.1205\n",
      "Epoch [144/500] Train Loss: 7.9595, Val Loss: 8.1055\n",
      "Epoch [145/500] Train Loss: 7.9594, Val Loss: 8.1278\n",
      "Epoch [146/500] Train Loss: 7.9580, Val Loss: 8.1050\n",
      "Epoch [147/500] Train Loss: 7.9572, Val Loss: 8.1073\n",
      "Epoch [148/500] Train Loss: 7.9564, Val Loss: 8.1315\n",
      "Epoch [149/500] Train Loss: 7.9565, Val Loss: 8.1179\n",
      "Epoch [150/500] Train Loss: 7.9564, Val Loss: 8.1117\n",
      "Epoch [151/500] Train Loss: 7.9582, Val Loss: 8.1287\n",
      "Epoch [152/500] Train Loss: 7.9536, Val Loss: 8.0986\n",
      "Epoch [153/500] Train Loss: 7.9577, Val Loss: 8.0916\n",
      "Epoch [154/500] Train Loss: 7.9569, Val Loss: 8.1149\n",
      "Epoch [155/500] Train Loss: 7.9532, Val Loss: 8.1088\n",
      "Epoch [156/500] Train Loss: 7.9530, Val Loss: 8.1045\n",
      "Epoch [157/500] Train Loss: 7.9541, Val Loss: 8.0922\n",
      "Epoch [158/500] Train Loss: 7.9528, Val Loss: 8.1092\n",
      "Epoch [159/500] Train Loss: 7.9538, Val Loss: 8.0980\n",
      "Epoch [160/500] Train Loss: 7.9519, Val Loss: 8.1090\n",
      "Epoch [161/500] Train Loss: 7.9524, Val Loss: 8.0937\n",
      "Epoch [162/500] Train Loss: 7.9545, Val Loss: 8.1144\n",
      "Epoch [163/500] Train Loss: 7.9510, Val Loss: 8.0994\n",
      "Epoch [164/500] Train Loss: 7.9497, Val Loss: 8.1118\n",
      "Epoch [165/500] Train Loss: 7.9505, Val Loss: 8.1098\n",
      "Epoch [166/500] Train Loss: 7.9497, Val Loss: 8.1138\n",
      "Epoch [167/500] Train Loss: 7.9526, Val Loss: 8.0886\n",
      "Epoch [168/500] Train Loss: 7.9478, Val Loss: 8.1097\n",
      "Epoch [169/500] Train Loss: 7.9518, Val Loss: 8.1273\n",
      "Epoch [170/500] Train Loss: 7.9517, Val Loss: 8.0859\n",
      "Epoch [171/500] Train Loss: 7.9487, Val Loss: 8.0895\n",
      "Epoch [172/500] Train Loss: 7.9570, Val Loss: 8.1382\n",
      "Epoch [173/500] Train Loss: 7.9476, Val Loss: 8.1027\n",
      "Epoch [174/500] Train Loss: 7.9464, Val Loss: 8.0967\n",
      "Epoch [175/500] Train Loss: 7.9466, Val Loss: 8.0869\n",
      "Epoch [176/500] Train Loss: 7.9445, Val Loss: 8.1098\n",
      "Epoch [177/500] Train Loss: 7.9460, Val Loss: 8.1010\n",
      "Epoch [178/500] Train Loss: 7.9462, Val Loss: 8.0930\n",
      "Epoch [179/500] Train Loss: 7.9440, Val Loss: 8.1076\n",
      "Epoch [180/500] Train Loss: 7.9433, Val Loss: 8.0981\n",
      "Epoch [181/500] Train Loss: 7.9425, Val Loss: 8.0869\n",
      "Epoch [182/500] Train Loss: 7.9434, Val Loss: 8.0893\n",
      "Epoch [183/500] Train Loss: 7.9436, Val Loss: 8.0957\n",
      "Epoch [184/500] Train Loss: 7.9414, Val Loss: 8.1027\n",
      "Epoch [185/500] Train Loss: 7.9420, Val Loss: 8.0773\n",
      "Epoch [186/500] Train Loss: 7.9403, Val Loss: 8.0958\n",
      "Epoch [187/500] Train Loss: 7.9383, Val Loss: 8.0840\n",
      "Epoch [188/500] Train Loss: 7.9389, Val Loss: 8.0792\n",
      "Epoch [189/500] Train Loss: 7.9360, Val Loss: 8.0926\n",
      "Epoch [190/500] Train Loss: 7.9355, Val Loss: 8.1019\n",
      "Epoch [191/500] Train Loss: 7.9339, Val Loss: 8.0812\n",
      "Epoch [192/500] Train Loss: 7.9309, Val Loss: 8.0741\n",
      "Epoch [193/500] Train Loss: 7.9288, Val Loss: 8.0827\n",
      "Epoch [194/500] Train Loss: 7.9275, Val Loss: 8.0831\n",
      "Epoch [195/500] Train Loss: 7.9266, Val Loss: 8.0699\n",
      "Epoch [196/500] Train Loss: 7.9226, Val Loss: 8.0875\n",
      "Epoch [197/500] Train Loss: 7.9219, Val Loss: 8.0703\n",
      "Epoch [198/500] Train Loss: 7.9164, Val Loss: 8.0569\n",
      "Epoch [199/500] Train Loss: 7.9122, Val Loss: 8.0661\n",
      "Epoch [200/500] Train Loss: 7.9101, Val Loss: 8.0682\n",
      "Epoch [201/500] Train Loss: 7.9107, Val Loss: 8.0246\n",
      "Epoch [202/500] Train Loss: 7.8967, Val Loss: 8.0512\n",
      "Epoch [203/500] Train Loss: 7.8887, Val Loss: 8.0204\n",
      "Epoch [204/500] Train Loss: 7.8780, Val Loss: 7.9936\n",
      "Epoch [205/500] Train Loss: 7.8454, Val Loss: 7.9706\n",
      "Epoch [206/500] Train Loss: 7.8171, Val Loss: 7.9359\n",
      "Epoch [207/500] Train Loss: 7.7931, Val Loss: 7.9090\n",
      "Epoch [208/500] Train Loss: 7.7222, Val Loss: 7.7910\n",
      "Epoch [209/500] Train Loss: 7.6599, Val Loss: 7.7698\n",
      "Epoch [210/500] Train Loss: 7.5452, Val Loss: 7.5725\n",
      "Epoch [211/500] Train Loss: 7.3849, Val Loss: 7.4102\n",
      "Epoch [212/500] Train Loss: 7.1825, Val Loss: 7.1743\n",
      "Epoch [213/500] Train Loss: 6.9288, Val Loss: 6.9173\n",
      "Epoch [214/500] Train Loss: 6.6279, Val Loss: 6.5818\n",
      "Epoch [215/500] Train Loss: 6.2387, Val Loss: 6.1440\n",
      "Epoch [216/500] Train Loss: 5.8465, Val Loss: 5.8124\n",
      "Epoch [217/500] Train Loss: 5.5000, Val Loss: 5.4772\n",
      "Epoch [218/500] Train Loss: 5.1940, Val Loss: 5.2994\n",
      "Epoch [219/500] Train Loss: 5.0549, Val Loss: 5.1311\n",
      "Epoch [220/500] Train Loss: 4.9259, Val Loss: 4.9696\n",
      "Epoch [221/500] Train Loss: 4.8160, Val Loss: 4.8739\n",
      "Epoch [222/500] Train Loss: 4.7229, Val Loss: 4.8303\n",
      "Epoch [223/500] Train Loss: 4.6333, Val Loss: 4.7407\n",
      "Epoch [224/500] Train Loss: 4.5664, Val Loss: 4.7181\n",
      "Epoch [225/500] Train Loss: 4.5242, Val Loss: 4.6543\n",
      "Epoch [226/500] Train Loss: 4.4751, Val Loss: 4.6080\n",
      "Epoch [227/500] Train Loss: 4.4286, Val Loss: 4.5887\n",
      "Epoch [228/500] Train Loss: 4.3853, Val Loss: 4.5281\n",
      "Epoch [229/500] Train Loss: 4.3229, Val Loss: 4.4814\n",
      "Epoch [230/500] Train Loss: 4.2736, Val Loss: 4.4446\n",
      "Epoch [231/500] Train Loss: 4.2420, Val Loss: 4.4389\n",
      "Epoch [232/500] Train Loss: 4.2048, Val Loss: 4.3965\n",
      "Epoch [233/500] Train Loss: 4.1764, Val Loss: 4.3728\n",
      "Epoch [234/500] Train Loss: 4.1497, Val Loss: 4.3722\n",
      "Epoch [235/500] Train Loss: 4.1324, Val Loss: 4.3385\n",
      "Epoch [236/500] Train Loss: 4.1139, Val Loss: 4.3519\n",
      "Epoch [237/500] Train Loss: 4.0945, Val Loss: 4.3443\n",
      "Epoch [238/500] Train Loss: 4.0819, Val Loss: 4.3031\n",
      "Epoch [239/500] Train Loss: 4.0669, Val Loss: 4.2988\n",
      "Epoch [240/500] Train Loss: 4.0532, Val Loss: 4.2838\n",
      "Epoch [241/500] Train Loss: 4.0403, Val Loss: 4.3010\n",
      "Epoch [242/500] Train Loss: 4.0287, Val Loss: 4.2689\n",
      "Epoch [243/500] Train Loss: 4.0251, Val Loss: 4.2835\n",
      "Epoch [244/500] Train Loss: 4.0115, Val Loss: 4.2528\n",
      "Epoch [245/500] Train Loss: 3.9952, Val Loss: 4.2718\n",
      "Epoch [246/500] Train Loss: 3.9867, Val Loss: 4.2421\n",
      "Epoch [247/500] Train Loss: 3.9895, Val Loss: 4.2750\n",
      "Epoch [248/500] Train Loss: 3.9781, Val Loss: 4.2300\n",
      "Epoch [249/500] Train Loss: 3.9728, Val Loss: 4.2358\n",
      "Epoch [250/500] Train Loss: 3.9539, Val Loss: 4.2315\n",
      "Epoch [251/500] Train Loss: 3.9430, Val Loss: 4.2028\n",
      "Epoch [252/500] Train Loss: 3.9392, Val Loss: 4.2124\n",
      "Epoch [253/500] Train Loss: 3.9299, Val Loss: 4.2364\n",
      "Epoch [254/500] Train Loss: 3.9194, Val Loss: 4.2267\n",
      "Epoch [255/500] Train Loss: 3.9086, Val Loss: 4.2076\n",
      "Epoch [256/500] Train Loss: 3.8994, Val Loss: 4.2170\n",
      "Epoch [257/500] Train Loss: 3.8948, Val Loss: 4.1774\n",
      "Epoch [258/500] Train Loss: 3.8783, Val Loss: 4.1772\n",
      "Epoch [259/500] Train Loss: 3.8705, Val Loss: 4.1766\n",
      "Epoch [260/500] Train Loss: 3.8579, Val Loss: 4.1567\n",
      "Epoch [261/500] Train Loss: 3.8476, Val Loss: 4.1621\n",
      "Epoch [262/500] Train Loss: 3.8347, Val Loss: 4.1644\n",
      "Epoch [263/500] Train Loss: 3.8250, Val Loss: 4.1418\n",
      "Epoch [264/500] Train Loss: 3.8124, Val Loss: 4.1220\n",
      "Epoch [265/500] Train Loss: 3.8046, Val Loss: 4.1393\n",
      "Epoch [266/500] Train Loss: 3.7889, Val Loss: 4.1143\n",
      "Epoch [267/500] Train Loss: 3.7792, Val Loss: 4.1300\n",
      "Epoch [268/500] Train Loss: 3.7764, Val Loss: 4.0784\n",
      "Epoch [269/500] Train Loss: 3.7535, Val Loss: 4.1230\n",
      "Epoch [270/500] Train Loss: 3.7438, Val Loss: 4.0536\n",
      "Epoch [271/500] Train Loss: 3.7319, Val Loss: 4.0631\n",
      "Epoch [272/500] Train Loss: 3.7150, Val Loss: 4.1017\n",
      "Epoch [273/500] Train Loss: 3.7060, Val Loss: 4.0259\n",
      "Epoch [274/500] Train Loss: 3.6881, Val Loss: 4.0937\n",
      "Epoch [275/500] Train Loss: 3.6831, Val Loss: 3.9995\n",
      "Epoch [276/500] Train Loss: 3.6591, Val Loss: 3.9879\n",
      "Epoch [277/500] Train Loss: 3.6388, Val Loss: 4.0035\n",
      "Epoch [278/500] Train Loss: 3.6214, Val Loss: 3.9842\n",
      "Epoch [279/500] Train Loss: 3.6051, Val Loss: 3.9299\n",
      "Epoch [280/500] Train Loss: 3.5842, Val Loss: 3.9614\n",
      "Epoch [281/500] Train Loss: 3.5655, Val Loss: 3.9122\n",
      "Epoch [282/500] Train Loss: 3.5401, Val Loss: 3.8600\n",
      "Epoch [283/500] Train Loss: 3.5308, Val Loss: 3.9204\n",
      "Epoch [284/500] Train Loss: 3.5058, Val Loss: 3.8366\n",
      "Epoch [285/500] Train Loss: 3.4829, Val Loss: 3.8022\n",
      "Epoch [286/500] Train Loss: 3.4606, Val Loss: 3.8164\n",
      "Epoch [287/500] Train Loss: 3.4398, Val Loss: 3.7450\n",
      "Epoch [288/500] Train Loss: 3.4246, Val Loss: 3.7176\n",
      "Epoch [289/500] Train Loss: 3.4047, Val Loss: 3.7962\n",
      "Epoch [290/500] Train Loss: 3.3790, Val Loss: 3.6386\n",
      "Epoch [291/500] Train Loss: 3.3545, Val Loss: 3.6279\n",
      "Epoch [292/500] Train Loss: 3.3336, Val Loss: 3.6071\n",
      "Epoch [293/500] Train Loss: 3.3163, Val Loss: 3.5723\n",
      "Epoch [294/500] Train Loss: 3.3029, Val Loss: 3.5366\n",
      "Epoch [295/500] Train Loss: 3.2933, Val Loss: 3.5904\n",
      "Epoch [296/500] Train Loss: 3.2672, Val Loss: 3.5134\n",
      "Epoch [297/500] Train Loss: 3.2473, Val Loss: 3.4753\n",
      "Epoch [298/500] Train Loss: 3.2371, Val Loss: 3.4546\n",
      "Epoch [299/500] Train Loss: 3.2232, Val Loss: 3.4728\n",
      "Epoch [300/500] Train Loss: 3.2167, Val Loss: 3.4024\n",
      "Epoch [301/500] Train Loss: 3.1958, Val Loss: 3.3790\n",
      "Epoch [302/500] Train Loss: 3.1796, Val Loss: 3.3646\n",
      "Epoch [303/500] Train Loss: 3.1618, Val Loss: 3.3782\n",
      "Epoch [304/500] Train Loss: 3.1530, Val Loss: 3.3438\n",
      "Epoch [305/500] Train Loss: 3.1411, Val Loss: 3.3338\n",
      "Epoch [306/500] Train Loss: 3.1470, Val Loss: 3.3092\n",
      "Epoch [307/500] Train Loss: 3.1175, Val Loss: 3.2784\n",
      "Epoch [308/500] Train Loss: 3.1049, Val Loss: 3.2676\n",
      "Epoch [309/500] Train Loss: 3.1041, Val Loss: 3.2912\n",
      "Epoch [310/500] Train Loss: 3.0975, Val Loss: 3.2659\n",
      "Epoch [311/500] Train Loss: 3.0803, Val Loss: 3.2216\n",
      "Epoch [312/500] Train Loss: 3.0723, Val Loss: 3.1967\n",
      "Epoch [313/500] Train Loss: 3.0572, Val Loss: 3.2132\n",
      "Epoch [314/500] Train Loss: 3.0428, Val Loss: 3.1906\n",
      "Epoch [315/500] Train Loss: 3.0348, Val Loss: 3.1486\n",
      "Epoch [316/500] Train Loss: 3.0228, Val Loss: 3.1423\n",
      "Epoch [317/500] Train Loss: 3.0134, Val Loss: 3.1253\n",
      "Epoch [318/500] Train Loss: 2.9999, Val Loss: 3.1169\n",
      "Epoch [319/500] Train Loss: 2.9862, Val Loss: 3.1143\n",
      "Epoch [320/500] Train Loss: 2.9799, Val Loss: 3.0940\n",
      "Epoch [321/500] Train Loss: 2.9668, Val Loss: 3.0714\n",
      "Epoch [322/500] Train Loss: 2.9686, Val Loss: 3.0942\n",
      "Epoch [323/500] Train Loss: 2.9515, Val Loss: 3.0598\n",
      "Epoch [324/500] Train Loss: 2.9458, Val Loss: 3.0511\n",
      "Epoch [325/500] Train Loss: 2.9298, Val Loss: 3.0536\n",
      "Epoch [326/500] Train Loss: 2.9270, Val Loss: 3.0279\n",
      "Epoch [327/500] Train Loss: 2.9137, Val Loss: 3.0168\n",
      "Epoch [328/500] Train Loss: 2.9072, Val Loss: 2.9940\n",
      "Epoch [329/500] Train Loss: 2.9077, Val Loss: 3.0203\n",
      "Epoch [330/500] Train Loss: 2.8983, Val Loss: 2.9766\n",
      "Epoch [331/500] Train Loss: 2.8824, Val Loss: 3.0129\n",
      "Epoch [332/500] Train Loss: 2.8757, Val Loss: 2.9714\n",
      "Epoch [333/500] Train Loss: 2.8775, Val Loss: 2.9519\n",
      "Epoch [334/500] Train Loss: 2.8567, Val Loss: 2.9788\n",
      "Epoch [335/500] Train Loss: 2.8606, Val Loss: 2.9632\n",
      "Epoch [336/500] Train Loss: 2.8487, Val Loss: 2.9274\n",
      "Epoch [337/500] Train Loss: 2.8355, Val Loss: 2.9256\n",
      "Epoch [338/500] Train Loss: 2.8333, Val Loss: 2.9229\n",
      "Epoch [339/500] Train Loss: 2.8444, Val Loss: 2.9097\n",
      "Epoch [340/500] Train Loss: 2.8294, Val Loss: 2.9003\n",
      "Epoch [341/500] Train Loss: 2.8385, Val Loss: 2.9475\n",
      "Epoch [342/500] Train Loss: 2.8391, Val Loss: 2.8836\n",
      "Epoch [343/500] Train Loss: 2.8103, Val Loss: 2.9046\n",
      "Epoch [344/500] Train Loss: 2.8105, Val Loss: 2.9271\n",
      "Epoch [345/500] Train Loss: 2.8023, Val Loss: 2.8573\n",
      "Epoch [346/500] Train Loss: 2.7772, Val Loss: 2.8601\n",
      "Epoch [347/500] Train Loss: 2.7707, Val Loss: 2.8321\n",
      "Epoch [348/500] Train Loss: 2.7626, Val Loss: 2.8394\n",
      "Epoch [349/500] Train Loss: 2.7704, Val Loss: 2.8250\n",
      "Epoch [350/500] Train Loss: 2.7469, Val Loss: 2.8288\n",
      "Epoch [351/500] Train Loss: 2.7512, Val Loss: 2.8072\n",
      "Epoch [352/500] Train Loss: 2.7531, Val Loss: 2.8707\n",
      "Epoch [353/500] Train Loss: 2.7462, Val Loss: 2.7836\n",
      "Epoch [354/500] Train Loss: 2.7305, Val Loss: 2.7883\n",
      "Epoch [355/500] Train Loss: 2.7450, Val Loss: 2.7955\n",
      "Epoch [356/500] Train Loss: 2.7379, Val Loss: 2.7674\n",
      "Epoch [357/500] Train Loss: 2.7155, Val Loss: 2.7753\n",
      "Epoch [358/500] Train Loss: 2.7144, Val Loss: 2.7666\n",
      "Epoch [359/500] Train Loss: 2.7052, Val Loss: 2.7612\n",
      "Epoch [360/500] Train Loss: 2.6991, Val Loss: 2.7494\n",
      "Epoch [361/500] Train Loss: 2.6999, Val Loss: 2.7794\n",
      "Epoch [362/500] Train Loss: 2.6978, Val Loss: 2.7617\n",
      "Epoch [363/500] Train Loss: 2.6862, Val Loss: 2.7607\n",
      "Epoch [364/500] Train Loss: 2.7051, Val Loss: 2.7479\n",
      "Epoch [365/500] Train Loss: 2.7090, Val Loss: 2.7583\n",
      "Epoch [366/500] Train Loss: 2.6827, Val Loss: 2.7569\n",
      "Epoch [367/500] Train Loss: 2.6814, Val Loss: 2.7154\n",
      "Epoch [368/500] Train Loss: 2.6791, Val Loss: 2.7155\n",
      "Epoch [369/500] Train Loss: 2.6622, Val Loss: 2.7131\n",
      "Epoch [370/500] Train Loss: 2.6757, Val Loss: 2.8308\n",
      "Epoch [371/500] Train Loss: 2.6794, Val Loss: 2.7239\n",
      "Epoch [372/500] Train Loss: 2.6545, Val Loss: 2.7132\n",
      "Epoch [373/500] Train Loss: 2.6437, Val Loss: 2.6922\n",
      "Epoch [374/500] Train Loss: 2.6395, Val Loss: 2.6830\n",
      "Epoch [375/500] Train Loss: 2.6353, Val Loss: 2.6828\n",
      "Epoch [376/500] Train Loss: 2.6341, Val Loss: 2.6764\n",
      "Epoch [377/500] Train Loss: 2.6525, Val Loss: 2.7178\n",
      "Epoch [378/500] Train Loss: 2.6339, Val Loss: 2.6727\n",
      "Epoch [379/500] Train Loss: 2.6377, Val Loss: 2.6623\n",
      "Epoch [380/500] Train Loss: 2.6363, Val Loss: 2.7466\n",
      "Epoch [381/500] Train Loss: 2.6203, Val Loss: 2.6505\n",
      "Epoch [382/500] Train Loss: 2.6048, Val Loss: 2.6435\n",
      "Epoch [383/500] Train Loss: 2.5971, Val Loss: 2.6289\n",
      "Epoch [384/500] Train Loss: 2.5955, Val Loss: 2.6679\n",
      "Epoch [385/500] Train Loss: 2.5927, Val Loss: 2.6312\n",
      "Epoch [386/500] Train Loss: 2.5800, Val Loss: 2.6450\n",
      "Epoch [387/500] Train Loss: 2.5752, Val Loss: 2.6302\n",
      "Epoch [388/500] Train Loss: 2.5757, Val Loss: 2.6325\n",
      "Epoch [389/500] Train Loss: 2.5754, Val Loss: 2.6166\n",
      "Epoch [390/500] Train Loss: 2.5605, Val Loss: 2.6238\n",
      "Epoch [391/500] Train Loss: 2.5652, Val Loss: 2.6078\n",
      "Epoch [392/500] Train Loss: 2.5625, Val Loss: 2.6068\n",
      "Epoch [393/500] Train Loss: 2.5581, Val Loss: 2.6050\n",
      "Epoch [394/500] Train Loss: 2.5431, Val Loss: 2.5996\n",
      "Epoch [395/500] Train Loss: 2.5531, Val Loss: 2.5977\n",
      "Epoch [396/500] Train Loss: 2.5493, Val Loss: 2.5945\n",
      "Epoch [397/500] Train Loss: 2.5381, Val Loss: 2.5806\n",
      "Epoch [398/500] Train Loss: 2.5305, Val Loss: 2.5672\n",
      "Epoch [399/500] Train Loss: 2.5306, Val Loss: 2.5821\n",
      "Epoch [400/500] Train Loss: 2.5303, Val Loss: 2.6057\n",
      "Epoch [401/500] Train Loss: 2.5290, Val Loss: 2.5645\n",
      "Epoch [402/500] Train Loss: 2.5270, Val Loss: 2.5525\n",
      "Epoch [403/500] Train Loss: 2.5136, Val Loss: 2.5475\n",
      "Epoch [404/500] Train Loss: 2.5119, Val Loss: 2.5858\n",
      "Epoch [405/500] Train Loss: 2.5359, Val Loss: 2.6135\n",
      "Epoch [406/500] Train Loss: 2.5166, Val Loss: 2.5484\n",
      "Epoch [407/500] Train Loss: 2.4981, Val Loss: 2.5411\n",
      "Epoch [408/500] Train Loss: 2.4939, Val Loss: 2.5561\n",
      "Epoch [409/500] Train Loss: 2.5026, Val Loss: 2.5240\n",
      "Epoch [410/500] Train Loss: 2.4901, Val Loss: 2.5242\n",
      "Epoch [411/500] Train Loss: 2.4828, Val Loss: 2.5136\n",
      "Epoch [412/500] Train Loss: 2.4835, Val Loss: 2.5254\n",
      "Epoch [413/500] Train Loss: 2.4749, Val Loss: 2.5147\n",
      "Epoch [414/500] Train Loss: 2.4740, Val Loss: 2.5325\n",
      "Epoch [415/500] Train Loss: 2.4785, Val Loss: 2.5167\n",
      "Epoch [416/500] Train Loss: 2.4633, Val Loss: 2.4935\n",
      "Epoch [417/500] Train Loss: 2.4534, Val Loss: 2.5062\n",
      "Epoch [418/500] Train Loss: 2.4531, Val Loss: 2.4883\n",
      "Epoch [419/500] Train Loss: 2.4546, Val Loss: 2.5097\n",
      "Epoch [420/500] Train Loss: 2.4568, Val Loss: 2.4735\n",
      "Epoch [421/500] Train Loss: 2.4393, Val Loss: 2.4808\n",
      "Epoch [422/500] Train Loss: 2.4414, Val Loss: 2.4725\n",
      "Epoch [423/500] Train Loss: 2.4594, Val Loss: 2.4737\n",
      "Epoch [424/500] Train Loss: 2.4498, Val Loss: 2.4635\n",
      "Epoch [425/500] Train Loss: 2.4329, Val Loss: 2.4733\n",
      "Epoch [426/500] Train Loss: 2.4199, Val Loss: 2.4962\n",
      "Epoch [427/500] Train Loss: 2.4294, Val Loss: 2.4931\n",
      "Epoch [428/500] Train Loss: 2.4290, Val Loss: 2.4556\n",
      "Epoch [429/500] Train Loss: 2.4262, Val Loss: 2.4646\n",
      "Epoch [430/500] Train Loss: 2.4305, Val Loss: 2.5468\n",
      "Epoch [431/500] Train Loss: 2.4465, Val Loss: 2.4632\n",
      "Epoch [432/500] Train Loss: 2.4056, Val Loss: 2.4277\n",
      "Epoch [433/500] Train Loss: 2.3931, Val Loss: 2.4263\n",
      "Epoch [434/500] Train Loss: 2.3895, Val Loss: 2.4217\n",
      "Epoch [435/500] Train Loss: 2.3925, Val Loss: 2.4428\n",
      "Epoch [436/500] Train Loss: 2.3919, Val Loss: 2.4166\n",
      "Epoch [437/500] Train Loss: 2.3859, Val Loss: 2.4616\n",
      "Epoch [438/500] Train Loss: 2.3828, Val Loss: 2.4165\n",
      "Epoch [439/500] Train Loss: 2.3903, Val Loss: 2.4277\n",
      "Epoch [440/500] Train Loss: 2.3808, Val Loss: 2.4567\n",
      "Epoch [441/500] Train Loss: 2.3773, Val Loss: 2.4129\n",
      "Epoch [442/500] Train Loss: 2.3742, Val Loss: 2.4054\n",
      "Epoch [443/500] Train Loss: 2.3621, Val Loss: 2.4147\n",
      "Epoch [444/500] Train Loss: 2.3674, Val Loss: 2.3988\n",
      "Epoch [445/500] Train Loss: 2.3583, Val Loss: 2.3983\n",
      "Epoch [446/500] Train Loss: 2.3551, Val Loss: 2.3775\n",
      "Epoch [447/500] Train Loss: 2.3496, Val Loss: 2.3899\n",
      "Epoch [448/500] Train Loss: 2.3555, Val Loss: 2.4114\n",
      "Epoch [449/500] Train Loss: 2.3436, Val Loss: 2.3763\n",
      "Epoch [450/500] Train Loss: 2.3406, Val Loss: 2.3805\n",
      "Epoch [451/500] Train Loss: 2.3353, Val Loss: 2.3875\n",
      "Epoch [452/500] Train Loss: 2.3460, Val Loss: 2.3881\n",
      "Epoch [453/500] Train Loss: 2.3341, Val Loss: 2.4154\n",
      "Epoch [454/500] Train Loss: 2.3510, Val Loss: 2.3976\n",
      "Epoch [455/500] Train Loss: 2.3250, Val Loss: 2.3628\n",
      "Epoch [456/500] Train Loss: 2.3351, Val Loss: 2.3707\n",
      "Epoch [457/500] Train Loss: 2.3270, Val Loss: 2.3551\n",
      "Epoch [458/500] Train Loss: 2.3209, Val Loss: 2.3498\n",
      "Epoch [459/500] Train Loss: 2.3019, Val Loss: 2.3455\n",
      "Epoch [460/500] Train Loss: 2.3027, Val Loss: 2.3446\n",
      "Epoch [461/500] Train Loss: 2.3076, Val Loss: 2.3240\n",
      "Epoch [462/500] Train Loss: 2.3034, Val Loss: 2.3306\n",
      "Epoch [463/500] Train Loss: 2.2890, Val Loss: 2.3254\n",
      "Epoch [464/500] Train Loss: 2.2961, Val Loss: 2.3419\n",
      "Epoch [465/500] Train Loss: 2.3020, Val Loss: 2.3487\n",
      "Epoch [466/500] Train Loss: 2.2891, Val Loss: 2.3100\n",
      "Epoch [467/500] Train Loss: 2.2835, Val Loss: 2.3154\n",
      "Epoch [468/500] Train Loss: 2.2813, Val Loss: 2.3070\n",
      "Epoch [469/500] Train Loss: 2.2758, Val Loss: 2.3146\n",
      "Epoch [470/500] Train Loss: 2.2727, Val Loss: 2.3091\n",
      "Epoch [471/500] Train Loss: 2.2746, Val Loss: 2.3031\n",
      "Epoch [472/500] Train Loss: 2.2635, Val Loss: 2.2993\n",
      "Epoch [473/500] Train Loss: 2.2676, Val Loss: 2.3174\n",
      "Epoch [474/500] Train Loss: 2.2718, Val Loss: 2.3125\n",
      "Epoch [475/500] Train Loss: 2.2625, Val Loss: 2.2900\n",
      "Epoch [476/500] Train Loss: 2.2495, Val Loss: 2.2805\n",
      "Epoch [477/500] Train Loss: 2.2553, Val Loss: 2.2824\n",
      "Epoch [478/500] Train Loss: 2.2634, Val Loss: 2.3383\n",
      "Epoch [479/500] Train Loss: 2.2658, Val Loss: 2.2668\n",
      "Epoch [480/500] Train Loss: 2.2434, Val Loss: 2.2609\n",
      "Epoch [481/500] Train Loss: 2.2517, Val Loss: 2.2772\n",
      "Epoch [482/500] Train Loss: 2.2421, Val Loss: 2.3076\n",
      "Epoch [483/500] Train Loss: 2.2504, Val Loss: 2.3213\n",
      "Epoch [484/500] Train Loss: 2.2624, Val Loss: 2.2471\n",
      "Epoch [485/500] Train Loss: 2.2443, Val Loss: 2.2548\n",
      "Epoch [486/500] Train Loss: 2.2242, Val Loss: 2.2527\n",
      "Epoch [487/500] Train Loss: 2.2240, Val Loss: 2.2391\n",
      "Epoch [488/500] Train Loss: 2.2131, Val Loss: 2.2492\n",
      "Epoch [489/500] Train Loss: 2.2102, Val Loss: 2.2517\n",
      "Epoch [490/500] Train Loss: 2.2128, Val Loss: 2.2990\n",
      "Epoch [491/500] Train Loss: 2.2283, Val Loss: 2.2242\n",
      "Epoch [492/500] Train Loss: 2.2144, Val Loss: 2.2411\n",
      "Epoch [493/500] Train Loss: 2.2156, Val Loss: 2.2400\n",
      "Epoch [494/500] Train Loss: 2.2136, Val Loss: 2.2488\n",
      "Epoch [495/500] Train Loss: 2.2463, Val Loss: 2.2270\n",
      "Epoch [496/500] Train Loss: 2.2417, Val Loss: 2.2286\n",
      "Epoch [497/500] Train Loss: 2.1985, Val Loss: 2.2406\n",
      "Epoch [498/500] Train Loss: 2.1883, Val Loss: 2.1943\n",
      "Epoch [499/500] Train Loss: 2.1847, Val Loss: 2.1926\n",
      "Epoch [500/500] Train Loss: 2.1787, Val Loss: 2.1895\n",
      "\n",
      "=== Fold 5 ===\n",
      "Epoch [1/500] Train Loss: 41.7555, Val Loss: 41.0077\n",
      "Epoch [2/500] Train Loss: 41.7385, Val Loss: 40.9902\n",
      "Epoch [3/500] Train Loss: 41.7197, Val Loss: 40.9695\n",
      "Epoch [4/500] Train Loss: 41.6970, Val Loss: 40.9450\n",
      "Epoch [5/500] Train Loss: 41.6763, Val Loss: 40.9310\n",
      "Epoch [6/500] Train Loss: 41.6601, Val Loss: 40.9003\n",
      "Epoch [7/500] Train Loss: 41.5953, Val Loss: 40.7887\n",
      "Epoch [8/500] Train Loss: 41.3885, Val Loss: 40.2298\n",
      "Epoch [9/500] Train Loss: 40.1430, Val Loss: 38.0453\n",
      "Epoch [10/500] Train Loss: 36.9641, Val Loss: 33.3520\n",
      "Epoch [11/500] Train Loss: 30.4831, Val Loss: 25.0044\n",
      "Epoch [12/500] Train Loss: 21.6717, Val Loss: 17.8372\n",
      "Epoch [13/500] Train Loss: 16.7756, Val Loss: 15.2254\n",
      "Epoch [14/500] Train Loss: 14.1569, Val Loss: 12.8872\n",
      "Epoch [15/500] Train Loss: 12.1668, Val Loss: 11.1751\n",
      "Epoch [16/500] Train Loss: 10.5583, Val Loss: 9.8218\n",
      "Epoch [17/500] Train Loss: 9.4602, Val Loss: 9.1495\n",
      "Epoch [18/500] Train Loss: 9.0169, Val Loss: 8.7887\n",
      "Epoch [19/500] Train Loss: 8.6983, Val Loss: 8.5066\n",
      "Epoch [20/500] Train Loss: 8.4798, Val Loss: 8.3788\n",
      "Epoch [21/500] Train Loss: 8.3905, Val Loss: 8.2953\n",
      "Epoch [22/500] Train Loss: 8.3166, Val Loss: 8.2416\n",
      "Epoch [23/500] Train Loss: 8.2675, Val Loss: 8.2043\n",
      "Epoch [24/500] Train Loss: 8.2366, Val Loss: 8.1731\n",
      "Epoch [25/500] Train Loss: 8.2097, Val Loss: 8.1480\n",
      "Epoch [26/500] Train Loss: 8.1863, Val Loss: 8.1259\n",
      "Epoch [27/500] Train Loss: 8.1660, Val Loss: 8.1030\n",
      "Epoch [28/500] Train Loss: 8.1469, Val Loss: 8.0819\n",
      "Epoch [29/500] Train Loss: 8.1269, Val Loss: 8.0623\n",
      "Epoch [30/500] Train Loss: 8.1114, Val Loss: 8.0437\n",
      "Epoch [31/500] Train Loss: 8.0957, Val Loss: 8.0260\n",
      "Epoch [32/500] Train Loss: 8.0818, Val Loss: 8.0117\n",
      "Epoch [33/500] Train Loss: 8.0686, Val Loss: 7.9966\n",
      "Epoch [34/500] Train Loss: 8.0573, Val Loss: 7.9837\n",
      "Epoch [35/500] Train Loss: 8.0461, Val Loss: 7.9705\n",
      "Epoch [36/500] Train Loss: 8.0366, Val Loss: 7.9564\n",
      "Epoch [37/500] Train Loss: 8.0239, Val Loss: 7.9441\n",
      "Epoch [38/500] Train Loss: 8.0154, Val Loss: 7.9327\n",
      "Epoch [39/500] Train Loss: 8.0061, Val Loss: 7.9227\n",
      "Epoch [40/500] Train Loss: 7.9986, Val Loss: 7.9151\n",
      "Epoch [41/500] Train Loss: 7.9935, Val Loss: 7.9080\n",
      "Epoch [42/500] Train Loss: 7.9880, Val Loss: 7.9012\n",
      "Epoch [43/500] Train Loss: 7.9834, Val Loss: 7.8956\n",
      "Epoch [44/500] Train Loss: 7.9803, Val Loss: 7.8908\n",
      "Epoch [45/500] Train Loss: 7.9755, Val Loss: 7.8860\n",
      "Epoch [46/500] Train Loss: 7.9729, Val Loss: 7.8823\n",
      "Epoch [47/500] Train Loss: 7.9704, Val Loss: 7.8790\n",
      "Epoch [48/500] Train Loss: 7.9680, Val Loss: 7.8755\n",
      "Epoch [49/500] Train Loss: 7.9662, Val Loss: 7.8725\n",
      "Epoch [50/500] Train Loss: 7.9648, Val Loss: 7.8697\n",
      "Epoch [51/500] Train Loss: 7.9634, Val Loss: 7.8676\n",
      "Epoch [52/500] Train Loss: 7.9623, Val Loss: 7.8658\n",
      "Epoch [53/500] Train Loss: 7.9603, Val Loss: 7.8641\n",
      "Epoch [54/500] Train Loss: 7.9592, Val Loss: 7.8627\n",
      "Epoch [55/500] Train Loss: 7.9584, Val Loss: 7.8608\n",
      "Epoch [56/500] Train Loss: 7.9580, Val Loss: 7.8593\n",
      "Epoch [57/500] Train Loss: 7.9573, Val Loss: 7.8579\n",
      "Epoch [58/500] Train Loss: 7.9559, Val Loss: 7.8563\n",
      "Epoch [59/500] Train Loss: 7.9554, Val Loss: 7.8549\n",
      "Epoch [60/500] Train Loss: 7.9557, Val Loss: 7.8539\n",
      "Epoch [61/500] Train Loss: 7.9548, Val Loss: 7.8528\n",
      "Epoch [62/500] Train Loss: 7.9534, Val Loss: 7.8521\n",
      "Epoch [63/500] Train Loss: 7.9527, Val Loss: 7.8513\n",
      "Epoch [64/500] Train Loss: 7.9530, Val Loss: 7.8502\n",
      "Epoch [65/500] Train Loss: 7.9518, Val Loss: 7.8493\n",
      "Epoch [66/500] Train Loss: 7.9515, Val Loss: 7.8481\n",
      "Epoch [67/500] Train Loss: 7.9507, Val Loss: 7.8475\n",
      "Epoch [68/500] Train Loss: 7.9497, Val Loss: 7.8460\n",
      "Epoch [69/500] Train Loss: 7.9494, Val Loss: 7.8449\n",
      "Epoch [70/500] Train Loss: 7.9485, Val Loss: 7.8439\n",
      "Epoch [71/500] Train Loss: 7.9472, Val Loss: 7.8430\n",
      "Epoch [72/500] Train Loss: 7.9467, Val Loss: 7.8416\n",
      "Epoch [73/500] Train Loss: 7.9461, Val Loss: 7.8405\n",
      "Epoch [74/500] Train Loss: 7.9458, Val Loss: 7.8392\n",
      "Epoch [75/500] Train Loss: 7.9438, Val Loss: 7.8376\n",
      "Epoch [76/500] Train Loss: 7.9425, Val Loss: 7.8364\n",
      "Epoch [77/500] Train Loss: 7.9419, Val Loss: 7.8345\n",
      "Epoch [78/500] Train Loss: 7.9396, Val Loss: 7.8329\n",
      "Epoch [79/500] Train Loss: 7.9378, Val Loss: 7.8310\n",
      "Epoch [80/500] Train Loss: 7.9367, Val Loss: 7.8287\n",
      "Epoch [81/500] Train Loss: 7.9346, Val Loss: 7.8261\n",
      "Epoch [82/500] Train Loss: 7.9325, Val Loss: 7.8224\n",
      "Epoch [83/500] Train Loss: 7.9301, Val Loss: 7.8184\n",
      "Epoch [84/500] Train Loss: 7.9261, Val Loss: 7.8143\n",
      "Epoch [85/500] Train Loss: 7.9223, Val Loss: 7.8091\n",
      "Epoch [86/500] Train Loss: 7.9157, Val Loss: 7.8013\n",
      "Epoch [87/500] Train Loss: 7.9064, Val Loss: 7.7546\n",
      "Epoch [88/500] Train Loss: 7.8661, Val Loss: 7.7280\n",
      "Epoch [89/500] Train Loss: 7.8255, Val Loss: 7.6628\n",
      "Epoch [90/500] Train Loss: 7.7735, Val Loss: 7.6114\n",
      "Epoch [91/500] Train Loss: 7.7162, Val Loss: 7.4905\n",
      "Epoch [92/500] Train Loss: 7.5972, Val Loss: 7.3287\n",
      "Epoch [93/500] Train Loss: 7.4358, Val Loss: 7.0994\n",
      "Epoch [94/500] Train Loss: 7.1851, Val Loss: 6.7291\n",
      "Epoch [95/500] Train Loss: 6.8257, Val Loss: 6.2147\n",
      "Epoch [96/500] Train Loss: 6.3366, Val Loss: 5.5648\n",
      "Epoch [97/500] Train Loss: 5.7566, Val Loss: 4.8747\n",
      "Epoch [98/500] Train Loss: 5.1643, Val Loss: 4.3585\n",
      "Epoch [99/500] Train Loss: 4.7918, Val Loss: 4.0930\n",
      "Epoch [100/500] Train Loss: 4.5803, Val Loss: 3.9306\n",
      "Epoch [101/500] Train Loss: 4.4412, Val Loss: 3.7930\n",
      "Epoch [102/500] Train Loss: 4.3372, Val Loss: 3.7323\n",
      "Epoch [103/500] Train Loss: 4.2798, Val Loss: 3.6788\n",
      "Epoch [104/500] Train Loss: 4.2261, Val Loss: 3.6250\n",
      "Epoch [105/500] Train Loss: 4.1788, Val Loss: 3.5912\n",
      "Epoch [106/500] Train Loss: 4.1440, Val Loss: 3.5564\n",
      "Epoch [107/500] Train Loss: 4.1150, Val Loss: 3.5278\n",
      "Epoch [108/500] Train Loss: 4.0896, Val Loss: 3.5034\n",
      "Epoch [109/500] Train Loss: 4.0736, Val Loss: 3.4846\n",
      "Epoch [110/500] Train Loss: 4.0571, Val Loss: 3.4624\n",
      "Epoch [111/500] Train Loss: 4.0449, Val Loss: 3.4513\n",
      "Epoch [112/500] Train Loss: 4.0313, Val Loss: 3.4366\n",
      "Epoch [113/500] Train Loss: 4.0252, Val Loss: 3.4287\n",
      "Epoch [114/500] Train Loss: 4.0116, Val Loss: 3.4199\n",
      "Epoch [115/500] Train Loss: 4.0039, Val Loss: 3.4109\n",
      "Epoch [116/500] Train Loss: 3.9922, Val Loss: 3.3987\n",
      "Epoch [117/500] Train Loss: 3.9864, Val Loss: 3.3900\n",
      "Epoch [118/500] Train Loss: 3.9817, Val Loss: 3.3858\n",
      "Epoch [119/500] Train Loss: 3.9741, Val Loss: 3.3757\n",
      "Epoch [120/500] Train Loss: 3.9649, Val Loss: 3.3699\n",
      "Epoch [121/500] Train Loss: 3.9579, Val Loss: 3.3603\n",
      "Epoch [122/500] Train Loss: 3.9509, Val Loss: 3.3520\n",
      "Epoch [123/500] Train Loss: 3.9433, Val Loss: 3.3506\n",
      "Epoch [124/500] Train Loss: 3.9405, Val Loss: 3.3470\n",
      "Epoch [125/500] Train Loss: 3.9307, Val Loss: 3.3363\n",
      "Epoch [126/500] Train Loss: 3.9240, Val Loss: 3.3273\n",
      "Epoch [127/500] Train Loss: 3.9143, Val Loss: 3.3192\n",
      "Epoch [128/500] Train Loss: 3.9044, Val Loss: 3.3176\n",
      "Epoch [129/500] Train Loss: 3.8973, Val Loss: 3.3040\n",
      "Epoch [130/500] Train Loss: 3.8887, Val Loss: 3.2978\n",
      "Epoch [131/500] Train Loss: 3.8808, Val Loss: 3.2897\n",
      "Epoch [132/500] Train Loss: 3.8734, Val Loss: 3.2801\n",
      "Epoch [133/500] Train Loss: 3.8672, Val Loss: 3.2755\n",
      "Epoch [134/500] Train Loss: 3.8549, Val Loss: 3.2616\n",
      "Epoch [135/500] Train Loss: 3.8459, Val Loss: 3.2537\n",
      "Epoch [136/500] Train Loss: 3.8384, Val Loss: 3.2562\n",
      "Epoch [137/500] Train Loss: 3.8295, Val Loss: 3.2328\n",
      "Epoch [138/500] Train Loss: 3.8147, Val Loss: 3.2292\n",
      "Epoch [139/500] Train Loss: 3.8055, Val Loss: 3.2165\n",
      "Epoch [140/500] Train Loss: 3.7938, Val Loss: 3.2058\n",
      "Epoch [141/500] Train Loss: 3.7859, Val Loss: 3.1941\n",
      "Epoch [142/500] Train Loss: 3.7748, Val Loss: 3.1943\n",
      "Epoch [143/500] Train Loss: 3.7692, Val Loss: 3.1746\n",
      "Epoch [144/500] Train Loss: 3.7538, Val Loss: 3.1645\n",
      "Epoch [145/500] Train Loss: 3.7423, Val Loss: 3.1590\n",
      "Epoch [146/500] Train Loss: 3.7292, Val Loss: 3.1511\n",
      "Epoch [147/500] Train Loss: 3.7163, Val Loss: 3.1353\n",
      "Epoch [148/500] Train Loss: 3.7053, Val Loss: 3.1339\n",
      "Epoch [149/500] Train Loss: 3.6936, Val Loss: 3.1232\n",
      "Epoch [150/500] Train Loss: 3.6838, Val Loss: 3.1152\n",
      "Epoch [151/500] Train Loss: 3.6692, Val Loss: 3.1106\n",
      "Epoch [152/500] Train Loss: 3.6632, Val Loss: 3.1013\n",
      "Epoch [153/500] Train Loss: 3.6533, Val Loss: 3.1011\n",
      "Epoch [154/500] Train Loss: 3.6486, Val Loss: 3.0944\n",
      "Epoch [155/500] Train Loss: 3.6317, Val Loss: 3.0816\n",
      "Epoch [156/500] Train Loss: 3.6209, Val Loss: 3.0721\n",
      "Epoch [157/500] Train Loss: 3.6113, Val Loss: 3.0743\n",
      "Epoch [158/500] Train Loss: 3.6058, Val Loss: 3.0909\n",
      "Epoch [159/500] Train Loss: 3.6059, Val Loss: 3.0503\n",
      "Epoch [160/500] Train Loss: 3.5951, Val Loss: 3.0416\n",
      "Epoch [161/500] Train Loss: 3.5812, Val Loss: 3.0517\n",
      "Epoch [162/500] Train Loss: 3.5712, Val Loss: 3.0454\n",
      "Epoch [163/500] Train Loss: 3.5646, Val Loss: 3.0295\n",
      "Epoch [164/500] Train Loss: 3.5611, Val Loss: 3.0406\n",
      "Epoch [165/500] Train Loss: 3.5531, Val Loss: 3.0172\n",
      "Epoch [166/500] Train Loss: 3.5428, Val Loss: 3.0256\n",
      "Epoch [167/500] Train Loss: 3.5458, Val Loss: 3.0208\n",
      "Epoch [168/500] Train Loss: 3.5397, Val Loss: 3.0421\n",
      "Epoch [169/500] Train Loss: 3.5340, Val Loss: 3.0102\n",
      "Epoch [170/500] Train Loss: 3.5251, Val Loss: 3.0037\n",
      "Epoch [171/500] Train Loss: 3.5093, Val Loss: 2.9991\n",
      "Epoch [172/500] Train Loss: 3.5110, Val Loss: 3.0008\n",
      "Epoch [173/500] Train Loss: 3.5062, Val Loss: 2.9903\n",
      "Epoch [174/500] Train Loss: 3.4992, Val Loss: 2.9851\n",
      "Epoch [175/500] Train Loss: 3.4906, Val Loss: 2.9824\n",
      "Epoch [176/500] Train Loss: 3.4835, Val Loss: 2.9762\n",
      "Epoch [177/500] Train Loss: 3.4755, Val Loss: 2.9675\n",
      "Epoch [178/500] Train Loss: 3.4683, Val Loss: 2.9713\n",
      "Epoch [179/500] Train Loss: 3.4617, Val Loss: 2.9691\n",
      "Epoch [180/500] Train Loss: 3.4557, Val Loss: 2.9670\n",
      "Epoch [181/500] Train Loss: 3.4484, Val Loss: 2.9586\n",
      "Epoch [182/500] Train Loss: 3.4472, Val Loss: 2.9523\n",
      "Epoch [183/500] Train Loss: 3.4455, Val Loss: 2.9485\n",
      "Epoch [184/500] Train Loss: 3.4391, Val Loss: 2.9559\n",
      "Epoch [185/500] Train Loss: 3.4416, Val Loss: 2.9654\n",
      "Epoch [186/500] Train Loss: 3.4248, Val Loss: 2.9449\n",
      "Epoch [187/500] Train Loss: 3.4176, Val Loss: 2.9452\n",
      "Epoch [188/500] Train Loss: 3.4213, Val Loss: 2.9427\n",
      "Epoch [189/500] Train Loss: 3.4208, Val Loss: 2.9419\n",
      "Epoch [190/500] Train Loss: 3.4028, Val Loss: 2.9691\n",
      "Epoch [191/500] Train Loss: 3.4019, Val Loss: 2.9422\n",
      "Epoch [192/500] Train Loss: 3.3867, Val Loss: 2.9486\n",
      "Epoch [193/500] Train Loss: 3.3900, Val Loss: 2.9348\n",
      "Epoch [194/500] Train Loss: 3.3873, Val Loss: 2.9487\n",
      "Epoch [195/500] Train Loss: 3.3738, Val Loss: 2.9294\n",
      "Epoch [196/500] Train Loss: 3.3628, Val Loss: 2.9287\n",
      "Epoch [197/500] Train Loss: 3.3601, Val Loss: 2.9180\n",
      "Epoch [198/500] Train Loss: 3.3512, Val Loss: 2.9211\n",
      "Epoch [199/500] Train Loss: 3.3533, Val Loss: 2.9324\n",
      "Epoch [200/500] Train Loss: 3.3441, Val Loss: 2.9115\n",
      "Epoch [201/500] Train Loss: 3.3346, Val Loss: 2.9080\n",
      "Epoch [202/500] Train Loss: 3.3280, Val Loss: 2.8978\n",
      "Epoch [203/500] Train Loss: 3.3233, Val Loss: 2.9075\n",
      "Epoch [204/500] Train Loss: 3.3177, Val Loss: 2.8843\n",
      "Epoch [205/500] Train Loss: 3.3044, Val Loss: 2.9039\n",
      "Epoch [206/500] Train Loss: 3.2977, Val Loss: 2.8851\n",
      "Epoch [207/500] Train Loss: 3.2918, Val Loss: 2.8729\n",
      "Epoch [208/500] Train Loss: 3.2858, Val Loss: 2.8760\n",
      "Epoch [209/500] Train Loss: 3.2791, Val Loss: 2.8732\n",
      "Epoch [210/500] Train Loss: 3.2752, Val Loss: 2.8763\n",
      "Epoch [211/500] Train Loss: 3.2653, Val Loss: 2.8647\n",
      "Epoch [212/500] Train Loss: 3.2615, Val Loss: 2.8531\n",
      "Epoch [213/500] Train Loss: 3.2550, Val Loss: 2.8549\n",
      "Epoch [214/500] Train Loss: 3.2497, Val Loss: 2.8652\n",
      "Epoch [215/500] Train Loss: 3.2526, Val Loss: 2.8714\n",
      "Epoch [216/500] Train Loss: 3.2592, Val Loss: 2.8617\n",
      "Epoch [217/500] Train Loss: 3.2435, Val Loss: 2.8649\n",
      "Epoch [218/500] Train Loss: 3.2263, Val Loss: 2.8475\n",
      "Epoch [219/500] Train Loss: 3.2232, Val Loss: 2.8453\n",
      "Epoch [220/500] Train Loss: 3.2209, Val Loss: 2.8595\n",
      "Epoch [221/500] Train Loss: 3.2124, Val Loss: 2.8536\n",
      "Epoch [222/500] Train Loss: 3.2098, Val Loss: 2.8703\n",
      "Epoch [223/500] Train Loss: 3.2115, Val Loss: 2.8223\n",
      "Epoch [224/500] Train Loss: 3.1870, Val Loss: 2.8171\n",
      "Epoch [225/500] Train Loss: 3.1838, Val Loss: 2.8197\n",
      "Epoch [226/500] Train Loss: 3.1793, Val Loss: 2.8198\n",
      "Epoch [227/500] Train Loss: 3.1720, Val Loss: 2.8147\n",
      "Epoch [228/500] Train Loss: 3.1630, Val Loss: 2.8076\n",
      "Epoch [229/500] Train Loss: 3.1574, Val Loss: 2.8192\n",
      "Epoch [230/500] Train Loss: 3.1573, Val Loss: 2.7989\n",
      "Epoch [231/500] Train Loss: 3.1460, Val Loss: 2.7947\n",
      "Epoch [232/500] Train Loss: 3.1402, Val Loss: 2.8107\n",
      "Epoch [233/500] Train Loss: 3.1390, Val Loss: 2.7872\n",
      "Epoch [234/500] Train Loss: 3.1310, Val Loss: 2.8051\n",
      "Epoch [235/500] Train Loss: 3.1337, Val Loss: 2.7793\n",
      "Epoch [236/500] Train Loss: 3.1182, Val Loss: 2.7644\n",
      "Epoch [237/500] Train Loss: 3.1074, Val Loss: 2.7754\n",
      "Epoch [238/500] Train Loss: 3.1050, Val Loss: 2.8164\n",
      "Epoch [239/500] Train Loss: 3.1308, Val Loss: 2.8097\n",
      "Epoch [240/500] Train Loss: 3.1355, Val Loss: 2.7510\n",
      "Epoch [241/500] Train Loss: 3.0929, Val Loss: 2.7789\n",
      "Epoch [242/500] Train Loss: 3.0815, Val Loss: 2.7738\n",
      "Epoch [243/500] Train Loss: 3.0898, Val Loss: 2.7495\n",
      "Epoch [244/500] Train Loss: 3.0805, Val Loss: 2.7384\n",
      "Epoch [245/500] Train Loss: 3.0702, Val Loss: 2.7411\n",
      "Epoch [246/500] Train Loss: 3.0589, Val Loss: 2.7185\n",
      "Epoch [247/500] Train Loss: 3.0447, Val Loss: 2.7200\n",
      "Epoch [248/500] Train Loss: 3.0425, Val Loss: 2.7114\n",
      "Epoch [249/500] Train Loss: 3.0366, Val Loss: 2.7130\n",
      "Epoch [250/500] Train Loss: 3.0240, Val Loss: 2.7011\n",
      "Epoch [251/500] Train Loss: 3.0192, Val Loss: 2.6937\n",
      "Epoch [252/500] Train Loss: 3.0216, Val Loss: 2.7099\n",
      "Epoch [253/500] Train Loss: 3.0077, Val Loss: 2.6893\n",
      "Epoch [254/500] Train Loss: 2.9951, Val Loss: 2.6774\n",
      "Epoch [255/500] Train Loss: 2.9995, Val Loss: 2.6697\n",
      "Epoch [256/500] Train Loss: 2.9932, Val Loss: 2.6669\n",
      "Epoch [257/500] Train Loss: 2.9796, Val Loss: 2.6661\n",
      "Epoch [258/500] Train Loss: 2.9676, Val Loss: 2.6670\n",
      "Epoch [259/500] Train Loss: 2.9669, Val Loss: 2.6537\n",
      "Epoch [260/500] Train Loss: 2.9588, Val Loss: 2.6632\n",
      "Epoch [261/500] Train Loss: 2.9729, Val Loss: 2.6576\n",
      "Epoch [262/500] Train Loss: 2.9524, Val Loss: 2.6587\n",
      "Epoch [263/500] Train Loss: 2.9526, Val Loss: 2.6487\n",
      "Epoch [264/500] Train Loss: 2.9413, Val Loss: 2.6482\n",
      "Epoch [265/500] Train Loss: 2.9331, Val Loss: 2.6303\n",
      "Epoch [266/500] Train Loss: 2.9154, Val Loss: 2.6415\n",
      "Epoch [267/500] Train Loss: 2.9198, Val Loss: 2.6311\n",
      "Epoch [268/500] Train Loss: 2.9067, Val Loss: 2.6281\n",
      "Epoch [269/500] Train Loss: 2.8941, Val Loss: 2.6262\n",
      "Epoch [270/500] Train Loss: 2.8853, Val Loss: 2.6554\n",
      "Epoch [271/500] Train Loss: 2.8904, Val Loss: 2.6352\n",
      "Epoch [272/500] Train Loss: 2.8838, Val Loss: 2.6133\n",
      "Epoch [273/500] Train Loss: 2.8887, Val Loss: 2.6372\n",
      "Epoch [274/500] Train Loss: 2.8753, Val Loss: 2.6254\n",
      "Epoch [275/500] Train Loss: 2.8603, Val Loss: 2.6019\n",
      "Epoch [276/500] Train Loss: 2.8518, Val Loss: 2.5896\n",
      "Epoch [277/500] Train Loss: 2.8419, Val Loss: 2.5839\n",
      "Epoch [278/500] Train Loss: 2.8408, Val Loss: 2.5903\n",
      "Epoch [279/500] Train Loss: 2.8385, Val Loss: 2.6181\n",
      "Epoch [280/500] Train Loss: 2.8386, Val Loss: 2.5896\n",
      "Epoch [281/500] Train Loss: 2.8235, Val Loss: 2.5958\n",
      "Epoch [282/500] Train Loss: 2.8134, Val Loss: 2.5712\n",
      "Epoch [283/500] Train Loss: 2.8067, Val Loss: 2.5772\n",
      "Epoch [284/500] Train Loss: 2.8009, Val Loss: 2.5686\n",
      "Epoch [285/500] Train Loss: 2.7870, Val Loss: 2.5557\n",
      "Epoch [286/500] Train Loss: 2.7783, Val Loss: 2.5578\n",
      "Epoch [287/500] Train Loss: 2.7766, Val Loss: 2.5700\n",
      "Epoch [288/500] Train Loss: 2.7903, Val Loss: 2.5380\n",
      "Epoch [289/500] Train Loss: 2.7773, Val Loss: 2.5268\n",
      "Epoch [290/500] Train Loss: 2.7628, Val Loss: 2.5230\n",
      "Epoch [291/500] Train Loss: 2.7586, Val Loss: 2.5268\n",
      "Epoch [292/500] Train Loss: 2.7465, Val Loss: 2.5261\n",
      "Epoch [293/500] Train Loss: 2.7453, Val Loss: 2.5218\n",
      "Epoch [294/500] Train Loss: 2.7473, Val Loss: 2.5077\n",
      "Epoch [295/500] Train Loss: 2.7266, Val Loss: 2.5169\n",
      "Epoch [296/500] Train Loss: 2.7510, Val Loss: 2.5335\n",
      "Epoch [297/500] Train Loss: 2.7504, Val Loss: 2.5120\n",
      "Epoch [298/500] Train Loss: 2.7271, Val Loss: 2.5259\n",
      "Epoch [299/500] Train Loss: 2.7276, Val Loss: 2.5295\n",
      "Epoch [300/500] Train Loss: 2.7357, Val Loss: 2.4839\n",
      "Epoch [301/500] Train Loss: 2.7020, Val Loss: 2.4814\n",
      "Epoch [302/500] Train Loss: 2.7031, Val Loss: 2.4695\n",
      "Epoch [303/500] Train Loss: 2.6911, Val Loss: 2.4790\n",
      "Epoch [304/500] Train Loss: 2.6896, Val Loss: 2.4861\n",
      "Epoch [305/500] Train Loss: 2.6878, Val Loss: 2.5293\n",
      "Epoch [306/500] Train Loss: 2.7121, Val Loss: 2.4938\n",
      "Epoch [307/500] Train Loss: 2.6966, Val Loss: 2.5013\n",
      "Epoch [308/500] Train Loss: 2.6806, Val Loss: 2.4736\n",
      "Epoch [309/500] Train Loss: 2.6549, Val Loss: 2.5001\n",
      "Epoch [310/500] Train Loss: 2.6881, Val Loss: 2.4647\n",
      "Epoch [311/500] Train Loss: 2.6734, Val Loss: 2.4759\n",
      "Epoch [312/500] Train Loss: 2.6666, Val Loss: 2.4654\n",
      "Epoch [313/500] Train Loss: 2.6845, Val Loss: 2.4940\n",
      "Epoch [314/500] Train Loss: 2.6712, Val Loss: 2.5161\n",
      "Epoch [315/500] Train Loss: 2.6591, Val Loss: 2.4585\n",
      "Epoch [316/500] Train Loss: 2.6359, Val Loss: 2.4916\n",
      "Epoch [317/500] Train Loss: 2.6376, Val Loss: 2.4390\n",
      "Epoch [318/500] Train Loss: 2.6363, Val Loss: 2.4452\n",
      "Epoch [319/500] Train Loss: 2.6390, Val Loss: 2.4536\n",
      "Epoch [320/500] Train Loss: 2.6222, Val Loss: 2.4385\n",
      "Epoch [321/500] Train Loss: 2.6085, Val Loss: 2.4204\n",
      "Epoch [322/500] Train Loss: 2.6132, Val Loss: 2.4236\n",
      "Epoch [323/500] Train Loss: 2.6046, Val Loss: 2.4139\n",
      "Epoch [324/500] Train Loss: 2.6259, Val Loss: 2.4634\n",
      "Epoch [325/500] Train Loss: 2.6240, Val Loss: 2.4203\n",
      "Epoch [326/500] Train Loss: 2.5943, Val Loss: 2.3987\n",
      "Epoch [327/500] Train Loss: 2.5882, Val Loss: 2.3870\n",
      "Epoch [328/500] Train Loss: 2.5810, Val Loss: 2.4004\n",
      "Epoch [329/500] Train Loss: 2.5991, Val Loss: 2.3730\n",
      "Epoch [330/500] Train Loss: 2.5842, Val Loss: 2.3754\n",
      "Epoch [331/500] Train Loss: 2.5794, Val Loss: 2.3672\n",
      "Epoch [332/500] Train Loss: 2.5663, Val Loss: 2.3654\n",
      "Epoch [333/500] Train Loss: 2.5593, Val Loss: 2.3638\n",
      "Epoch [334/500] Train Loss: 2.5635, Val Loss: 2.3910\n",
      "Epoch [335/500] Train Loss: 2.5746, Val Loss: 2.3905\n",
      "Epoch [336/500] Train Loss: 2.5577, Val Loss: 2.3591\n",
      "Epoch [337/500] Train Loss: 2.5595, Val Loss: 2.3746\n",
      "Epoch [338/500] Train Loss: 2.5580, Val Loss: 2.3821\n",
      "Epoch [339/500] Train Loss: 2.5698, Val Loss: 2.3652\n",
      "Epoch [340/500] Train Loss: 2.5591, Val Loss: 2.3464\n",
      "Epoch [341/500] Train Loss: 2.5418, Val Loss: 2.3486\n",
      "Epoch [342/500] Train Loss: 2.5369, Val Loss: 2.3521\n",
      "Epoch [343/500] Train Loss: 2.5290, Val Loss: 2.3529\n",
      "Epoch [344/500] Train Loss: 2.5312, Val Loss: 2.3524\n",
      "Epoch [345/500] Train Loss: 2.5458, Val Loss: 2.3455\n",
      "Epoch [346/500] Train Loss: 2.5246, Val Loss: 2.3692\n",
      "Epoch [347/500] Train Loss: 2.5087, Val Loss: 2.3628\n",
      "Epoch [348/500] Train Loss: 2.5033, Val Loss: 2.3715\n",
      "Epoch [349/500] Train Loss: 2.5049, Val Loss: 2.3631\n",
      "Epoch [350/500] Train Loss: 2.4972, Val Loss: 2.3680\n",
      "Epoch [351/500] Train Loss: 2.4888, Val Loss: 2.3859\n",
      "Epoch [352/500] Train Loss: 2.4921, Val Loss: 2.3992\n",
      "Epoch [353/500] Train Loss: 2.4675, Val Loss: 2.4140\n",
      "Epoch [354/500] Train Loss: 2.4753, Val Loss: 2.4130\n",
      "Epoch [355/500] Train Loss: 2.4695, Val Loss: 2.4146\n",
      "Epoch [356/500] Train Loss: 2.4745, Val Loss: 2.3961\n",
      "Epoch [357/500] Train Loss: 2.4681, Val Loss: 2.3835\n",
      "Epoch [358/500] Train Loss: 2.4666, Val Loss: 2.4338\n",
      "Epoch [359/500] Train Loss: 2.4557, Val Loss: 2.4162\n",
      "Epoch [360/500] Train Loss: 2.4969, Val Loss: 2.4072\n",
      "Epoch [361/500] Train Loss: 2.4844, Val Loss: 2.3838\n",
      "Epoch [362/500] Train Loss: 2.4538, Val Loss: 2.4038\n",
      "Epoch [363/500] Train Loss: 2.4413, Val Loss: 2.3912\n",
      "Epoch [364/500] Train Loss: 2.4382, Val Loss: 2.3743\n",
      "Epoch [365/500] Train Loss: 2.4165, Val Loss: 2.3801\n",
      "Epoch [366/500] Train Loss: 2.4209, Val Loss: 2.3697\n",
      "Epoch [367/500] Train Loss: 2.4143, Val Loss: 2.3767\n",
      "Epoch [368/500] Train Loss: 2.4096, Val Loss: 2.3665\n",
      "Epoch [369/500] Train Loss: 2.4110, Val Loss: 2.3549\n",
      "Epoch [370/500] Train Loss: 2.3979, Val Loss: 2.3537\n",
      "Epoch [371/500] Train Loss: 2.3922, Val Loss: 2.3688\n",
      "Epoch [372/500] Train Loss: 2.4018, Val Loss: 2.3698\n",
      "Epoch [373/500] Train Loss: 2.3899, Val Loss: 2.3634\n",
      "Epoch [374/500] Train Loss: 2.3866, Val Loss: 2.3435\n",
      "Epoch [375/500] Train Loss: 2.3865, Val Loss: 2.3392\n",
      "Epoch [376/500] Train Loss: 2.3879, Val Loss: 2.3749\n",
      "Epoch [377/500] Train Loss: 2.3959, Val Loss: 2.3507\n",
      "Epoch [378/500] Train Loss: 2.4018, Val Loss: 2.4021\n",
      "Epoch [379/500] Train Loss: 2.3916, Val Loss: 2.3288\n",
      "Epoch [380/500] Train Loss: 2.3988, Val Loss: 2.4046\n",
      "Epoch [381/500] Train Loss: 2.3963, Val Loss: 2.2868\n",
      "Epoch [382/500] Train Loss: 2.3648, Val Loss: 2.3252\n",
      "Epoch [383/500] Train Loss: 2.3574, Val Loss: 2.2986\n",
      "Epoch [384/500] Train Loss: 2.3435, Val Loss: 2.3087\n",
      "Epoch [385/500] Train Loss: 2.3465, Val Loss: 2.2995\n",
      "Epoch [386/500] Train Loss: 2.3305, Val Loss: 2.2885\n",
      "Epoch [387/500] Train Loss: 2.3380, Val Loss: 2.2992\n",
      "Epoch [388/500] Train Loss: 2.3226, Val Loss: 2.2870\n",
      "Epoch [389/500] Train Loss: 2.3215, Val Loss: 2.2890\n",
      "Epoch [390/500] Train Loss: 2.3124, Val Loss: 2.2767\n",
      "Epoch [391/500] Train Loss: 2.3108, Val Loss: 2.2977\n",
      "Epoch [392/500] Train Loss: 2.3271, Val Loss: 2.2621\n",
      "Epoch [393/500] Train Loss: 2.3151, Val Loss: 2.2666\n",
      "Epoch [394/500] Train Loss: 2.2940, Val Loss: 2.2771\n",
      "Epoch [395/500] Train Loss: 2.3252, Val Loss: 2.2555\n",
      "Epoch [396/500] Train Loss: 2.3120, Val Loss: 2.2058\n",
      "Epoch [397/500] Train Loss: 2.3162, Val Loss: 2.2147\n",
      "Epoch [398/500] Train Loss: 2.2990, Val Loss: 2.1956\n",
      "Epoch [399/500] Train Loss: 2.2804, Val Loss: 2.1852\n",
      "Epoch [400/500] Train Loss: 2.2657, Val Loss: 2.1997\n",
      "Epoch [401/500] Train Loss: 2.2767, Val Loss: 2.1754\n",
      "Epoch [402/500] Train Loss: 2.2958, Val Loss: 2.1690\n",
      "Epoch [403/500] Train Loss: 2.2585, Val Loss: 2.1614\n",
      "Epoch [404/500] Train Loss: 2.2656, Val Loss: 2.1528\n",
      "Epoch [405/500] Train Loss: 2.2845, Val Loss: 2.1819\n",
      "Epoch [406/500] Train Loss: 2.2826, Val Loss: 2.1720\n",
      "Epoch [407/500] Train Loss: 2.2564, Val Loss: 2.1505\n",
      "Epoch [408/500] Train Loss: 2.2406, Val Loss: 2.1602\n",
      "Epoch [409/500] Train Loss: 2.2362, Val Loss: 2.1487\n",
      "Epoch [410/500] Train Loss: 2.2383, Val Loss: 2.1426\n",
      "Epoch [411/500] Train Loss: 2.2466, Val Loss: 2.1161\n",
      "Epoch [412/500] Train Loss: 2.2356, Val Loss: 2.1247\n",
      "Epoch [413/500] Train Loss: 2.2265, Val Loss: 2.1170\n",
      "Epoch [414/500] Train Loss: 2.2249, Val Loss: 2.1365\n",
      "Epoch [415/500] Train Loss: 2.2159, Val Loss: 2.1311\n",
      "Epoch [416/500] Train Loss: 2.2145, Val Loss: 2.1108\n",
      "Epoch [417/500] Train Loss: 2.2124, Val Loss: 2.1055\n",
      "Epoch [418/500] Train Loss: 2.2127, Val Loss: 2.1069\n",
      "Epoch [419/500] Train Loss: 2.2163, Val Loss: 2.1935\n",
      "Epoch [420/500] Train Loss: 2.2404, Val Loss: 2.1337\n",
      "Epoch [421/500] Train Loss: 2.2365, Val Loss: 2.1195\n",
      "Epoch [422/500] Train Loss: 2.2040, Val Loss: 2.0905\n",
      "Epoch [423/500] Train Loss: 2.1958, Val Loss: 2.0886\n",
      "Epoch [424/500] Train Loss: 2.1933, Val Loss: 2.0951\n",
      "Epoch [425/500] Train Loss: 2.1882, Val Loss: 2.1039\n",
      "Epoch [426/500] Train Loss: 2.1879, Val Loss: 2.1076\n",
      "Epoch [427/500] Train Loss: 2.1913, Val Loss: 2.0796\n",
      "Epoch [428/500] Train Loss: 2.1836, Val Loss: 2.0921\n",
      "Epoch [429/500] Train Loss: 2.1835, Val Loss: 2.0942\n",
      "Epoch [430/500] Train Loss: 2.1758, Val Loss: 2.0990\n",
      "Epoch [431/500] Train Loss: 2.1820, Val Loss: 2.0922\n",
      "Epoch [432/500] Train Loss: 2.1797, Val Loss: 2.0618\n",
      "Epoch [433/500] Train Loss: 2.1687, Val Loss: 2.0788\n",
      "Epoch [434/500] Train Loss: 2.1588, Val Loss: 2.0828\n",
      "Epoch [435/500] Train Loss: 2.1583, Val Loss: 2.0659\n",
      "Epoch [436/500] Train Loss: 2.1670, Val Loss: 2.0800\n",
      "Epoch [437/500] Train Loss: 2.1560, Val Loss: 2.0977\n",
      "Epoch [438/500] Train Loss: 2.1537, Val Loss: 2.1056\n",
      "Epoch [439/500] Train Loss: 2.1536, Val Loss: 2.1054\n",
      "Epoch [440/500] Train Loss: 2.1662, Val Loss: 2.0766\n",
      "Epoch [441/500] Train Loss: 2.1479, Val Loss: 2.0722\n",
      "Epoch [442/500] Train Loss: 2.1349, Val Loss: 2.0781\n",
      "Epoch [443/500] Train Loss: 2.1432, Val Loss: 2.1257\n",
      "Epoch [444/500] Train Loss: 2.1480, Val Loss: 2.0579\n",
      "Epoch [445/500] Train Loss: 2.1201, Val Loss: 2.0853\n",
      "Epoch [446/500] Train Loss: 2.1217, Val Loss: 2.1020\n",
      "Epoch [447/500] Train Loss: 2.1322, Val Loss: 2.0289\n",
      "Epoch [448/500] Train Loss: 2.1154, Val Loss: 2.0456\n",
      "Epoch [449/500] Train Loss: 2.1149, Val Loss: 2.0399\n",
      "Epoch [450/500] Train Loss: 2.1056, Val Loss: 2.0468\n",
      "Epoch [451/500] Train Loss: 2.0974, Val Loss: 2.0800\n",
      "Epoch [452/500] Train Loss: 2.0974, Val Loss: 2.0336\n",
      "Epoch [453/500] Train Loss: 2.0912, Val Loss: 2.0223\n",
      "Epoch [454/500] Train Loss: 2.0872, Val Loss: 2.0411\n",
      "Epoch [455/500] Train Loss: 2.0788, Val Loss: 2.0082\n",
      "Epoch [456/500] Train Loss: 2.0759, Val Loss: 2.0195\n",
      "Epoch [457/500] Train Loss: 2.0806, Val Loss: 2.0611\n",
      "Epoch [458/500] Train Loss: 2.0989, Val Loss: 2.1150\n",
      "Epoch [459/500] Train Loss: 2.1008, Val Loss: 2.0070\n",
      "Epoch [460/500] Train Loss: 2.0617, Val Loss: 2.0713\n",
      "Epoch [461/500] Train Loss: 2.0547, Val Loss: 1.9757\n",
      "Epoch [462/500] Train Loss: 2.0372, Val Loss: 1.9654\n",
      "Epoch [463/500] Train Loss: 2.0252, Val Loss: 1.9603\n",
      "Epoch [464/500] Train Loss: 2.0038, Val Loss: 1.9698\n",
      "Epoch [465/500] Train Loss: 1.9917, Val Loss: 1.9924\n",
      "Epoch [466/500] Train Loss: 2.0089, Val Loss: 1.9797\n",
      "Epoch [467/500] Train Loss: 2.0001, Val Loss: 1.9835\n",
      "Epoch [468/500] Train Loss: 1.9677, Val Loss: 1.9633\n",
      "Epoch [469/500] Train Loss: 1.9677, Val Loss: 1.9773\n",
      "Epoch [470/500] Train Loss: 1.9574, Val Loss: 1.9725\n",
      "Epoch [471/500] Train Loss: 1.9517, Val Loss: 1.9711\n",
      "Epoch [472/500] Train Loss: 1.9486, Val Loss: 1.9789\n",
      "Epoch [473/500] Train Loss: 1.9448, Val Loss: 1.9846\n",
      "Epoch [474/500] Train Loss: 1.9335, Val Loss: 1.9879\n",
      "Epoch [475/500] Train Loss: 1.9415, Val Loss: 1.9682\n",
      "Epoch [476/500] Train Loss: 1.9325, Val Loss: 1.9517\n",
      "Epoch [477/500] Train Loss: 1.9260, Val Loss: 1.9727\n",
      "Epoch [478/500] Train Loss: 1.9159, Val Loss: 1.9713\n",
      "Epoch [479/500] Train Loss: 1.9363, Val Loss: 2.0363\n",
      "Epoch [480/500] Train Loss: 1.9342, Val Loss: 1.9843\n",
      "Epoch [481/500] Train Loss: 1.9147, Val Loss: 2.0072\n",
      "Epoch [482/500] Train Loss: 1.9114, Val Loss: 1.9611\n",
      "Epoch [483/500] Train Loss: 1.9043, Val Loss: 1.9825\n",
      "Epoch [484/500] Train Loss: 1.8802, Val Loss: 1.9496\n",
      "Epoch [485/500] Train Loss: 1.8831, Val Loss: 1.9652\n",
      "Epoch [486/500] Train Loss: 1.8755, Val Loss: 1.9544\n",
      "Epoch [487/500] Train Loss: 1.8682, Val Loss: 1.9155\n",
      "Epoch [488/500] Train Loss: 1.8517, Val Loss: 1.9352\n",
      "Epoch [489/500] Train Loss: 1.8705, Val Loss: 1.9371\n",
      "Epoch [490/500] Train Loss: 1.8472, Val Loss: 1.8845\n",
      "Epoch [491/500] Train Loss: 1.8395, Val Loss: 1.8806\n",
      "Epoch [492/500] Train Loss: 1.8404, Val Loss: 2.0307\n",
      "Epoch [493/500] Train Loss: 1.8669, Val Loss: 1.9061\n",
      "Epoch [494/500] Train Loss: 1.8077, Val Loss: 1.8591\n",
      "Epoch [495/500] Train Loss: 1.8152, Val Loss: 1.7781\n",
      "Epoch [496/500] Train Loss: 1.7783, Val Loss: 1.8116\n",
      "Epoch [497/500] Train Loss: 1.7637, Val Loss: 1.8933\n",
      "Epoch [498/500] Train Loss: 1.7513, Val Loss: 1.8028\n",
      "Epoch [499/500] Train Loss: 1.7317, Val Loss: 1.8373\n",
      "Epoch [500/500] Train Loss: 1.7288, Val Loss: 1.7792\n",
      "\n",
      "=== Fold 6 ===\n",
      "Epoch [1/500] Train Loss: 40.8336, Val Loss: 40.0389\n",
      "Epoch [2/500] Train Loss: 40.7929, Val Loss: 40.0039\n",
      "Epoch [3/500] Train Loss: 40.7586, Val Loss: 39.9678\n",
      "Epoch [4/500] Train Loss: 40.7144, Val Loss: 39.9190\n",
      "Epoch [5/500] Train Loss: 40.6595, Val Loss: 39.8597\n",
      "Epoch [6/500] Train Loss: 40.5930, Val Loss: 39.7907\n",
      "Epoch [7/500] Train Loss: 40.5257, Val Loss: 39.7376\n",
      "Epoch [8/500] Train Loss: 40.4670, Val Loss: 39.6611\n",
      "Epoch [9/500] Train Loss: 40.2732, Val Loss: 39.2123\n",
      "Epoch [10/500] Train Loss: 39.5012, Val Loss: 38.0603\n",
      "Epoch [11/500] Train Loss: 37.7982, Val Loss: 35.5787\n",
      "Epoch [12/500] Train Loss: 34.1155, Val Loss: 30.3916\n",
      "Epoch [13/500] Train Loss: 27.3656, Val Loss: 22.5292\n",
      "Epoch [14/500] Train Loss: 20.0937, Val Loss: 18.4450\n",
      "Epoch [15/500] Train Loss: 18.7862, Val Loss: 18.5017\n",
      "Epoch [16/500] Train Loss: 18.4738, Val Loss: 18.3992\n",
      "Epoch [17/500] Train Loss: 18.4336, Val Loss: 18.3089\n",
      "Epoch [18/500] Train Loss: 18.3724, Val Loss: 18.2784\n",
      "Epoch [19/500] Train Loss: 18.3437, Val Loss: 18.2530\n",
      "Epoch [20/500] Train Loss: 18.3158, Val Loss: 18.2275\n",
      "Epoch [21/500] Train Loss: 18.2914, Val Loss: 18.2035\n",
      "Epoch [22/500] Train Loss: 18.2667, Val Loss: 18.1834\n",
      "Epoch [23/500] Train Loss: 18.2448, Val Loss: 18.1626\n",
      "Epoch [24/500] Train Loss: 18.2236, Val Loss: 18.1451\n",
      "Epoch [25/500] Train Loss: 18.2037, Val Loss: 18.1296\n",
      "Epoch [26/500] Train Loss: 18.1838, Val Loss: 18.1133\n",
      "Epoch [27/500] Train Loss: 18.1634, Val Loss: 18.0971\n",
      "Epoch [28/500] Train Loss: 18.1460, Val Loss: 18.0834\n",
      "Epoch [29/500] Train Loss: 18.1292, Val Loss: 18.0719\n",
      "Epoch [30/500] Train Loss: 18.1160, Val Loss: 18.0628\n",
      "Epoch [31/500] Train Loss: 18.1042, Val Loss: 18.0545\n",
      "Epoch [32/500] Train Loss: 18.0952, Val Loss: 18.0487\n",
      "Epoch [33/500] Train Loss: 18.0876, Val Loss: 18.0426\n",
      "Epoch [34/500] Train Loss: 18.0810, Val Loss: 18.0395\n",
      "Epoch [35/500] Train Loss: 18.0762, Val Loss: 18.0353\n",
      "Epoch [36/500] Train Loss: 18.0713, Val Loss: 18.0331\n",
      "Epoch [37/500] Train Loss: 18.0675, Val Loss: 18.0305\n",
      "Epoch [38/500] Train Loss: 18.0637, Val Loss: 18.0286\n",
      "Epoch [39/500] Train Loss: 18.0603, Val Loss: 18.0265\n",
      "Epoch [40/500] Train Loss: 18.0575, Val Loss: 18.0248\n",
      "Epoch [41/500] Train Loss: 18.0540, Val Loss: 18.0235\n",
      "Epoch [42/500] Train Loss: 18.0511, Val Loss: 18.0216\n",
      "Epoch [43/500] Train Loss: 18.0482, Val Loss: 18.0200\n",
      "Epoch [44/500] Train Loss: 18.0448, Val Loss: 18.0175\n",
      "Epoch [45/500] Train Loss: 18.0409, Val Loss: 18.0142\n",
      "Epoch [46/500] Train Loss: 18.0360, Val Loss: 18.0098\n",
      "Epoch [47/500] Train Loss: 18.0288, Val Loss: 18.0051\n",
      "Epoch [48/500] Train Loss: 18.0185, Val Loss: 17.9947\n",
      "Epoch [49/500] Train Loss: 18.0014, Val Loss: 17.9783\n",
      "Epoch [50/500] Train Loss: 17.9668, Val Loss: 17.9298\n",
      "Epoch [51/500] Train Loss: 17.8826, Val Loss: 17.7041\n",
      "Epoch [52/500] Train Loss: 16.3212, Val Loss: 12.9575\n",
      "Epoch [53/500] Train Loss: 10.9258, Val Loss: 9.3255\n",
      "Epoch [54/500] Train Loss: 8.3875, Val Loss: 8.0671\n",
      "Epoch [55/500] Train Loss: 7.4102, Val Loss: 7.3816\n",
      "Epoch [56/500] Train Loss: 6.7886, Val Loss: 6.8938\n",
      "Epoch [57/500] Train Loss: 6.3389, Val Loss: 6.5306\n",
      "Epoch [58/500] Train Loss: 5.9988, Val Loss: 6.1994\n",
      "Epoch [59/500] Train Loss: 5.7040, Val Loss: 5.8906\n",
      "Epoch [60/500] Train Loss: 5.4304, Val Loss: 5.5971\n",
      "Epoch [61/500] Train Loss: 5.1825, Val Loss: 5.3391\n",
      "Epoch [62/500] Train Loss: 4.9609, Val Loss: 5.1231\n",
      "Epoch [63/500] Train Loss: 4.7762, Val Loss: 4.9282\n",
      "Epoch [64/500] Train Loss: 4.6154, Val Loss: 4.7946\n",
      "Epoch [65/500] Train Loss: 4.4968, Val Loss: 4.6612\n",
      "Epoch [66/500] Train Loss: 4.3968, Val Loss: 4.5838\n",
      "Epoch [67/500] Train Loss: 4.3211, Val Loss: 4.5145\n",
      "Epoch [68/500] Train Loss: 4.2661, Val Loss: 4.4725\n",
      "Epoch [69/500] Train Loss: 4.2291, Val Loss: 4.4376\n",
      "Epoch [70/500] Train Loss: 4.1985, Val Loss: 4.4122\n",
      "Epoch [71/500] Train Loss: 4.1766, Val Loss: 4.3903\n",
      "Epoch [72/500] Train Loss: 4.1572, Val Loss: 4.3757\n",
      "Epoch [73/500] Train Loss: 4.1414, Val Loss: 4.3626\n",
      "Epoch [74/500] Train Loss: 4.1303, Val Loss: 4.3522\n",
      "Epoch [75/500] Train Loss: 4.1196, Val Loss: 4.3420\n",
      "Epoch [76/500] Train Loss: 4.1103, Val Loss: 4.3331\n",
      "Epoch [77/500] Train Loss: 4.1039, Val Loss: 4.3279\n",
      "Epoch [78/500] Train Loss: 4.0965, Val Loss: 4.3205\n",
      "Epoch [79/500] Train Loss: 4.0870, Val Loss: 4.3117\n",
      "Epoch [80/500] Train Loss: 4.0789, Val Loss: 4.3075\n",
      "Epoch [81/500] Train Loss: 4.0718, Val Loss: 4.2987\n",
      "Epoch [82/500] Train Loss: 4.0660, Val Loss: 4.2934\n",
      "Epoch [83/500] Train Loss: 4.0609, Val Loss: 4.2878\n",
      "Epoch [84/500] Train Loss: 4.0558, Val Loss: 4.2833\n",
      "Epoch [85/500] Train Loss: 4.0503, Val Loss: 4.2781\n",
      "Epoch [86/500] Train Loss: 4.0462, Val Loss: 4.2773\n",
      "Epoch [87/500] Train Loss: 4.0420, Val Loss: 4.2718\n",
      "Epoch [88/500] Train Loss: 4.0386, Val Loss: 4.2690\n",
      "Epoch [89/500] Train Loss: 4.0357, Val Loss: 4.2650\n",
      "Epoch [90/500] Train Loss: 4.0320, Val Loss: 4.2630\n",
      "Epoch [91/500] Train Loss: 4.0288, Val Loss: 4.2591\n",
      "Epoch [92/500] Train Loss: 4.0264, Val Loss: 4.2592\n",
      "Epoch [93/500] Train Loss: 4.0252, Val Loss: 4.2555\n",
      "Epoch [94/500] Train Loss: 4.0218, Val Loss: 4.2554\n",
      "Epoch [95/500] Train Loss: 4.0202, Val Loss: 4.2508\n",
      "Epoch [96/500] Train Loss: 4.0168, Val Loss: 4.2510\n",
      "Epoch [97/500] Train Loss: 4.0149, Val Loss: 4.2485\n",
      "Epoch [98/500] Train Loss: 4.0122, Val Loss: 4.2462\n",
      "Epoch [99/500] Train Loss: 4.0105, Val Loss: 4.2433\n",
      "Epoch [100/500] Train Loss: 4.0095, Val Loss: 4.2383\n",
      "Epoch [101/500] Train Loss: 4.0071, Val Loss: 4.2368\n",
      "Epoch [102/500] Train Loss: 4.0061, Val Loss: 4.2373\n",
      "Epoch [103/500] Train Loss: 4.0039, Val Loss: 4.2360\n",
      "Epoch [104/500] Train Loss: 4.0030, Val Loss: 4.2314\n",
      "Epoch [105/500] Train Loss: 4.0003, Val Loss: 4.2311\n",
      "Epoch [106/500] Train Loss: 3.9995, Val Loss: 4.2304\n",
      "Epoch [107/500] Train Loss: 3.9987, Val Loss: 4.2303\n",
      "Epoch [108/500] Train Loss: 3.9972, Val Loss: 4.2272\n",
      "Epoch [109/500] Train Loss: 3.9967, Val Loss: 4.2232\n",
      "Epoch [110/500] Train Loss: 3.9945, Val Loss: 4.2240\n",
      "Epoch [111/500] Train Loss: 3.9933, Val Loss: 4.2216\n",
      "Epoch [112/500] Train Loss: 3.9930, Val Loss: 4.2246\n",
      "Epoch [113/500] Train Loss: 3.9914, Val Loss: 4.2223\n",
      "Epoch [114/500] Train Loss: 3.9897, Val Loss: 4.2227\n",
      "Epoch [115/500] Train Loss: 3.9893, Val Loss: 4.2188\n",
      "Epoch [116/500] Train Loss: 3.9881, Val Loss: 4.2201\n",
      "Epoch [117/500] Train Loss: 3.9873, Val Loss: 4.2174\n",
      "Epoch [118/500] Train Loss: 3.9868, Val Loss: 4.2172\n",
      "Epoch [119/500] Train Loss: 3.9862, Val Loss: 4.2148\n",
      "Epoch [120/500] Train Loss: 3.9848, Val Loss: 4.2194\n",
      "Epoch [121/500] Train Loss: 3.9851, Val Loss: 4.2121\n",
      "Epoch [122/500] Train Loss: 3.9839, Val Loss: 4.2123\n",
      "Epoch [123/500] Train Loss: 3.9827, Val Loss: 4.2128\n",
      "Epoch [124/500] Train Loss: 3.9829, Val Loss: 4.2103\n",
      "Epoch [125/500] Train Loss: 3.9818, Val Loss: 4.2115\n",
      "Epoch [126/500] Train Loss: 3.9819, Val Loss: 4.2090\n",
      "Epoch [127/500] Train Loss: 3.9790, Val Loss: 4.2058\n",
      "Epoch [128/500] Train Loss: 3.9795, Val Loss: 4.2048\n",
      "Epoch [129/500] Train Loss: 3.9788, Val Loss: 4.2027\n",
      "Epoch [130/500] Train Loss: 3.9780, Val Loss: 4.2035\n",
      "Epoch [131/500] Train Loss: 3.9775, Val Loss: 4.2037\n",
      "Epoch [132/500] Train Loss: 3.9768, Val Loss: 4.2031\n",
      "Epoch [133/500] Train Loss: 3.9772, Val Loss: 4.2007\n",
      "Epoch [134/500] Train Loss: 3.9764, Val Loss: 4.2010\n",
      "Epoch [135/500] Train Loss: 3.9763, Val Loss: 4.2027\n",
      "Epoch [136/500] Train Loss: 3.9753, Val Loss: 4.2029\n",
      "Epoch [137/500] Train Loss: 3.9757, Val Loss: 4.2006\n",
      "Epoch [138/500] Train Loss: 3.9741, Val Loss: 4.2003\n",
      "Epoch [139/500] Train Loss: 3.9739, Val Loss: 4.1995\n",
      "Epoch [140/500] Train Loss: 3.9735, Val Loss: 4.1987\n",
      "Epoch [141/500] Train Loss: 3.9730, Val Loss: 4.1984\n",
      "Epoch [142/500] Train Loss: 3.9732, Val Loss: 4.1980\n",
      "Epoch [143/500] Train Loss: 3.9725, Val Loss: 4.1985\n",
      "Epoch [144/500] Train Loss: 3.9719, Val Loss: 4.1979\n",
      "Epoch [145/500] Train Loss: 3.9726, Val Loss: 4.2003\n",
      "Epoch [146/500] Train Loss: 3.9721, Val Loss: 4.1985\n",
      "Epoch [147/500] Train Loss: 3.9713, Val Loss: 4.1986\n",
      "Epoch [148/500] Train Loss: 3.9710, Val Loss: 4.1979\n",
      "Epoch [149/500] Train Loss: 3.9710, Val Loss: 4.1975\n",
      "Epoch [150/500] Train Loss: 3.9710, Val Loss: 4.1966\n",
      "Epoch [151/500] Train Loss: 3.9700, Val Loss: 4.1988\n",
      "Epoch [152/500] Train Loss: 3.9698, Val Loss: 4.1974\n",
      "Epoch [153/500] Train Loss: 3.9695, Val Loss: 4.1960\n",
      "Epoch [154/500] Train Loss: 3.9695, Val Loss: 4.1946\n",
      "Epoch [155/500] Train Loss: 3.9696, Val Loss: 4.1971\n",
      "Epoch [156/500] Train Loss: 3.9695, Val Loss: 4.1949\n",
      "Epoch [157/500] Train Loss: 3.9682, Val Loss: 4.1940\n",
      "Epoch [158/500] Train Loss: 3.9682, Val Loss: 4.1938\n",
      "Epoch [159/500] Train Loss: 3.9682, Val Loss: 4.1921\n",
      "Epoch [160/500] Train Loss: 3.9685, Val Loss: 4.1939\n",
      "Epoch [161/500] Train Loss: 3.9675, Val Loss: 4.1919\n",
      "Epoch [162/500] Train Loss: 3.9672, Val Loss: 4.1929\n",
      "Epoch [163/500] Train Loss: 3.9670, Val Loss: 4.1919\n",
      "Epoch [164/500] Train Loss: 3.9665, Val Loss: 4.1903\n",
      "Epoch [165/500] Train Loss: 3.9666, Val Loss: 4.1916\n",
      "Epoch [166/500] Train Loss: 3.9669, Val Loss: 4.1909\n",
      "Epoch [167/500] Train Loss: 3.9662, Val Loss: 4.1920\n",
      "Epoch [168/500] Train Loss: 3.9657, Val Loss: 4.1896\n",
      "Epoch [169/500] Train Loss: 3.9658, Val Loss: 4.1898\n",
      "Epoch [170/500] Train Loss: 3.9662, Val Loss: 4.1898\n",
      "Epoch [171/500] Train Loss: 3.9664, Val Loss: 4.1911\n",
      "Epoch [172/500] Train Loss: 3.9663, Val Loss: 4.1896\n",
      "Epoch [173/500] Train Loss: 3.9649, Val Loss: 4.1913\n",
      "Epoch [174/500] Train Loss: 3.9648, Val Loss: 4.1913\n",
      "Epoch [175/500] Train Loss: 3.9643, Val Loss: 4.1883\n",
      "Epoch [176/500] Train Loss: 3.9650, Val Loss: 4.1903\n",
      "Epoch [177/500] Train Loss: 3.9645, Val Loss: 4.1874\n",
      "Epoch [178/500] Train Loss: 3.9641, Val Loss: 4.1877\n",
      "Epoch [179/500] Train Loss: 3.9642, Val Loss: 4.1880\n",
      "Epoch [180/500] Train Loss: 3.9644, Val Loss: 4.1874\n",
      "Epoch [181/500] Train Loss: 3.9640, Val Loss: 4.1887\n",
      "Epoch [182/500] Train Loss: 3.9635, Val Loss: 4.1901\n",
      "Epoch [183/500] Train Loss: 3.9633, Val Loss: 4.1895\n",
      "Epoch [184/500] Train Loss: 3.9632, Val Loss: 4.1882\n",
      "Epoch [185/500] Train Loss: 3.9645, Val Loss: 4.1860\n",
      "Epoch [186/500] Train Loss: 3.9631, Val Loss: 4.1893\n",
      "Epoch [187/500] Train Loss: 3.9641, Val Loss: 4.1884\n",
      "Epoch [188/500] Train Loss: 3.9644, Val Loss: 4.1860\n",
      "Epoch [189/500] Train Loss: 3.9627, Val Loss: 4.1858\n",
      "Epoch [190/500] Train Loss: 3.9624, Val Loss: 4.1864\n",
      "Epoch [191/500] Train Loss: 3.9621, Val Loss: 4.1861\n",
      "Epoch [192/500] Train Loss: 3.9624, Val Loss: 4.1865\n",
      "Epoch [193/500] Train Loss: 3.9625, Val Loss: 4.1882\n",
      "Epoch [194/500] Train Loss: 3.9621, Val Loss: 4.1878\n",
      "Epoch [195/500] Train Loss: 3.9632, Val Loss: 4.1903\n",
      "Epoch [196/500] Train Loss: 3.9619, Val Loss: 4.1879\n",
      "Epoch [197/500] Train Loss: 3.9618, Val Loss: 4.1853\n",
      "Epoch [198/500] Train Loss: 3.9614, Val Loss: 4.1867\n",
      "Epoch [199/500] Train Loss: 3.9614, Val Loss: 4.1847\n",
      "Epoch [200/500] Train Loss: 3.9625, Val Loss: 4.1863\n",
      "Epoch [201/500] Train Loss: 3.9611, Val Loss: 4.1857\n",
      "Epoch [202/500] Train Loss: 3.9610, Val Loss: 4.1821\n",
      "Epoch [203/500] Train Loss: 3.9614, Val Loss: 4.1830\n",
      "Epoch [204/500] Train Loss: 3.9611, Val Loss: 4.1815\n",
      "Epoch [205/500] Train Loss: 3.9606, Val Loss: 4.1821\n",
      "Epoch [206/500] Train Loss: 3.9620, Val Loss: 4.1848\n",
      "Epoch [207/500] Train Loss: 3.9603, Val Loss: 4.1813\n",
      "Epoch [208/500] Train Loss: 3.9604, Val Loss: 4.1816\n",
      "Epoch [209/500] Train Loss: 3.9609, Val Loss: 4.1827\n",
      "Epoch [210/500] Train Loss: 3.9602, Val Loss: 4.1816\n",
      "Epoch [211/500] Train Loss: 3.9599, Val Loss: 4.1839\n",
      "Epoch [212/500] Train Loss: 3.9606, Val Loss: 4.1817\n",
      "Epoch [213/500] Train Loss: 3.9597, Val Loss: 4.1849\n",
      "Epoch [214/500] Train Loss: 3.9598, Val Loss: 4.1839\n",
      "Epoch [215/500] Train Loss: 3.9596, Val Loss: 4.1857\n",
      "Epoch [216/500] Train Loss: 3.9596, Val Loss: 4.1854\n",
      "Epoch [217/500] Train Loss: 3.9595, Val Loss: 4.1839\n",
      "Epoch [218/500] Train Loss: 3.9591, Val Loss: 4.1833\n",
      "Epoch [219/500] Train Loss: 3.9588, Val Loss: 4.1822\n",
      "Epoch [220/500] Train Loss: 3.9594, Val Loss: 4.1802\n",
      "Epoch [221/500] Train Loss: 3.9593, Val Loss: 4.1823\n",
      "Epoch [222/500] Train Loss: 3.9585, Val Loss: 4.1824\n",
      "Epoch [223/500] Train Loss: 3.9587, Val Loss: 4.1808\n",
      "Epoch [224/500] Train Loss: 3.9592, Val Loss: 4.1842\n",
      "Epoch [225/500] Train Loss: 3.9595, Val Loss: 4.1796\n",
      "Epoch [226/500] Train Loss: 3.9589, Val Loss: 4.1836\n",
      "Epoch [227/500] Train Loss: 3.9583, Val Loss: 4.1810\n",
      "Epoch [228/500] Train Loss: 3.9582, Val Loss: 4.1827\n",
      "Epoch [229/500] Train Loss: 3.9581, Val Loss: 4.1822\n",
      "Epoch [230/500] Train Loss: 3.9579, Val Loss: 4.1812\n",
      "Epoch [231/500] Train Loss: 3.9585, Val Loss: 4.1842\n",
      "Epoch [232/500] Train Loss: 3.9576, Val Loss: 4.1825\n",
      "Epoch [233/500] Train Loss: 3.9572, Val Loss: 4.1810\n",
      "Epoch [234/500] Train Loss: 3.9574, Val Loss: 4.1802\n",
      "Epoch [235/500] Train Loss: 3.9572, Val Loss: 4.1812\n",
      "Epoch [236/500] Train Loss: 3.9583, Val Loss: 4.1806\n",
      "Epoch [237/500] Train Loss: 3.9581, Val Loss: 4.1768\n",
      "Epoch [238/500] Train Loss: 3.9578, Val Loss: 4.1779\n",
      "Epoch [239/500] Train Loss: 3.9571, Val Loss: 4.1776\n",
      "Epoch [240/500] Train Loss: 3.9570, Val Loss: 4.1789\n",
      "Epoch [241/500] Train Loss: 3.9576, Val Loss: 4.1806\n",
      "Epoch [242/500] Train Loss: 3.9569, Val Loss: 4.1815\n",
      "Epoch [243/500] Train Loss: 3.9577, Val Loss: 4.1819\n",
      "Epoch [244/500] Train Loss: 3.9565, Val Loss: 4.1801\n",
      "Epoch [245/500] Train Loss: 3.9569, Val Loss: 4.1784\n",
      "Epoch [246/500] Train Loss: 3.9570, Val Loss: 4.1778\n",
      "Epoch [247/500] Train Loss: 3.9571, Val Loss: 4.1785\n",
      "Epoch [248/500] Train Loss: 3.9563, Val Loss: 4.1776\n",
      "Epoch [249/500] Train Loss: 3.9572, Val Loss: 4.1792\n",
      "Epoch [250/500] Train Loss: 3.9566, Val Loss: 4.1768\n",
      "Epoch [251/500] Train Loss: 3.9562, Val Loss: 4.1779\n",
      "Epoch [252/500] Train Loss: 3.9565, Val Loss: 4.1793\n",
      "Epoch [253/500] Train Loss: 3.9558, Val Loss: 4.1788\n",
      "Epoch [254/500] Train Loss: 3.9558, Val Loss: 4.1793\n",
      "Epoch [255/500] Train Loss: 3.9563, Val Loss: 4.1776\n",
      "Epoch [256/500] Train Loss: 3.9560, Val Loss: 4.1817\n",
      "Epoch [257/500] Train Loss: 3.9557, Val Loss: 4.1812\n",
      "Epoch [258/500] Train Loss: 3.9558, Val Loss: 4.1809\n",
      "Epoch [259/500] Train Loss: 3.9580, Val Loss: 4.1830\n",
      "Epoch [260/500] Train Loss: 3.9561, Val Loss: 4.1837\n",
      "Epoch [261/500] Train Loss: 3.9566, Val Loss: 4.1792\n",
      "Epoch [262/500] Train Loss: 3.9557, Val Loss: 4.1787\n",
      "Epoch [263/500] Train Loss: 3.9551, Val Loss: 4.1775\n",
      "Epoch [264/500] Train Loss: 3.9549, Val Loss: 4.1779\n",
      "Epoch [265/500] Train Loss: 3.9559, Val Loss: 4.1811\n",
      "Epoch [266/500] Train Loss: 3.9562, Val Loss: 4.1785\n",
      "Epoch [267/500] Train Loss: 3.9549, Val Loss: 4.1798\n",
      "Epoch [268/500] Train Loss: 3.9553, Val Loss: 4.1790\n",
      "Epoch [269/500] Train Loss: 3.9552, Val Loss: 4.1797\n",
      "Epoch [270/500] Train Loss: 3.9547, Val Loss: 4.1790\n",
      "Epoch [271/500] Train Loss: 3.9549, Val Loss: 4.1788\n",
      "Epoch [272/500] Train Loss: 3.9551, Val Loss: 4.1793\n",
      "Epoch [273/500] Train Loss: 3.9549, Val Loss: 4.1797\n",
      "Epoch [274/500] Train Loss: 3.9549, Val Loss: 4.1804\n",
      "Epoch [275/500] Train Loss: 3.9559, Val Loss: 4.1778\n",
      "Epoch [276/500] Train Loss: 3.9545, Val Loss: 4.1801\n",
      "Epoch [277/500] Train Loss: 3.9557, Val Loss: 4.1814\n",
      "Epoch [278/500] Train Loss: 3.9541, Val Loss: 4.1795\n",
      "Epoch [279/500] Train Loss: 3.9542, Val Loss: 4.1788\n",
      "Epoch [280/500] Train Loss: 3.9540, Val Loss: 4.1775\n",
      "Epoch [281/500] Train Loss: 3.9537, Val Loss: 4.1771\n",
      "Epoch [282/500] Train Loss: 3.9538, Val Loss: 4.1758\n",
      "Epoch [283/500] Train Loss: 3.9538, Val Loss: 4.1769\n",
      "Epoch [284/500] Train Loss: 3.9545, Val Loss: 4.1778\n",
      "Epoch [285/500] Train Loss: 3.9537, Val Loss: 4.1761\n",
      "Epoch [286/500] Train Loss: 3.9538, Val Loss: 4.1762\n",
      "Epoch [287/500] Train Loss: 3.9534, Val Loss: 4.1780\n",
      "Epoch [288/500] Train Loss: 3.9539, Val Loss: 4.1777\n",
      "Epoch [289/500] Train Loss: 3.9546, Val Loss: 4.1767\n",
      "Epoch [290/500] Train Loss: 3.9531, Val Loss: 4.1758\n",
      "Epoch [291/500] Train Loss: 3.9544, Val Loss: 4.1762\n",
      "Epoch [292/500] Train Loss: 3.9531, Val Loss: 4.1769\n",
      "Epoch [293/500] Train Loss: 3.9539, Val Loss: 4.1746\n",
      "Epoch [294/500] Train Loss: 3.9530, Val Loss: 4.1771\n",
      "Epoch [295/500] Train Loss: 3.9537, Val Loss: 4.1793\n",
      "Epoch [296/500] Train Loss: 3.9531, Val Loss: 4.1766\n",
      "Epoch [297/500] Train Loss: 3.9545, Val Loss: 4.1766\n",
      "Epoch [298/500] Train Loss: 3.9530, Val Loss: 4.1764\n",
      "Epoch [299/500] Train Loss: 3.9525, Val Loss: 4.1773\n",
      "Epoch [300/500] Train Loss: 3.9534, Val Loss: 4.1785\n",
      "Epoch [301/500] Train Loss: 3.9522, Val Loss: 4.1748\n",
      "Epoch [302/500] Train Loss: 3.9532, Val Loss: 4.1730\n",
      "Epoch [303/500] Train Loss: 3.9527, Val Loss: 4.1766\n",
      "Epoch [304/500] Train Loss: 3.9532, Val Loss: 4.1791\n",
      "Epoch [305/500] Train Loss: 3.9525, Val Loss: 4.1772\n",
      "Epoch [306/500] Train Loss: 3.9521, Val Loss: 4.1784\n",
      "Epoch [307/500] Train Loss: 3.9524, Val Loss: 4.1779\n",
      "Epoch [308/500] Train Loss: 3.9527, Val Loss: 4.1760\n",
      "Epoch [309/500] Train Loss: 3.9536, Val Loss: 4.1794\n",
      "Epoch [310/500] Train Loss: 3.9533, Val Loss: 4.1744\n",
      "Epoch [311/500] Train Loss: 3.9522, Val Loss: 4.1764\n",
      "Epoch [312/500] Train Loss: 3.9526, Val Loss: 4.1763\n",
      "Epoch [313/500] Train Loss: 3.9517, Val Loss: 4.1744\n",
      "Epoch [314/500] Train Loss: 3.9517, Val Loss: 4.1734\n",
      "Epoch [315/500] Train Loss: 3.9528, Val Loss: 4.1743\n",
      "Epoch [316/500] Train Loss: 3.9528, Val Loss: 4.1749\n",
      "Epoch [317/500] Train Loss: 3.9522, Val Loss: 4.1743\n",
      "Epoch [318/500] Train Loss: 3.9518, Val Loss: 4.1768\n",
      "Epoch [319/500] Train Loss: 3.9531, Val Loss: 4.1754\n",
      "Epoch [320/500] Train Loss: 3.9513, Val Loss: 4.1739\n",
      "Epoch [321/500] Train Loss: 3.9513, Val Loss: 4.1757\n",
      "Epoch [322/500] Train Loss: 3.9515, Val Loss: 4.1752\n",
      "Epoch [323/500] Train Loss: 3.9519, Val Loss: 4.1745\n",
      "Epoch [324/500] Train Loss: 3.9521, Val Loss: 4.1738\n",
      "Epoch [325/500] Train Loss: 3.9517, Val Loss: 4.1754\n",
      "Epoch [326/500] Train Loss: 3.9509, Val Loss: 4.1750\n",
      "Epoch [327/500] Train Loss: 3.9510, Val Loss: 4.1743\n",
      "Epoch [328/500] Train Loss: 3.9521, Val Loss: 4.1739\n",
      "Epoch [329/500] Train Loss: 3.9508, Val Loss: 4.1742\n",
      "Epoch [330/500] Train Loss: 3.9507, Val Loss: 4.1756\n",
      "Epoch [331/500] Train Loss: 3.9507, Val Loss: 4.1751\n",
      "Epoch [332/500] Train Loss: 3.9510, Val Loss: 4.1764\n",
      "Epoch [333/500] Train Loss: 3.9508, Val Loss: 4.1736\n",
      "Epoch [334/500] Train Loss: 3.9510, Val Loss: 4.1739\n",
      "Epoch [335/500] Train Loss: 3.9507, Val Loss: 4.1727\n",
      "Epoch [336/500] Train Loss: 3.9507, Val Loss: 4.1739\n",
      "Epoch [337/500] Train Loss: 3.9503, Val Loss: 4.1735\n",
      "Epoch [338/500] Train Loss: 3.9501, Val Loss: 4.1735\n",
      "Epoch [339/500] Train Loss: 3.9509, Val Loss: 4.1727\n",
      "Epoch [340/500] Train Loss: 3.9500, Val Loss: 4.1742\n",
      "Epoch [341/500] Train Loss: 3.9507, Val Loss: 4.1735\n",
      "Epoch [342/500] Train Loss: 3.9503, Val Loss: 4.1761\n",
      "Epoch [343/500] Train Loss: 3.9510, Val Loss: 4.1726\n",
      "Epoch [344/500] Train Loss: 3.9511, Val Loss: 4.1722\n",
      "Epoch [345/500] Train Loss: 3.9501, Val Loss: 4.1758\n",
      "Epoch [346/500] Train Loss: 3.9498, Val Loss: 4.1751\n",
      "Epoch [347/500] Train Loss: 3.9531, Val Loss: 4.1712\n",
      "Epoch [348/500] Train Loss: 3.9500, Val Loss: 4.1735\n",
      "Epoch [349/500] Train Loss: 3.9493, Val Loss: 4.1732\n",
      "Epoch [350/500] Train Loss: 3.9503, Val Loss: 4.1761\n",
      "Epoch [351/500] Train Loss: 3.9506, Val Loss: 4.1751\n",
      "Epoch [352/500] Train Loss: 3.9496, Val Loss: 4.1749\n",
      "Epoch [353/500] Train Loss: 3.9492, Val Loss: 4.1731\n",
      "Epoch [354/500] Train Loss: 3.9489, Val Loss: 4.1726\n",
      "Epoch [355/500] Train Loss: 3.9489, Val Loss: 4.1749\n",
      "Epoch [356/500] Train Loss: 3.9496, Val Loss: 4.1744\n",
      "Epoch [357/500] Train Loss: 3.9501, Val Loss: 4.1746\n",
      "Epoch [358/500] Train Loss: 3.9493, Val Loss: 4.1732\n",
      "Epoch [359/500] Train Loss: 3.9494, Val Loss: 4.1752\n",
      "Epoch [360/500] Train Loss: 3.9486, Val Loss: 4.1733\n",
      "Epoch [361/500] Train Loss: 3.9487, Val Loss: 4.1733\n",
      "Epoch [362/500] Train Loss: 3.9487, Val Loss: 4.1735\n",
      "Epoch [363/500] Train Loss: 3.9484, Val Loss: 4.1733\n",
      "Epoch [364/500] Train Loss: 3.9492, Val Loss: 4.1746\n",
      "Epoch [365/500] Train Loss: 3.9492, Val Loss: 4.1711\n",
      "Epoch [366/500] Train Loss: 3.9491, Val Loss: 4.1751\n",
      "Epoch [367/500] Train Loss: 3.9486, Val Loss: 4.1745\n",
      "Epoch [368/500] Train Loss: 3.9497, Val Loss: 4.1744\n",
      "Epoch [369/500] Train Loss: 3.9512, Val Loss: 4.1792\n",
      "Epoch [370/500] Train Loss: 3.9487, Val Loss: 4.1723\n",
      "Epoch [371/500] Train Loss: 3.9483, Val Loss: 4.1715\n",
      "Epoch [372/500] Train Loss: 3.9491, Val Loss: 4.1739\n",
      "Epoch [373/500] Train Loss: 3.9484, Val Loss: 4.1724\n",
      "Epoch [374/500] Train Loss: 3.9473, Val Loss: 4.1716\n",
      "Epoch [375/500] Train Loss: 3.9487, Val Loss: 4.1740\n",
      "Epoch [376/500] Train Loss: 3.9484, Val Loss: 4.1725\n",
      "Epoch [377/500] Train Loss: 3.9491, Val Loss: 4.1731\n",
      "Epoch [378/500] Train Loss: 3.9506, Val Loss: 4.1713\n",
      "Epoch [379/500] Train Loss: 3.9484, Val Loss: 4.1724\n",
      "Epoch [380/500] Train Loss: 3.9489, Val Loss: 4.1702\n",
      "Epoch [381/500] Train Loss: 3.9468, Val Loss: 4.1706\n",
      "Epoch [382/500] Train Loss: 3.9474, Val Loss: 4.1720\n",
      "Epoch [383/500] Train Loss: 3.9470, Val Loss: 4.1719\n",
      "Epoch [384/500] Train Loss: 3.9473, Val Loss: 4.1731\n",
      "Epoch [385/500] Train Loss: 3.9474, Val Loss: 4.1697\n",
      "Epoch [386/500] Train Loss: 3.9475, Val Loss: 4.1700\n",
      "Epoch [387/500] Train Loss: 3.9474, Val Loss: 4.1714\n",
      "Epoch [388/500] Train Loss: 3.9469, Val Loss: 4.1698\n",
      "Epoch [389/500] Train Loss: 3.9462, Val Loss: 4.1707\n",
      "Epoch [390/500] Train Loss: 3.9464, Val Loss: 4.1716\n",
      "Epoch [391/500] Train Loss: 3.9467, Val Loss: 4.1726\n",
      "Epoch [392/500] Train Loss: 3.9467, Val Loss: 4.1703\n",
      "Epoch [393/500] Train Loss: 3.9468, Val Loss: 4.1730\n",
      "Epoch [394/500] Train Loss: 3.9467, Val Loss: 4.1698\n",
      "Epoch [395/500] Train Loss: 3.9454, Val Loss: 4.1713\n",
      "Epoch [396/500] Train Loss: 3.9459, Val Loss: 4.1710\n",
      "Epoch [397/500] Train Loss: 3.9463, Val Loss: 4.1712\n",
      "Epoch [398/500] Train Loss: 3.9455, Val Loss: 4.1715\n",
      "Epoch [399/500] Train Loss: 3.9453, Val Loss: 4.1692\n",
      "Epoch [400/500] Train Loss: 3.9465, Val Loss: 4.1691\n",
      "Epoch [401/500] Train Loss: 3.9456, Val Loss: 4.1711\n",
      "Epoch [402/500] Train Loss: 3.9462, Val Loss: 4.1750\n",
      "Epoch [403/500] Train Loss: 3.9456, Val Loss: 4.1742\n",
      "Epoch [404/500] Train Loss: 3.9455, Val Loss: 4.1728\n",
      "Epoch [405/500] Train Loss: 3.9444, Val Loss: 4.1690\n",
      "Epoch [406/500] Train Loss: 3.9455, Val Loss: 4.1682\n",
      "Epoch [407/500] Train Loss: 3.9452, Val Loss: 4.1687\n",
      "Epoch [408/500] Train Loss: 3.9450, Val Loss: 4.1685\n",
      "Epoch [409/500] Train Loss: 3.9447, Val Loss: 4.1698\n",
      "Epoch [410/500] Train Loss: 3.9442, Val Loss: 4.1710\n",
      "Epoch [411/500] Train Loss: 3.9463, Val Loss: 4.1730\n",
      "Epoch [412/500] Train Loss: 3.9444, Val Loss: 4.1705\n",
      "Epoch [413/500] Train Loss: 3.9449, Val Loss: 4.1680\n",
      "Epoch [414/500] Train Loss: 3.9443, Val Loss: 4.1675\n",
      "Epoch [415/500] Train Loss: 3.9436, Val Loss: 4.1702\n",
      "Epoch [416/500] Train Loss: 3.9443, Val Loss: 4.1729\n",
      "Epoch [417/500] Train Loss: 3.9448, Val Loss: 4.1739\n",
      "Epoch [418/500] Train Loss: 3.9441, Val Loss: 4.1712\n",
      "Epoch [419/500] Train Loss: 3.9453, Val Loss: 4.1661\n",
      "Epoch [420/500] Train Loss: 3.9439, Val Loss: 4.1672\n",
      "Epoch [421/500] Train Loss: 3.9432, Val Loss: 4.1681\n",
      "Epoch [422/500] Train Loss: 3.9448, Val Loss: 4.1728\n",
      "Epoch [423/500] Train Loss: 3.9443, Val Loss: 4.1718\n",
      "Epoch [424/500] Train Loss: 3.9445, Val Loss: 4.1681\n",
      "Epoch [425/500] Train Loss: 3.9431, Val Loss: 4.1687\n",
      "Epoch [426/500] Train Loss: 3.9436, Val Loss: 4.1673\n",
      "Epoch [427/500] Train Loss: 3.9425, Val Loss: 4.1681\n",
      "Epoch [428/500] Train Loss: 3.9426, Val Loss: 4.1688\n",
      "Epoch [429/500] Train Loss: 3.9426, Val Loss: 4.1676\n",
      "Epoch [430/500] Train Loss: 3.9425, Val Loss: 4.1665\n",
      "Epoch [431/500] Train Loss: 3.9423, Val Loss: 4.1663\n",
      "Epoch [432/500] Train Loss: 3.9422, Val Loss: 4.1679\n",
      "Epoch [433/500] Train Loss: 3.9428, Val Loss: 4.1723\n",
      "Epoch [434/500] Train Loss: 3.9426, Val Loss: 4.1707\n",
      "Epoch [435/500] Train Loss: 3.9423, Val Loss: 4.1686\n",
      "Epoch [436/500] Train Loss: 3.9411, Val Loss: 4.1659\n",
      "Epoch [437/500] Train Loss: 3.9412, Val Loss: 4.1674\n",
      "Epoch [438/500] Train Loss: 3.9420, Val Loss: 4.1696\n",
      "Epoch [439/500] Train Loss: 3.9422, Val Loss: 4.1732\n",
      "Epoch [440/500] Train Loss: 3.9416, Val Loss: 4.1672\n",
      "Epoch [441/500] Train Loss: 3.9410, Val Loss: 4.1665\n",
      "Epoch [442/500] Train Loss: 3.9400, Val Loss: 4.1679\n",
      "Epoch [443/500] Train Loss: 3.9425, Val Loss: 4.1690\n",
      "Epoch [444/500] Train Loss: 3.9404, Val Loss: 4.1681\n",
      "Epoch [445/500] Train Loss: 3.9399, Val Loss: 4.1660\n",
      "Epoch [446/500] Train Loss: 3.9423, Val Loss: 4.1623\n",
      "Epoch [447/500] Train Loss: 3.9409, Val Loss: 4.1669\n",
      "Epoch [448/500] Train Loss: 3.9389, Val Loss: 4.1630\n",
      "Epoch [449/500] Train Loss: 3.9412, Val Loss: 4.1647\n",
      "Epoch [450/500] Train Loss: 3.9399, Val Loss: 4.1650\n",
      "Epoch [451/500] Train Loss: 3.9400, Val Loss: 4.1651\n",
      "Epoch [452/500] Train Loss: 3.9393, Val Loss: 4.1633\n",
      "Epoch [453/500] Train Loss: 3.9385, Val Loss: 4.1652\n",
      "Epoch [454/500] Train Loss: 3.9389, Val Loss: 4.1665\n",
      "Epoch [455/500] Train Loss: 3.9382, Val Loss: 4.1651\n",
      "Epoch [456/500] Train Loss: 3.9379, Val Loss: 4.1653\n",
      "Epoch [457/500] Train Loss: 3.9380, Val Loss: 4.1646\n",
      "Epoch [458/500] Train Loss: 3.9379, Val Loss: 4.1652\n",
      "Epoch [459/500] Train Loss: 3.9403, Val Loss: 4.1583\n",
      "Epoch [460/500] Train Loss: 3.9382, Val Loss: 4.1649\n",
      "Epoch [461/500] Train Loss: 3.9369, Val Loss: 4.1644\n",
      "Epoch [462/500] Train Loss: 3.9371, Val Loss: 4.1609\n",
      "Epoch [463/500] Train Loss: 3.9366, Val Loss: 4.1632\n",
      "Epoch [464/500] Train Loss: 3.9372, Val Loss: 4.1647\n",
      "Epoch [465/500] Train Loss: 3.9374, Val Loss: 4.1600\n",
      "Epoch [466/500] Train Loss: 3.9350, Val Loss: 4.1633\n",
      "Epoch [467/500] Train Loss: 3.9359, Val Loss: 4.1666\n",
      "Epoch [468/500] Train Loss: 3.9366, Val Loss: 4.1677\n",
      "Epoch [469/500] Train Loss: 3.9363, Val Loss: 4.1610\n",
      "Epoch [470/500] Train Loss: 3.9365, Val Loss: 4.1616\n",
      "Epoch [471/500] Train Loss: 3.9367, Val Loss: 4.1601\n",
      "Epoch [472/500] Train Loss: 3.9357, Val Loss: 4.1600\n",
      "Epoch [473/500] Train Loss: 3.9340, Val Loss: 4.1575\n",
      "Epoch [474/500] Train Loss: 3.9335, Val Loss: 4.1598\n",
      "Epoch [475/500] Train Loss: 3.9336, Val Loss: 4.1594\n",
      "Epoch [476/500] Train Loss: 3.9341, Val Loss: 4.1579\n",
      "Epoch [477/500] Train Loss: 3.9332, Val Loss: 4.1594\n",
      "Epoch [478/500] Train Loss: 3.9361, Val Loss: 4.1638\n",
      "Epoch [479/500] Train Loss: 3.9344, Val Loss: 4.1548\n",
      "Epoch [480/500] Train Loss: 3.9342, Val Loss: 4.1607\n",
      "Epoch [481/500] Train Loss: 3.9349, Val Loss: 4.1526\n",
      "Epoch [482/500] Train Loss: 3.9335, Val Loss: 4.1574\n",
      "Epoch [483/500] Train Loss: 3.9314, Val Loss: 4.1535\n",
      "Epoch [484/500] Train Loss: 3.9308, Val Loss: 4.1557\n",
      "Epoch [485/500] Train Loss: 3.9327, Val Loss: 4.1516\n",
      "Epoch [486/500] Train Loss: 3.9342, Val Loss: 4.1606\n",
      "Epoch [487/500] Train Loss: 3.9320, Val Loss: 4.1586\n",
      "Epoch [488/500] Train Loss: 3.9303, Val Loss: 4.1537\n",
      "Epoch [489/500] Train Loss: 3.9299, Val Loss: 4.1519\n",
      "Epoch [490/500] Train Loss: 3.9302, Val Loss: 4.1490\n",
      "Epoch [491/500] Train Loss: 3.9286, Val Loss: 4.1510\n",
      "Epoch [492/500] Train Loss: 3.9281, Val Loss: 4.1503\n",
      "Epoch [493/500] Train Loss: 3.9278, Val Loss: 4.1514\n",
      "Epoch [494/500] Train Loss: 3.9277, Val Loss: 4.1529\n",
      "Epoch [495/500] Train Loss: 3.9264, Val Loss: 4.1484\n",
      "Epoch [496/500] Train Loss: 3.9270, Val Loss: 4.1470\n",
      "Epoch [497/500] Train Loss: 3.9269, Val Loss: 4.1506\n",
      "Epoch [498/500] Train Loss: 3.9255, Val Loss: 4.1486\n",
      "Epoch [499/500] Train Loss: 3.9251, Val Loss: 4.1474\n",
      "Epoch [500/500] Train Loss: 3.9254, Val Loss: 4.1475\n",
      "\n",
      "=== Fold 7 ===\n",
      "Epoch [1/500] Train Loss: 41.1384, Val Loss: 42.6482\n",
      "Epoch [2/500] Train Loss: 41.1258, Val Loss: 42.6374\n",
      "Epoch [3/500] Train Loss: 41.1135, Val Loss: 42.6266\n",
      "Epoch [4/500] Train Loss: 41.1009, Val Loss: 42.6156\n",
      "Epoch [5/500] Train Loss: 41.0881, Val Loss: 42.6039\n",
      "Epoch [6/500] Train Loss: 41.0743, Val Loss: 42.5920\n",
      "Epoch [7/500] Train Loss: 41.0599, Val Loss: 42.5774\n",
      "Epoch [8/500] Train Loss: 41.0398, Val Loss: 42.5542\n",
      "Epoch [9/500] Train Loss: 41.0082, Val Loss: 42.5242\n",
      "Epoch [10/500] Train Loss: 40.9707, Val Loss: 42.4807\n",
      "Epoch [11/500] Train Loss: 40.9039, Val Loss: 42.3958\n",
      "Epoch [12/500] Train Loss: 40.7869, Val Loss: 42.2653\n",
      "Epoch [13/500] Train Loss: 40.5856, Val Loss: 41.9814\n",
      "Epoch [14/500] Train Loss: 40.1254, Val Loss: 41.3243\n",
      "Epoch [15/500] Train Loss: 39.0825, Val Loss: 39.9291\n",
      "Epoch [16/500] Train Loss: 36.7739, Val Loss: 36.5289\n",
      "Epoch [17/500] Train Loss: 32.3982, Val Loss: 31.7481\n",
      "Epoch [18/500] Train Loss: 27.1753, Val Loss: 27.3415\n",
      "Epoch [19/500] Train Loss: 23.2241, Val Loss: 23.4339\n",
      "Epoch [20/500] Train Loss: 20.6695, Val Loss: 21.3676\n",
      "Epoch [21/500] Train Loss: 20.2352, Val Loss: 21.1256\n",
      "Epoch [22/500] Train Loss: 20.1090, Val Loss: 21.0994\n",
      "Epoch [23/500] Train Loss: 20.0019, Val Loss: 21.0516\n",
      "Epoch [24/500] Train Loss: 19.9209, Val Loss: 20.9653\n",
      "Epoch [25/500] Train Loss: 19.8446, Val Loss: 20.8682\n",
      "Epoch [26/500] Train Loss: 19.7688, Val Loss: 20.7823\n",
      "Epoch [27/500] Train Loss: 19.6927, Val Loss: 20.6572\n",
      "Epoch [28/500] Train Loss: 19.6185, Val Loss: 20.5283\n",
      "Epoch [29/500] Train Loss: 19.5463, Val Loss: 20.4627\n",
      "Epoch [30/500] Train Loss: 19.4734, Val Loss: 20.3973\n",
      "Epoch [31/500] Train Loss: 19.4006, Val Loss: 20.3063\n",
      "Epoch [32/500] Train Loss: 19.3306, Val Loss: 20.2125\n",
      "Epoch [33/500] Train Loss: 19.2588, Val Loss: 20.1375\n",
      "Epoch [34/500] Train Loss: 19.1878, Val Loss: 20.0276\n",
      "Epoch [35/500] Train Loss: 19.1165, Val Loss: 19.9480\n",
      "Epoch [36/500] Train Loss: 19.0459, Val Loss: 19.8732\n",
      "Epoch [37/500] Train Loss: 18.9755, Val Loss: 19.7791\n",
      "Epoch [38/500] Train Loss: 18.9046, Val Loss: 19.6517\n",
      "Epoch [39/500] Train Loss: 18.8361, Val Loss: 19.5986\n",
      "Epoch [40/500] Train Loss: 18.7657, Val Loss: 19.5288\n",
      "Epoch [41/500] Train Loss: 18.7020, Val Loss: 19.4411\n",
      "Epoch [42/500] Train Loss: 18.6398, Val Loss: 19.3903\n",
      "Epoch [43/500] Train Loss: 18.5838, Val Loss: 19.2878\n",
      "Epoch [44/500] Train Loss: 18.5261, Val Loss: 19.2372\n",
      "Epoch [45/500] Train Loss: 18.4754, Val Loss: 19.1989\n",
      "Epoch [46/500] Train Loss: 18.4237, Val Loss: 19.1062\n",
      "Epoch [47/500] Train Loss: 18.3707, Val Loss: 19.0580\n",
      "Epoch [48/500] Train Loss: 18.3278, Val Loss: 19.0263\n",
      "Epoch [49/500] Train Loss: 18.2862, Val Loss: 18.9664\n",
      "Epoch [50/500] Train Loss: 18.2468, Val Loss: 18.9183\n",
      "Epoch [51/500] Train Loss: 18.2116, Val Loss: 18.8928\n",
      "Epoch [52/500] Train Loss: 18.1819, Val Loss: 18.8757\n",
      "Epoch [53/500] Train Loss: 18.1577, Val Loss: 18.8490\n",
      "Epoch [54/500] Train Loss: 18.1346, Val Loss: 18.7916\n",
      "Epoch [55/500] Train Loss: 18.1176, Val Loss: 18.7917\n",
      "Epoch [56/500] Train Loss: 18.1013, Val Loss: 18.7697\n",
      "Epoch [57/500] Train Loss: 18.0897, Val Loss: 18.7673\n",
      "Epoch [58/500] Train Loss: 18.0802, Val Loss: 18.7724\n",
      "Epoch [59/500] Train Loss: 18.0723, Val Loss: 18.7654\n",
      "Epoch [60/500] Train Loss: 18.0658, Val Loss: 18.7510\n",
      "Epoch [61/500] Train Loss: 18.0611, Val Loss: 18.7406\n",
      "Epoch [62/500] Train Loss: 18.0568, Val Loss: 18.7434\n",
      "Epoch [63/500] Train Loss: 18.0527, Val Loss: 18.7412\n",
      "Epoch [64/500] Train Loss: 18.0473, Val Loss: 18.7542\n",
      "Epoch [65/500] Train Loss: 18.0442, Val Loss: 18.7718\n",
      "Epoch [66/500] Train Loss: 18.0386, Val Loss: 18.7320\n",
      "Epoch [67/500] Train Loss: 18.0352, Val Loss: 18.7303\n",
      "Epoch [68/500] Train Loss: 18.0312, Val Loss: 18.7544\n",
      "Epoch [69/500] Train Loss: 18.0276, Val Loss: 18.7427\n",
      "Epoch [70/500] Train Loss: 18.0245, Val Loss: 18.7345\n",
      "Epoch [71/500] Train Loss: 18.0210, Val Loss: 18.7374\n",
      "Epoch [72/500] Train Loss: 18.0192, Val Loss: 18.7342\n",
      "Epoch [73/500] Train Loss: 18.0171, Val Loss: 18.7074\n",
      "Epoch [74/500] Train Loss: 18.0156, Val Loss: 18.7154\n",
      "Epoch [75/500] Train Loss: 18.0134, Val Loss: 18.7058\n",
      "Epoch [76/500] Train Loss: 18.0132, Val Loss: 18.7315\n",
      "Epoch [77/500] Train Loss: 18.0091, Val Loss: 18.7135\n",
      "Epoch [78/500] Train Loss: 18.0103, Val Loss: 18.6990\n",
      "Epoch [79/500] Train Loss: 18.0068, Val Loss: 18.7217\n",
      "Epoch [80/500] Train Loss: 18.0049, Val Loss: 18.7129\n",
      "Epoch [81/500] Train Loss: 18.0037, Val Loss: 18.7347\n",
      "Epoch [82/500] Train Loss: 18.0007, Val Loss: 18.7160\n",
      "Epoch [83/500] Train Loss: 18.0020, Val Loss: 18.7116\n",
      "Epoch [84/500] Train Loss: 17.9961, Val Loss: 18.7063\n",
      "Epoch [85/500] Train Loss: 17.9939, Val Loss: 18.7031\n",
      "Epoch [86/500] Train Loss: 17.9921, Val Loss: 18.6970\n",
      "Epoch [87/500] Train Loss: 17.9890, Val Loss: 18.6938\n",
      "Epoch [88/500] Train Loss: 17.9862, Val Loss: 18.6975\n",
      "Epoch [89/500] Train Loss: 17.9823, Val Loss: 18.7090\n",
      "Epoch [90/500] Train Loss: 17.9781, Val Loss: 18.6962\n",
      "Epoch [91/500] Train Loss: 17.9732, Val Loss: 18.6758\n",
      "Epoch [92/500] Train Loss: 17.9683, Val Loss: 18.6925\n",
      "Epoch [93/500] Train Loss: 17.9598, Val Loss: 18.6680\n",
      "Epoch [94/500] Train Loss: 17.9600, Val Loss: 18.7022\n",
      "Epoch [95/500] Train Loss: 17.9447, Val Loss: 18.6121\n",
      "Epoch [96/500] Train Loss: 17.9197, Val Loss: 18.6393\n",
      "Epoch [97/500] Train Loss: 17.8944, Val Loss: 18.6008\n",
      "Epoch [98/500] Train Loss: 17.8618, Val Loss: 18.5623\n",
      "Epoch [99/500] Train Loss: 17.8049, Val Loss: 18.5345\n",
      "Epoch [100/500] Train Loss: 17.7136, Val Loss: 18.4260\n",
      "Epoch [101/500] Train Loss: 17.5677, Val Loss: 18.1917\n",
      "Epoch [102/500] Train Loss: 17.2837, Val Loss: 17.8551\n",
      "Epoch [103/500] Train Loss: 16.7561, Val Loss: 17.2832\n",
      "Epoch [104/500] Train Loss: 15.8632, Val Loss: 16.2197\n",
      "Epoch [105/500] Train Loss: 14.8912, Val Loss: 15.7119\n",
      "Epoch [106/500] Train Loss: 14.5363, Val Loss: 15.2002\n",
      "Epoch [107/500] Train Loss: 13.7403, Val Loss: 13.6541\n",
      "Epoch [108/500] Train Loss: 12.6424, Val Loss: 12.2872\n",
      "Epoch [109/500] Train Loss: 11.5311, Val Loss: 10.2456\n",
      "Epoch [110/500] Train Loss: 10.5513, Val Loss: 8.7545\n",
      "Epoch [111/500] Train Loss: 9.6007, Val Loss: 7.5963\n",
      "Epoch [112/500] Train Loss: 8.7675, Val Loss: 6.9106\n",
      "Epoch [113/500] Train Loss: 8.1093, Val Loss: 6.1901\n",
      "Epoch [114/500] Train Loss: 7.6259, Val Loss: 5.7009\n",
      "Epoch [115/500] Train Loss: 7.1496, Val Loss: 5.3207\n",
      "Epoch [116/500] Train Loss: 6.7624, Val Loss: 5.0432\n",
      "Epoch [117/500] Train Loss: 6.4459, Val Loss: 4.8233\n",
      "Epoch [118/500] Train Loss: 6.1583, Val Loss: 4.5796\n",
      "Epoch [119/500] Train Loss: 5.9116, Val Loss: 4.4242\n",
      "Epoch [120/500] Train Loss: 5.7049, Val Loss: 4.2906\n",
      "Epoch [121/500] Train Loss: 5.5227, Val Loss: 4.1665\n",
      "Epoch [122/500] Train Loss: 5.3742, Val Loss: 4.0499\n",
      "Epoch [123/500] Train Loss: 5.2409, Val Loss: 3.9807\n",
      "Epoch [124/500] Train Loss: 5.1197, Val Loss: 3.8731\n",
      "Epoch [125/500] Train Loss: 5.0156, Val Loss: 3.8086\n",
      "Epoch [126/500] Train Loss: 4.9176, Val Loss: 3.7284\n",
      "Epoch [127/500] Train Loss: 4.8171, Val Loss: 3.6659\n",
      "Epoch [128/500] Train Loss: 4.7241, Val Loss: 3.6086\n",
      "Epoch [129/500] Train Loss: 4.6391, Val Loss: 3.5342\n",
      "Epoch [130/500] Train Loss: 4.5606, Val Loss: 3.5005\n",
      "Epoch [131/500] Train Loss: 4.4957, Val Loss: 3.4388\n",
      "Epoch [132/500] Train Loss: 4.4263, Val Loss: 3.3932\n",
      "Epoch [133/500] Train Loss: 4.3742, Val Loss: 3.3754\n",
      "Epoch [134/500] Train Loss: 4.3252, Val Loss: 3.3390\n",
      "Epoch [135/500] Train Loss: 4.2812, Val Loss: 3.3069\n",
      "Epoch [136/500] Train Loss: 4.2467, Val Loss: 3.2894\n",
      "Epoch [137/500] Train Loss: 4.2148, Val Loss: 3.2739\n",
      "Epoch [138/500] Train Loss: 4.1881, Val Loss: 3.2556\n",
      "Epoch [139/500] Train Loss: 4.1643, Val Loss: 3.2559\n",
      "Epoch [140/500] Train Loss: 4.1446, Val Loss: 3.2476\n",
      "Epoch [141/500] Train Loss: 4.1270, Val Loss: 3.2364\n",
      "Epoch [142/500] Train Loss: 4.1142, Val Loss: 3.2286\n",
      "Epoch [143/500] Train Loss: 4.1022, Val Loss: 3.2290\n",
      "Epoch [144/500] Train Loss: 4.0903, Val Loss: 3.2308\n",
      "Epoch [145/500] Train Loss: 4.0818, Val Loss: 3.2302\n",
      "Epoch [146/500] Train Loss: 4.0728, Val Loss: 3.2203\n",
      "Epoch [147/500] Train Loss: 4.0668, Val Loss: 3.2233\n",
      "Epoch [148/500] Train Loss: 4.0623, Val Loss: 3.2086\n",
      "Epoch [149/500] Train Loss: 4.0536, Val Loss: 3.2267\n",
      "Epoch [150/500] Train Loss: 4.0478, Val Loss: 3.2218\n",
      "Epoch [151/500] Train Loss: 4.0402, Val Loss: 3.2171\n",
      "Epoch [152/500] Train Loss: 4.0339, Val Loss: 3.2202\n",
      "Epoch [153/500] Train Loss: 4.0280, Val Loss: 3.2103\n",
      "Epoch [154/500] Train Loss: 4.0228, Val Loss: 3.2057\n",
      "Epoch [155/500] Train Loss: 4.0177, Val Loss: 3.2012\n",
      "Epoch [156/500] Train Loss: 4.0123, Val Loss: 3.1898\n",
      "Epoch [157/500] Train Loss: 4.0088, Val Loss: 3.1785\n",
      "Epoch [158/500] Train Loss: 4.0030, Val Loss: 3.1743\n",
      "Epoch [159/500] Train Loss: 3.9994, Val Loss: 3.1657\n",
      "Epoch [160/500] Train Loss: 3.9957, Val Loss: 3.1616\n",
      "Epoch [161/500] Train Loss: 3.9922, Val Loss: 3.1638\n",
      "Epoch [162/500] Train Loss: 3.9893, Val Loss: 3.1646\n",
      "Epoch [163/500] Train Loss: 3.9853, Val Loss: 3.1501\n",
      "Epoch [164/500] Train Loss: 3.9817, Val Loss: 3.1445\n",
      "Epoch [165/500] Train Loss: 3.9776, Val Loss: 3.1369\n",
      "Epoch [166/500] Train Loss: 3.9749, Val Loss: 3.1323\n",
      "Epoch [167/500] Train Loss: 3.9708, Val Loss: 3.1251\n",
      "Epoch [168/500] Train Loss: 3.9670, Val Loss: 3.1213\n",
      "Epoch [169/500] Train Loss: 3.9641, Val Loss: 3.1131\n",
      "Epoch [170/500] Train Loss: 3.9625, Val Loss: 3.1176\n",
      "Epoch [171/500] Train Loss: 3.9579, Val Loss: 3.1065\n",
      "Epoch [172/500] Train Loss: 3.9550, Val Loss: 3.1077\n",
      "Epoch [173/500] Train Loss: 3.9523, Val Loss: 3.0993\n",
      "Epoch [174/500] Train Loss: 3.9482, Val Loss: 3.0948\n",
      "Epoch [175/500] Train Loss: 3.9458, Val Loss: 3.0939\n",
      "Epoch [176/500] Train Loss: 3.9426, Val Loss: 3.0825\n",
      "Epoch [177/500] Train Loss: 3.9418, Val Loss: 3.0755\n",
      "Epoch [178/500] Train Loss: 3.9382, Val Loss: 3.0743\n",
      "Epoch [179/500] Train Loss: 3.9368, Val Loss: 3.0666\n",
      "Epoch [180/500] Train Loss: 3.9319, Val Loss: 3.0665\n",
      "Epoch [181/500] Train Loss: 3.9304, Val Loss: 3.0687\n",
      "Epoch [182/500] Train Loss: 3.9262, Val Loss: 3.0654\n",
      "Epoch [183/500] Train Loss: 3.9253, Val Loss: 3.0655\n",
      "Epoch [184/500] Train Loss: 3.9237, Val Loss: 3.0618\n",
      "Epoch [185/500] Train Loss: 3.9203, Val Loss: 3.0494\n",
      "Epoch [186/500] Train Loss: 3.9173, Val Loss: 3.0368\n",
      "Epoch [187/500] Train Loss: 3.9146, Val Loss: 3.0454\n",
      "Epoch [188/500] Train Loss: 3.9122, Val Loss: 3.0404\n",
      "Epoch [189/500] Train Loss: 3.9106, Val Loss: 3.0365\n",
      "Epoch [190/500] Train Loss: 3.9080, Val Loss: 3.0313\n",
      "Epoch [191/500] Train Loss: 3.9039, Val Loss: 3.0342\n",
      "Epoch [192/500] Train Loss: 3.9016, Val Loss: 3.0357\n",
      "Epoch [193/500] Train Loss: 3.8989, Val Loss: 3.0222\n",
      "Epoch [194/500] Train Loss: 3.8973, Val Loss: 3.0213\n",
      "Epoch [195/500] Train Loss: 3.8944, Val Loss: 3.0113\n",
      "Epoch [196/500] Train Loss: 3.8912, Val Loss: 3.0201\n",
      "Epoch [197/500] Train Loss: 3.8876, Val Loss: 3.0129\n",
      "Epoch [198/500] Train Loss: 3.8857, Val Loss: 3.0046\n",
      "Epoch [199/500] Train Loss: 3.8836, Val Loss: 3.0048\n",
      "Epoch [200/500] Train Loss: 3.8794, Val Loss: 2.9998\n",
      "Epoch [201/500] Train Loss: 3.8777, Val Loss: 2.9977\n",
      "Epoch [202/500] Train Loss: 3.8762, Val Loss: 2.9854\n",
      "Epoch [203/500] Train Loss: 3.8725, Val Loss: 2.9800\n",
      "Epoch [204/500] Train Loss: 3.8697, Val Loss: 2.9795\n",
      "Epoch [205/500] Train Loss: 3.8671, Val Loss: 2.9682\n",
      "Epoch [206/500] Train Loss: 3.8644, Val Loss: 2.9739\n",
      "Epoch [207/500] Train Loss: 3.8601, Val Loss: 2.9607\n",
      "Epoch [208/500] Train Loss: 3.8586, Val Loss: 2.9557\n",
      "Epoch [209/500] Train Loss: 3.8567, Val Loss: 2.9600\n",
      "Epoch [210/500] Train Loss: 3.8520, Val Loss: 2.9597\n",
      "Epoch [211/500] Train Loss: 3.8517, Val Loss: 2.9495\n",
      "Epoch [212/500] Train Loss: 3.8488, Val Loss: 2.9595\n",
      "Epoch [213/500] Train Loss: 3.8421, Val Loss: 2.9492\n",
      "Epoch [214/500] Train Loss: 3.8372, Val Loss: 2.9553\n",
      "Epoch [215/500] Train Loss: 3.8381, Val Loss: 2.9476\n",
      "Epoch [216/500] Train Loss: 3.8293, Val Loss: 2.9374\n",
      "Epoch [217/500] Train Loss: 3.8251, Val Loss: 2.9306\n",
      "Epoch [218/500] Train Loss: 3.8221, Val Loss: 2.9150\n",
      "Epoch [219/500] Train Loss: 3.8185, Val Loss: 2.9266\n",
      "Epoch [220/500] Train Loss: 3.8167, Val Loss: 2.9109\n",
      "Epoch [221/500] Train Loss: 3.8139, Val Loss: 2.9137\n",
      "Epoch [222/500] Train Loss: 3.8096, Val Loss: 2.9127\n",
      "Epoch [223/500] Train Loss: 3.8045, Val Loss: 2.9082\n",
      "Epoch [224/500] Train Loss: 3.8000, Val Loss: 2.9083\n",
      "Epoch [225/500] Train Loss: 3.7953, Val Loss: 2.9039\n",
      "Epoch [226/500] Train Loss: 3.7907, Val Loss: 2.8994\n",
      "Epoch [227/500] Train Loss: 3.7895, Val Loss: 2.9011\n",
      "Epoch [228/500] Train Loss: 3.7848, Val Loss: 2.8848\n",
      "Epoch [229/500] Train Loss: 3.7788, Val Loss: 2.8964\n",
      "Epoch [230/500] Train Loss: 3.7772, Val Loss: 2.8875\n",
      "Epoch [231/500] Train Loss: 3.7713, Val Loss: 2.9000\n",
      "Epoch [232/500] Train Loss: 3.7674, Val Loss: 2.8870\n",
      "Epoch [233/500] Train Loss: 3.7635, Val Loss: 2.8696\n",
      "Epoch [234/500] Train Loss: 3.7614, Val Loss: 2.8817\n",
      "Epoch [235/500] Train Loss: 3.7583, Val Loss: 2.8697\n",
      "Epoch [236/500] Train Loss: 3.7479, Val Loss: 2.8642\n",
      "Epoch [237/500] Train Loss: 3.7427, Val Loss: 2.8807\n",
      "Epoch [238/500] Train Loss: 3.7405, Val Loss: 2.8635\n",
      "Epoch [239/500] Train Loss: 3.7364, Val Loss: 2.8587\n",
      "Epoch [240/500] Train Loss: 3.7309, Val Loss: 2.8639\n",
      "Epoch [241/500] Train Loss: 3.7242, Val Loss: 2.8580\n",
      "Epoch [242/500] Train Loss: 3.7208, Val Loss: 2.8526\n",
      "Epoch [243/500] Train Loss: 3.7160, Val Loss: 2.8430\n",
      "Epoch [244/500] Train Loss: 3.7122, Val Loss: 2.8452\n",
      "Epoch [245/500] Train Loss: 3.7077, Val Loss: 2.8451\n",
      "Epoch [246/500] Train Loss: 3.7048, Val Loss: 2.8535\n",
      "Epoch [247/500] Train Loss: 3.7012, Val Loss: 2.8353\n",
      "Epoch [248/500] Train Loss: 3.6997, Val Loss: 2.8465\n",
      "Epoch [249/500] Train Loss: 3.6929, Val Loss: 2.8226\n",
      "Epoch [250/500] Train Loss: 3.6895, Val Loss: 2.8351\n",
      "Epoch [251/500] Train Loss: 3.6840, Val Loss: 2.8096\n",
      "Epoch [252/500] Train Loss: 3.6757, Val Loss: 2.8317\n",
      "Epoch [253/500] Train Loss: 3.6785, Val Loss: 2.8216\n",
      "Epoch [254/500] Train Loss: 3.6710, Val Loss: 2.8295\n",
      "Epoch [255/500] Train Loss: 3.6712, Val Loss: 2.8099\n",
      "Epoch [256/500] Train Loss: 3.6642, Val Loss: 2.8198\n",
      "Epoch [257/500] Train Loss: 3.6615, Val Loss: 2.8195\n",
      "Epoch [258/500] Train Loss: 3.6594, Val Loss: 2.8095\n",
      "Epoch [259/500] Train Loss: 3.6519, Val Loss: 2.8121\n",
      "Epoch [260/500] Train Loss: 3.6469, Val Loss: 2.7969\n",
      "Epoch [261/500] Train Loss: 3.6394, Val Loss: 2.7764\n",
      "Epoch [262/500] Train Loss: 3.6335, Val Loss: 2.7792\n",
      "Epoch [263/500] Train Loss: 3.6294, Val Loss: 2.7717\n",
      "Epoch [264/500] Train Loss: 3.6261, Val Loss: 2.7673\n",
      "Epoch [265/500] Train Loss: 3.6220, Val Loss: 2.7637\n",
      "Epoch [266/500] Train Loss: 3.6182, Val Loss: 2.7589\n",
      "Epoch [267/500] Train Loss: 3.6140, Val Loss: 2.7672\n",
      "Epoch [268/500] Train Loss: 3.6096, Val Loss: 2.7618\n",
      "Epoch [269/500] Train Loss: 3.6050, Val Loss: 2.7435\n",
      "Epoch [270/500] Train Loss: 3.5949, Val Loss: 2.7608\n",
      "Epoch [271/500] Train Loss: 3.6028, Val Loss: 2.7382\n",
      "Epoch [272/500] Train Loss: 3.5888, Val Loss: 2.7495\n",
      "Epoch [273/500] Train Loss: 3.5779, Val Loss: 2.7222\n",
      "Epoch [274/500] Train Loss: 3.5710, Val Loss: 2.7201\n",
      "Epoch [275/500] Train Loss: 3.5677, Val Loss: 2.7220\n",
      "Epoch [276/500] Train Loss: 3.5616, Val Loss: 2.7206\n",
      "Epoch [277/500] Train Loss: 3.5558, Val Loss: 2.7223\n",
      "Epoch [278/500] Train Loss: 3.5438, Val Loss: 2.7295\n",
      "Epoch [279/500] Train Loss: 3.5355, Val Loss: 2.7418\n",
      "Epoch [280/500] Train Loss: 3.5219, Val Loss: 2.7308\n",
      "Epoch [281/500] Train Loss: 3.5095, Val Loss: 2.7189\n",
      "Epoch [282/500] Train Loss: 3.5018, Val Loss: 2.7211\n",
      "Epoch [283/500] Train Loss: 3.4887, Val Loss: 2.6979\n",
      "Epoch [284/500] Train Loss: 3.4796, Val Loss: 2.7086\n",
      "Epoch [285/500] Train Loss: 3.4650, Val Loss: 2.6871\n",
      "Epoch [286/500] Train Loss: 3.4626, Val Loss: 2.6904\n",
      "Epoch [287/500] Train Loss: 3.4489, Val Loss: 2.6752\n",
      "Epoch [288/500] Train Loss: 3.4265, Val Loss: 2.6655\n",
      "Epoch [289/500] Train Loss: 3.4132, Val Loss: 2.6464\n",
      "Epoch [290/500] Train Loss: 3.3884, Val Loss: 2.6742\n",
      "Epoch [291/500] Train Loss: 3.3302, Val Loss: 2.6794\n",
      "Epoch [292/500] Train Loss: 3.2815, Val Loss: 2.6245\n",
      "Epoch [293/500] Train Loss: 3.2108, Val Loss: 2.6278\n",
      "Epoch [294/500] Train Loss: 3.0924, Val Loss: 2.5175\n",
      "Epoch [295/500] Train Loss: 2.8947, Val Loss: 2.3440\n",
      "Epoch [296/500] Train Loss: 2.4620, Val Loss: 1.9355\n",
      "Epoch [297/500] Train Loss: 1.7810, Val Loss: 1.4199\n",
      "Epoch [298/500] Train Loss: 1.1664, Val Loss: 0.9874\n",
      "Epoch [299/500] Train Loss: 0.7812, Val Loss: 0.7941\n",
      "Epoch [300/500] Train Loss: 0.6264, Val Loss: 0.6575\n",
      "Epoch [301/500] Train Loss: 0.5172, Val Loss: 0.5522\n",
      "Epoch [302/500] Train Loss: 0.4472, Val Loss: 0.4737\n",
      "Epoch [303/500] Train Loss: 0.3857, Val Loss: 0.4281\n",
      "Epoch [304/500] Train Loss: 0.3387, Val Loss: 0.3905\n",
      "Epoch [305/500] Train Loss: 0.2994, Val Loss: 0.3439\n",
      "Epoch [306/500] Train Loss: 0.2670, Val Loss: 0.3140\n",
      "Epoch [307/500] Train Loss: 0.2411, Val Loss: 0.2948\n",
      "Epoch [308/500] Train Loss: 0.2172, Val Loss: 0.2774\n",
      "Epoch [309/500] Train Loss: 0.2043, Val Loss: 0.2438\n",
      "Epoch [310/500] Train Loss: 0.1846, Val Loss: 0.2309\n",
      "Epoch [311/500] Train Loss: 0.1710, Val Loss: 0.2115\n",
      "Epoch [312/500] Train Loss: 0.1608, Val Loss: 0.1961\n",
      "Epoch [313/500] Train Loss: 0.1492, Val Loss: 0.1837\n",
      "Epoch [314/500] Train Loss: 0.1397, Val Loss: 0.1817\n",
      "Epoch [315/500] Train Loss: 0.1334, Val Loss: 0.1762\n",
      "Epoch [316/500] Train Loss: 0.1266, Val Loss: 0.1592\n",
      "Epoch [317/500] Train Loss: 0.1194, Val Loss: 0.1522\n",
      "Epoch [318/500] Train Loss: 0.1145, Val Loss: 0.1445\n",
      "Epoch [319/500] Train Loss: 0.1102, Val Loss: 0.1418\n",
      "Epoch [320/500] Train Loss: 0.1041, Val Loss: 0.1336\n",
      "Epoch [321/500] Train Loss: 0.0992, Val Loss: 0.1257\n",
      "Epoch [322/500] Train Loss: 0.0942, Val Loss: 0.1193\n",
      "Epoch [323/500] Train Loss: 0.0913, Val Loss: 0.1153\n",
      "Epoch [324/500] Train Loss: 0.0866, Val Loss: 0.1090\n",
      "Epoch [325/500] Train Loss: 0.0832, Val Loss: 0.1050\n",
      "Epoch [326/500] Train Loss: 0.0805, Val Loss: 0.0990\n",
      "Epoch [327/500] Train Loss: 0.0777, Val Loss: 0.1004\n",
      "Epoch [328/500] Train Loss: 0.0736, Val Loss: 0.0944\n",
      "Epoch [329/500] Train Loss: 0.0706, Val Loss: 0.0901\n",
      "Epoch [330/500] Train Loss: 0.0687, Val Loss: 0.0866\n",
      "Epoch [331/500] Train Loss: 0.0655, Val Loss: 0.0853\n",
      "Epoch [332/500] Train Loss: 0.0627, Val Loss: 0.0822\n",
      "Epoch [333/500] Train Loss: 0.0596, Val Loss: 0.0784\n",
      "Epoch [334/500] Train Loss: 0.0567, Val Loss: 0.0762\n",
      "Epoch [335/500] Train Loss: 0.0544, Val Loss: 0.0738\n",
      "Epoch [336/500] Train Loss: 0.0523, Val Loss: 0.0717\n",
      "Epoch [337/500] Train Loss: 0.0508, Val Loss: 0.0699\n",
      "Epoch [338/500] Train Loss: 0.0490, Val Loss: 0.0678\n",
      "Epoch [339/500] Train Loss: 0.0477, Val Loss: 0.0665\n",
      "Epoch [340/500] Train Loss: 0.0462, Val Loss: 0.0648\n",
      "Epoch [341/500] Train Loss: 0.0447, Val Loss: 0.0632\n",
      "Epoch [342/500] Train Loss: 0.0431, Val Loss: 0.0604\n",
      "Epoch [343/500] Train Loss: 0.0427, Val Loss: 0.0622\n",
      "Epoch [344/500] Train Loss: 0.0428, Val Loss: 0.0594\n",
      "Epoch [345/500] Train Loss: 0.0406, Val Loss: 0.0589\n",
      "Epoch [346/500] Train Loss: 0.0392, Val Loss: 0.0551\n",
      "Epoch [347/500] Train Loss: 0.0383, Val Loss: 0.0572\n",
      "Epoch [348/500] Train Loss: 0.0375, Val Loss: 0.0540\n",
      "Epoch [349/500] Train Loss: 0.0358, Val Loss: 0.0521\n",
      "Epoch [350/500] Train Loss: 0.0345, Val Loss: 0.0519\n",
      "Epoch [351/500] Train Loss: 0.0336, Val Loss: 0.0508\n",
      "Epoch [352/500] Train Loss: 0.0326, Val Loss: 0.0498\n",
      "Epoch [353/500] Train Loss: 0.0317, Val Loss: 0.0489\n",
      "Epoch [354/500] Train Loss: 0.0312, Val Loss: 0.0477\n",
      "Epoch [355/500] Train Loss: 0.0300, Val Loss: 0.0465\n",
      "Epoch [356/500] Train Loss: 0.0297, Val Loss: 0.0476\n",
      "Epoch [357/500] Train Loss: 0.0293, Val Loss: 0.0454\n",
      "Epoch [358/500] Train Loss: 0.0284, Val Loss: 0.0443\n",
      "Epoch [359/500] Train Loss: 0.0275, Val Loss: 0.0437\n",
      "Epoch [360/500] Train Loss: 0.0266, Val Loss: 0.0426\n",
      "Epoch [361/500] Train Loss: 0.0259, Val Loss: 0.0421\n",
      "Epoch [362/500] Train Loss: 0.0254, Val Loss: 0.0405\n",
      "Epoch [363/500] Train Loss: 0.0251, Val Loss: 0.0398\n",
      "Epoch [364/500] Train Loss: 0.0243, Val Loss: 0.0375\n",
      "Epoch [365/500] Train Loss: 0.0235, Val Loss: 0.0361\n",
      "Epoch [366/500] Train Loss: 0.0233, Val Loss: 0.0351\n",
      "Epoch [367/500] Train Loss: 0.0224, Val Loss: 0.0346\n",
      "Epoch [368/500] Train Loss: 0.0225, Val Loss: 0.0354\n",
      "Epoch [369/500] Train Loss: 0.0229, Val Loss: 0.0318\n",
      "Epoch [370/500] Train Loss: 0.0212, Val Loss: 0.0308\n",
      "Epoch [371/500] Train Loss: 0.0209, Val Loss: 0.0302\n",
      "Epoch [372/500] Train Loss: 0.0204, Val Loss: 0.0295\n",
      "Epoch [373/500] Train Loss: 0.0198, Val Loss: 0.0290\n",
      "Epoch [374/500] Train Loss: 0.0196, Val Loss: 0.0306\n",
      "Epoch [375/500] Train Loss: 0.0200, Val Loss: 0.0276\n",
      "Epoch [376/500] Train Loss: 0.0187, Val Loss: 0.0287\n",
      "Epoch [377/500] Train Loss: 0.0194, Val Loss: 0.0287\n",
      "Epoch [378/500] Train Loss: 0.0186, Val Loss: 0.0271\n",
      "Epoch [379/500] Train Loss: 0.0185, Val Loss: 0.0253\n",
      "Epoch [380/500] Train Loss: 0.0179, Val Loss: 0.0256\n",
      "Epoch [381/500] Train Loss: 0.0173, Val Loss: 0.0242\n",
      "Epoch [382/500] Train Loss: 0.0167, Val Loss: 0.0237\n",
      "Epoch [383/500] Train Loss: 0.0166, Val Loss: 0.0251\n",
      "Epoch [384/500] Train Loss: 0.0165, Val Loss: 0.0226\n",
      "Epoch [385/500] Train Loss: 0.0155, Val Loss: 0.0225\n",
      "Epoch [386/500] Train Loss: 0.0153, Val Loss: 0.0215\n",
      "Epoch [387/500] Train Loss: 0.0151, Val Loss: 0.0221\n",
      "Epoch [388/500] Train Loss: 0.0150, Val Loss: 0.0217\n",
      "Epoch [389/500] Train Loss: 0.0150, Val Loss: 0.0217\n",
      "Epoch [390/500] Train Loss: 0.0151, Val Loss: 0.0200\n",
      "Epoch [391/500] Train Loss: 0.0144, Val Loss: 0.0194\n",
      "Epoch [392/500] Train Loss: 0.0141, Val Loss: 0.0192\n",
      "Epoch [393/500] Train Loss: 0.0133, Val Loss: 0.0187\n",
      "Epoch [394/500] Train Loss: 0.0134, Val Loss: 0.0189\n",
      "Epoch [395/500] Train Loss: 0.0149, Val Loss: 0.0215\n",
      "Epoch [396/500] Train Loss: 0.0139, Val Loss: 0.0193\n",
      "Epoch [397/500] Train Loss: 0.0131, Val Loss: 0.0175\n",
      "Epoch [398/500] Train Loss: 0.0122, Val Loss: 0.0171\n",
      "Epoch [399/500] Train Loss: 0.0120, Val Loss: 0.0175\n",
      "Epoch [400/500] Train Loss: 0.0119, Val Loss: 0.0165\n",
      "Epoch [401/500] Train Loss: 0.0116, Val Loss: 0.0162\n",
      "Epoch [402/500] Train Loss: 0.0116, Val Loss: 0.0160\n",
      "Epoch [403/500] Train Loss: 0.0112, Val Loss: 0.0163\n",
      "Epoch [404/500] Train Loss: 0.0110, Val Loss: 0.0153\n",
      "Epoch [405/500] Train Loss: 0.0109, Val Loss: 0.0156\n",
      "Epoch [406/500] Train Loss: 0.0106, Val Loss: 0.0149\n",
      "Epoch [407/500] Train Loss: 0.0103, Val Loss: 0.0155\n",
      "Epoch [408/500] Train Loss: 0.0106, Val Loss: 0.0155\n",
      "Epoch [409/500] Train Loss: 0.0104, Val Loss: 0.0145\n",
      "Epoch [410/500] Train Loss: 0.0100, Val Loss: 0.0144\n",
      "Epoch [411/500] Train Loss: 0.0097, Val Loss: 0.0138\n",
      "Epoch [412/500] Train Loss: 0.0098, Val Loss: 0.0147\n",
      "Epoch [413/500] Train Loss: 0.0100, Val Loss: 0.0136\n",
      "Epoch [414/500] Train Loss: 0.0098, Val Loss: 0.0133\n",
      "Epoch [415/500] Train Loss: 0.0094, Val Loss: 0.0132\n",
      "Epoch [416/500] Train Loss: 0.0095, Val Loss: 0.0133\n",
      "Epoch [417/500] Train Loss: 0.0094, Val Loss: 0.0145\n",
      "Epoch [418/500] Train Loss: 0.0096, Val Loss: 0.0136\n",
      "Epoch [419/500] Train Loss: 0.0093, Val Loss: 0.0135\n",
      "Epoch [420/500] Train Loss: 0.0090, Val Loss: 0.0130\n",
      "Epoch [421/500] Train Loss: 0.0088, Val Loss: 0.0121\n",
      "Epoch [422/500] Train Loss: 0.0086, Val Loss: 0.0126\n",
      "Epoch [423/500] Train Loss: 0.0084, Val Loss: 0.0132\n",
      "Epoch [424/500] Train Loss: 0.0086, Val Loss: 0.0123\n",
      "Epoch [425/500] Train Loss: 0.0081, Val Loss: 0.0118\n",
      "Epoch [426/500] Train Loss: 0.0082, Val Loss: 0.0119\n",
      "Epoch [427/500] Train Loss: 0.0081, Val Loss: 0.0134\n",
      "Epoch [428/500] Train Loss: 0.0084, Val Loss: 0.0122\n",
      "Epoch [429/500] Train Loss: 0.0080, Val Loss: 0.0123\n",
      "Epoch [430/500] Train Loss: 0.0083, Val Loss: 0.0110\n",
      "Epoch [431/500] Train Loss: 0.0085, Val Loss: 0.0115\n",
      "Epoch [432/500] Train Loss: 0.0076, Val Loss: 0.0106\n",
      "Epoch [433/500] Train Loss: 0.0072, Val Loss: 0.0105\n",
      "Epoch [434/500] Train Loss: 0.0073, Val Loss: 0.0105\n",
      "Epoch [435/500] Train Loss: 0.0071, Val Loss: 0.0103\n",
      "Epoch [436/500] Train Loss: 0.0070, Val Loss: 0.0103\n",
      "Epoch [437/500] Train Loss: 0.0068, Val Loss: 0.0100\n",
      "Epoch [438/500] Train Loss: 0.0069, Val Loss: 0.0105\n",
      "Epoch [439/500] Train Loss: 0.0071, Val Loss: 0.0102\n",
      "Epoch [440/500] Train Loss: 0.0070, Val Loss: 0.0099\n",
      "Epoch [441/500] Train Loss: 0.0068, Val Loss: 0.0109\n",
      "Epoch [442/500] Train Loss: 0.0069, Val Loss: 0.0110\n",
      "Epoch [443/500] Train Loss: 0.0073, Val Loss: 0.0098\n",
      "Epoch [444/500] Train Loss: 0.0065, Val Loss: 0.0094\n",
      "Epoch [445/500] Train Loss: 0.0064, Val Loss: 0.0091\n",
      "Epoch [446/500] Train Loss: 0.0062, Val Loss: 0.0091\n",
      "Epoch [447/500] Train Loss: 0.0063, Val Loss: 0.0097\n",
      "Epoch [448/500] Train Loss: 0.0066, Val Loss: 0.0089\n",
      "Epoch [449/500] Train Loss: 0.0063, Val Loss: 0.0089\n",
      "Epoch [450/500] Train Loss: 0.0063, Val Loss: 0.0118\n",
      "Epoch [451/500] Train Loss: 0.0068, Val Loss: 0.0088\n",
      "Epoch [452/500] Train Loss: 0.0061, Val Loss: 0.0088\n",
      "Epoch [453/500] Train Loss: 0.0058, Val Loss: 0.0095\n",
      "Epoch [454/500] Train Loss: 0.0059, Val Loss: 0.0088\n",
      "Epoch [455/500] Train Loss: 0.0068, Val Loss: 0.0111\n",
      "Epoch [456/500] Train Loss: 0.0063, Val Loss: 0.0093\n",
      "Epoch [457/500] Train Loss: 0.0060, Val Loss: 0.0087\n",
      "Epoch [458/500] Train Loss: 0.0056, Val Loss: 0.0087\n",
      "Epoch [459/500] Train Loss: 0.0059, Val Loss: 0.0082\n",
      "Epoch [460/500] Train Loss: 0.0055, Val Loss: 0.0085\n",
      "Epoch [461/500] Train Loss: 0.0057, Val Loss: 0.0078\n",
      "Epoch [462/500] Train Loss: 0.0054, Val Loss: 0.0078\n",
      "Epoch [463/500] Train Loss: 0.0055, Val Loss: 0.0079\n",
      "Epoch [464/500] Train Loss: 0.0054, Val Loss: 0.0082\n",
      "Epoch [465/500] Train Loss: 0.0052, Val Loss: 0.0075\n",
      "Epoch [466/500] Train Loss: 0.0052, Val Loss: 0.0075\n",
      "Epoch [467/500] Train Loss: 0.0053, Val Loss: 0.0085\n",
      "Epoch [468/500] Train Loss: 0.0055, Val Loss: 0.0075\n",
      "Epoch [469/500] Train Loss: 0.0050, Val Loss: 0.0084\n",
      "Epoch [470/500] Train Loss: 0.0054, Val Loss: 0.0077\n",
      "Epoch [471/500] Train Loss: 0.0052, Val Loss: 0.0074\n",
      "Epoch [472/500] Train Loss: 0.0059, Val Loss: 0.0102\n",
      "Epoch [473/500] Train Loss: 0.0059, Val Loss: 0.0071\n",
      "Epoch [474/500] Train Loss: 0.0051, Val Loss: 0.0075\n",
      "Epoch [475/500] Train Loss: 0.0051, Val Loss: 0.0073\n",
      "Epoch [476/500] Train Loss: 0.0050, Val Loss: 0.0070\n",
      "Epoch [477/500] Train Loss: 0.0046, Val Loss: 0.0069\n",
      "Epoch [478/500] Train Loss: 0.0050, Val Loss: 0.0079\n",
      "Epoch [479/500] Train Loss: 0.0052, Val Loss: 0.0076\n",
      "Epoch [480/500] Train Loss: 0.0060, Val Loss: 0.0092\n",
      "Epoch [481/500] Train Loss: 0.0056, Val Loss: 0.0065\n",
      "Epoch [482/500] Train Loss: 0.0047, Val Loss: 0.0067\n",
      "Epoch [483/500] Train Loss: 0.0045, Val Loss: 0.0064\n",
      "Epoch [484/500] Train Loss: 0.0043, Val Loss: 0.0063\n",
      "Epoch [485/500] Train Loss: 0.0044, Val Loss: 0.0069\n",
      "Epoch [486/500] Train Loss: 0.0043, Val Loss: 0.0064\n",
      "Epoch [487/500] Train Loss: 0.0044, Val Loss: 0.0062\n",
      "Epoch [488/500] Train Loss: 0.0047, Val Loss: 0.0062\n",
      "Epoch [489/500] Train Loss: 0.0045, Val Loss: 0.0063\n",
      "Epoch [490/500] Train Loss: 0.0046, Val Loss: 0.0071\n",
      "Epoch [491/500] Train Loss: 0.0046, Val Loss: 0.0063\n",
      "Epoch [492/500] Train Loss: 0.0048, Val Loss: 0.0067\n",
      "Epoch [493/500] Train Loss: 0.0044, Val Loss: 0.0061\n",
      "Epoch [494/500] Train Loss: 0.0043, Val Loss: 0.0057\n",
      "Epoch [495/500] Train Loss: 0.0042, Val Loss: 0.0064\n",
      "Epoch [496/500] Train Loss: 0.0040, Val Loss: 0.0056\n",
      "Epoch [497/500] Train Loss: 0.0040, Val Loss: 0.0062\n",
      "Epoch [498/500] Train Loss: 0.0044, Val Loss: 0.0056\n",
      "Epoch [499/500] Train Loss: 0.0044, Val Loss: 0.0062\n",
      "Epoch [500/500] Train Loss: 0.0043, Val Loss: 0.0069\n",
      "\n",
      "=== Fold 8 ===\n",
      "Epoch [1/500] Train Loss: 41.6088, Val Loss: 40.3437\n",
      "Epoch [2/500] Train Loss: 41.5877, Val Loss: 40.3229\n",
      "Epoch [3/500] Train Loss: 41.5657, Val Loss: 40.2990\n",
      "Epoch [4/500] Train Loss: 41.5386, Val Loss: 40.2666\n",
      "Epoch [5/500] Train Loss: 41.4993, Val Loss: 40.2203\n",
      "Epoch [6/500] Train Loss: 41.4412, Val Loss: 40.1397\n",
      "Epoch [7/500] Train Loss: 41.3166, Val Loss: 39.9280\n",
      "Epoch [8/500] Train Loss: 40.8455, Val Loss: 39.1017\n",
      "Epoch [9/500] Train Loss: 39.5119, Val Loss: 37.1604\n",
      "Epoch [10/500] Train Loss: 36.5902, Val Loss: 33.1386\n",
      "Epoch [11/500] Train Loss: 31.0120, Val Loss: 26.4055\n",
      "Epoch [12/500] Train Loss: 23.4920, Val Loss: 20.6988\n",
      "Epoch [13/500] Train Loss: 20.5180, Val Loss: 20.9447\n",
      "Epoch [14/500] Train Loss: 20.3122, Val Loss: 20.4365\n",
      "Epoch [15/500] Train Loss: 20.1169, Val Loss: 20.3787\n",
      "Epoch [16/500] Train Loss: 20.0338, Val Loss: 20.3402\n",
      "Epoch [17/500] Train Loss: 19.9672, Val Loss: 20.2759\n",
      "Epoch [18/500] Train Loss: 19.9001, Val Loss: 20.2216\n",
      "Epoch [19/500] Train Loss: 19.8364, Val Loss: 20.1647\n",
      "Epoch [20/500] Train Loss: 19.7761, Val Loss: 20.1106\n",
      "Epoch [21/500] Train Loss: 19.7201, Val Loss: 20.0579\n",
      "Epoch [22/500] Train Loss: 19.6651, Val Loss: 20.0037\n",
      "Epoch [23/500] Train Loss: 19.6088, Val Loss: 19.9526\n",
      "Epoch [24/500] Train Loss: 19.5565, Val Loss: 19.8969\n",
      "Epoch [25/500] Train Loss: 19.4981, Val Loss: 19.8367\n",
      "Epoch [26/500] Train Loss: 19.4406, Val Loss: 19.7814\n",
      "Epoch [27/500] Train Loss: 19.3812, Val Loss: 19.7214\n",
      "Epoch [28/500] Train Loss: 19.3193, Val Loss: 19.6570\n",
      "Epoch [29/500] Train Loss: 19.2596, Val Loss: 19.5921\n",
      "Epoch [30/500] Train Loss: 19.1841, Val Loss: 19.5285\n",
      "Epoch [31/500] Train Loss: 19.1158, Val Loss: 19.4521\n",
      "Epoch [32/500] Train Loss: 19.0405, Val Loss: 19.3745\n",
      "Epoch [33/500] Train Loss: 18.9567, Val Loss: 19.2871\n",
      "Epoch [34/500] Train Loss: 18.8679, Val Loss: 19.1968\n",
      "Epoch [35/500] Train Loss: 18.7828, Val Loss: 19.1125\n",
      "Epoch [36/500] Train Loss: 18.7022, Val Loss: 19.0320\n",
      "Epoch [37/500] Train Loss: 18.6133, Val Loss: 18.9524\n",
      "Epoch [38/500] Train Loss: 18.5349, Val Loss: 18.8815\n",
      "Epoch [39/500] Train Loss: 18.4665, Val Loss: 18.8198\n",
      "Epoch [40/500] Train Loss: 18.4072, Val Loss: 18.7715\n",
      "Epoch [41/500] Train Loss: 18.3616, Val Loss: 18.7311\n",
      "Epoch [42/500] Train Loss: 18.3228, Val Loss: 18.7011\n",
      "Epoch [43/500] Train Loss: 18.2944, Val Loss: 18.6793\n",
      "Epoch [44/500] Train Loss: 18.2730, Val Loss: 18.6664\n",
      "Epoch [45/500] Train Loss: 18.2589, Val Loss: 18.6524\n",
      "Epoch [46/500] Train Loss: 18.2433, Val Loss: 18.6350\n",
      "Epoch [47/500] Train Loss: 18.2315, Val Loss: 18.6273\n",
      "Epoch [48/500] Train Loss: 18.2200, Val Loss: 18.6143\n",
      "Epoch [49/500] Train Loss: 18.2094, Val Loss: 18.5973\n",
      "Epoch [50/500] Train Loss: 18.1987, Val Loss: 18.5888\n",
      "Epoch [51/500] Train Loss: 18.1863, Val Loss: 18.5737\n",
      "Epoch [52/500] Train Loss: 18.1769, Val Loss: 18.5623\n",
      "Epoch [53/500] Train Loss: 18.1656, Val Loss: 18.5517\n",
      "Epoch [54/500] Train Loss: 18.1557, Val Loss: 18.5373\n",
      "Epoch [55/500] Train Loss: 18.1465, Val Loss: 18.5285\n",
      "Epoch [56/500] Train Loss: 18.1371, Val Loss: 18.5242\n",
      "Epoch [57/500] Train Loss: 18.1288, Val Loss: 18.5086\n",
      "Epoch [58/500] Train Loss: 18.1193, Val Loss: 18.5002\n",
      "Epoch [59/500] Train Loss: 18.1115, Val Loss: 18.4903\n",
      "Epoch [60/500] Train Loss: 18.1033, Val Loss: 18.4825\n",
      "Epoch [61/500] Train Loss: 18.0959, Val Loss: 18.4732\n",
      "Epoch [62/500] Train Loss: 18.0881, Val Loss: 18.4652\n",
      "Epoch [63/500] Train Loss: 18.0808, Val Loss: 18.4595\n",
      "Epoch [64/500] Train Loss: 18.0743, Val Loss: 18.4518\n",
      "Epoch [65/500] Train Loss: 18.0691, Val Loss: 18.4454\n",
      "Epoch [66/500] Train Loss: 18.0633, Val Loss: 18.4362\n",
      "Epoch [67/500] Train Loss: 18.0563, Val Loss: 18.4304\n",
      "Epoch [68/500] Train Loss: 18.0510, Val Loss: 18.4263\n",
      "Epoch [69/500] Train Loss: 18.0465, Val Loss: 18.4230\n",
      "Epoch [70/500] Train Loss: 18.0408, Val Loss: 18.4158\n",
      "Epoch [71/500] Train Loss: 18.0356, Val Loss: 18.4100\n",
      "Epoch [72/500] Train Loss: 18.0309, Val Loss: 18.4031\n",
      "Epoch [73/500] Train Loss: 18.0253, Val Loss: 18.3985\n",
      "Epoch [74/500] Train Loss: 18.0199, Val Loss: 18.3918\n",
      "Epoch [75/500] Train Loss: 18.0136, Val Loss: 18.3871\n",
      "Epoch [76/500] Train Loss: 18.0071, Val Loss: 18.3816\n",
      "Epoch [77/500] Train Loss: 18.0018, Val Loss: 18.3777\n",
      "Epoch [78/500] Train Loss: 17.9921, Val Loss: 18.3681\n",
      "Epoch [79/500] Train Loss: 17.9815, Val Loss: 18.3564\n",
      "Epoch [80/500] Train Loss: 17.9701, Val Loss: 18.3442\n",
      "Epoch [81/500] Train Loss: 17.9509, Val Loss: 18.3109\n",
      "Epoch [82/500] Train Loss: 17.8982, Val Loss: 18.2771\n",
      "Epoch [83/500] Train Loss: 17.8546, Val Loss: 18.2394\n",
      "Epoch [84/500] Train Loss: 17.7953, Val Loss: 18.1930\n",
      "Epoch [85/500] Train Loss: 17.7252, Val Loss: 18.1193\n",
      "Epoch [86/500] Train Loss: 16.9952, Val Loss: 15.2680\n",
      "Epoch [87/500] Train Loss: 13.6948, Val Loss: 13.0277\n",
      "Epoch [88/500] Train Loss: 12.2402, Val Loss: 11.7307\n",
      "Epoch [89/500] Train Loss: 11.1613, Val Loss: 10.9088\n",
      "Epoch [90/500] Train Loss: 10.3796, Val Loss: 10.1771\n",
      "Epoch [91/500] Train Loss: 9.7840, Val Loss: 9.6873\n",
      "Epoch [92/500] Train Loss: 9.3168, Val Loss: 9.2528\n",
      "Epoch [93/500] Train Loss: 8.9291, Val Loss: 8.9116\n",
      "Epoch [94/500] Train Loss: 8.6128, Val Loss: 8.6113\n",
      "Epoch [95/500] Train Loss: 8.3243, Val Loss: 8.3454\n",
      "Epoch [96/500] Train Loss: 8.0608, Val Loss: 8.0362\n",
      "Epoch [97/500] Train Loss: 7.7122, Val Loss: 7.5918\n",
      "Epoch [98/500] Train Loss: 7.2315, Val Loss: 6.9606\n",
      "Epoch [99/500] Train Loss: 6.6974, Val Loss: 6.3885\n",
      "Epoch [100/500] Train Loss: 6.2314, Val Loss: 5.8728\n",
      "Epoch [101/500] Train Loss: 5.8277, Val Loss: 5.4312\n",
      "Epoch [102/500] Train Loss: 5.4743, Val Loss: 5.0382\n",
      "Epoch [103/500] Train Loss: 5.1289, Val Loss: 4.6440\n",
      "Epoch [104/500] Train Loss: 4.8542, Val Loss: 4.3767\n",
      "Epoch [105/500] Train Loss: 4.6506, Val Loss: 4.2096\n",
      "Epoch [106/500] Train Loss: 4.5115, Val Loss: 4.0879\n",
      "Epoch [107/500] Train Loss: 4.4272, Val Loss: 4.0094\n",
      "Epoch [108/500] Train Loss: 4.3691, Val Loss: 3.9696\n",
      "Epoch [109/500] Train Loss: 4.3280, Val Loss: 3.9356\n",
      "Epoch [110/500] Train Loss: 4.2954, Val Loss: 3.9151\n",
      "Epoch [111/500] Train Loss: 4.2709, Val Loss: 3.8894\n",
      "Epoch [112/500] Train Loss: 4.2484, Val Loss: 3.8669\n",
      "Epoch [113/500] Train Loss: 4.2313, Val Loss: 3.8486\n",
      "Epoch [114/500] Train Loss: 4.2125, Val Loss: 3.8323\n",
      "Epoch [115/500] Train Loss: 4.1965, Val Loss: 3.8130\n",
      "Epoch [116/500] Train Loss: 4.1825, Val Loss: 3.7960\n",
      "Epoch [117/500] Train Loss: 4.1660, Val Loss: 3.7847\n",
      "Epoch [118/500] Train Loss: 4.1527, Val Loss: 3.7647\n",
      "Epoch [119/500] Train Loss: 4.1421, Val Loss: 3.7499\n",
      "Epoch [120/500] Train Loss: 4.1298, Val Loss: 3.7428\n",
      "Epoch [121/500] Train Loss: 4.1190, Val Loss: 3.7256\n",
      "Epoch [122/500] Train Loss: 4.1073, Val Loss: 3.7169\n",
      "Epoch [123/500] Train Loss: 4.0992, Val Loss: 3.7050\n",
      "Epoch [124/500] Train Loss: 4.0921, Val Loss: 3.6988\n",
      "Epoch [125/500] Train Loss: 4.0836, Val Loss: 3.6901\n",
      "Epoch [126/500] Train Loss: 4.0774, Val Loss: 3.6838\n",
      "Epoch [127/500] Train Loss: 4.0717, Val Loss: 3.6788\n",
      "Epoch [128/500] Train Loss: 4.0679, Val Loss: 3.6802\n",
      "Epoch [129/500] Train Loss: 4.0637, Val Loss: 3.6688\n",
      "Epoch [130/500] Train Loss: 4.0591, Val Loss: 3.6658\n",
      "Epoch [131/500] Train Loss: 4.0554, Val Loss: 3.6659\n",
      "Epoch [132/500] Train Loss: 4.0531, Val Loss: 3.6568\n",
      "Epoch [133/500] Train Loss: 4.0478, Val Loss: 3.6536\n",
      "Epoch [134/500] Train Loss: 4.0443, Val Loss: 3.6537\n",
      "Epoch [135/500] Train Loss: 4.0423, Val Loss: 3.6476\n",
      "Epoch [136/500] Train Loss: 4.0387, Val Loss: 3.6457\n",
      "Epoch [137/500] Train Loss: 4.0360, Val Loss: 3.6411\n",
      "Epoch [138/500] Train Loss: 4.0334, Val Loss: 3.6384\n",
      "Epoch [139/500] Train Loss: 4.0312, Val Loss: 3.6390\n",
      "Epoch [140/500] Train Loss: 4.0290, Val Loss: 3.6344\n",
      "Epoch [141/500] Train Loss: 4.0272, Val Loss: 3.6336\n",
      "Epoch [142/500] Train Loss: 4.0250, Val Loss: 3.6331\n",
      "Epoch [143/500] Train Loss: 4.0266, Val Loss: 3.6293\n",
      "Epoch [144/500] Train Loss: 4.0226, Val Loss: 3.6347\n",
      "Epoch [145/500] Train Loss: 4.0214, Val Loss: 3.6250\n",
      "Epoch [146/500] Train Loss: 4.0187, Val Loss: 3.6243\n",
      "Epoch [147/500] Train Loss: 4.0165, Val Loss: 3.6252\n",
      "Epoch [148/500] Train Loss: 4.0155, Val Loss: 3.6304\n",
      "Epoch [149/500] Train Loss: 4.0166, Val Loss: 3.6226\n",
      "Epoch [150/500] Train Loss: 4.0116, Val Loss: 3.6170\n",
      "Epoch [151/500] Train Loss: 4.0112, Val Loss: 3.6150\n",
      "Epoch [152/500] Train Loss: 4.0086, Val Loss: 3.6132\n",
      "Epoch [153/500] Train Loss: 4.0078, Val Loss: 3.6117\n",
      "Epoch [154/500] Train Loss: 4.0054, Val Loss: 3.6113\n",
      "Epoch [155/500] Train Loss: 4.0047, Val Loss: 3.6109\n",
      "Epoch [156/500] Train Loss: 4.0026, Val Loss: 3.6076\n",
      "Epoch [157/500] Train Loss: 4.0005, Val Loss: 3.6050\n",
      "Epoch [158/500] Train Loss: 3.9993, Val Loss: 3.6032\n",
      "Epoch [159/500] Train Loss: 3.9978, Val Loss: 3.6026\n",
      "Epoch [160/500] Train Loss: 3.9959, Val Loss: 3.5994\n",
      "Epoch [161/500] Train Loss: 3.9936, Val Loss: 3.6021\n",
      "Epoch [162/500] Train Loss: 3.9911, Val Loss: 3.5950\n",
      "Epoch [163/500] Train Loss: 3.9898, Val Loss: 3.5989\n",
      "Epoch [164/500] Train Loss: 3.9868, Val Loss: 3.5927\n",
      "Epoch [165/500] Train Loss: 3.9843, Val Loss: 3.5901\n",
      "Epoch [166/500] Train Loss: 3.9809, Val Loss: 3.5870\n",
      "Epoch [167/500] Train Loss: 3.9788, Val Loss: 3.5855\n",
      "Epoch [168/500] Train Loss: 3.9758, Val Loss: 3.5832\n",
      "Epoch [169/500] Train Loss: 3.9735, Val Loss: 3.5814\n",
      "Epoch [170/500] Train Loss: 3.9722, Val Loss: 3.5803\n",
      "Epoch [171/500] Train Loss: 3.9685, Val Loss: 3.5762\n",
      "Epoch [172/500] Train Loss: 3.9670, Val Loss: 3.5774\n",
      "Epoch [173/500] Train Loss: 3.9649, Val Loss: 3.5738\n",
      "Epoch [174/500] Train Loss: 3.9607, Val Loss: 3.5716\n",
      "Epoch [175/500] Train Loss: 3.9582, Val Loss: 3.5709\n",
      "Epoch [176/500] Train Loss: 3.9561, Val Loss: 3.5661\n",
      "Epoch [177/500] Train Loss: 3.9542, Val Loss: 3.5649\n",
      "Epoch [178/500] Train Loss: 3.9491, Val Loss: 3.5626\n",
      "Epoch [179/500] Train Loss: 3.9469, Val Loss: 3.5609\n",
      "Epoch [180/500] Train Loss: 3.9457, Val Loss: 3.5592\n",
      "Epoch [181/500] Train Loss: 3.9420, Val Loss: 3.5568\n",
      "Epoch [182/500] Train Loss: 3.9384, Val Loss: 3.5576\n",
      "Epoch [183/500] Train Loss: 3.9343, Val Loss: 3.5538\n",
      "Epoch [184/500] Train Loss: 3.9311, Val Loss: 3.5519\n",
      "Epoch [185/500] Train Loss: 3.9273, Val Loss: 3.5524\n",
      "Epoch [186/500] Train Loss: 3.9265, Val Loss: 3.5458\n",
      "Epoch [187/500] Train Loss: 3.9214, Val Loss: 3.5456\n",
      "Epoch [188/500] Train Loss: 3.9185, Val Loss: 3.5433\n",
      "Epoch [189/500] Train Loss: 3.9159, Val Loss: 3.5402\n",
      "Epoch [190/500] Train Loss: 3.9106, Val Loss: 3.5439\n",
      "Epoch [191/500] Train Loss: 3.9086, Val Loss: 3.5385\n",
      "Epoch [192/500] Train Loss: 3.9048, Val Loss: 3.5367\n",
      "Epoch [193/500] Train Loss: 3.9012, Val Loss: 3.5366\n",
      "Epoch [194/500] Train Loss: 3.8951, Val Loss: 3.5345\n",
      "Epoch [195/500] Train Loss: 3.8918, Val Loss: 3.5354\n",
      "Epoch [196/500] Train Loss: 3.8900, Val Loss: 3.5433\n",
      "Epoch [197/500] Train Loss: 3.8921, Val Loss: 3.5359\n",
      "Epoch [198/500] Train Loss: 3.8830, Val Loss: 3.5247\n",
      "Epoch [199/500] Train Loss: 3.8755, Val Loss: 3.5255\n",
      "Epoch [200/500] Train Loss: 3.8724, Val Loss: 3.5310\n",
      "Epoch [201/500] Train Loss: 3.8673, Val Loss: 3.5278\n",
      "Epoch [202/500] Train Loss: 3.8617, Val Loss: 3.5269\n",
      "Epoch [203/500] Train Loss: 3.8576, Val Loss: 3.5360\n",
      "Epoch [204/500] Train Loss: 3.8533, Val Loss: 3.5368\n",
      "Epoch [205/500] Train Loss: 3.8493, Val Loss: 3.5301\n",
      "Epoch [206/500] Train Loss: 3.8429, Val Loss: 3.5318\n",
      "Epoch [207/500] Train Loss: 3.8401, Val Loss: 3.5406\n",
      "Epoch [208/500] Train Loss: 3.8367, Val Loss: 3.5472\n",
      "Epoch [209/500] Train Loss: 3.8308, Val Loss: 3.5277\n",
      "Epoch [210/500] Train Loss: 3.8259, Val Loss: 3.5385\n",
      "Epoch [211/500] Train Loss: 3.8222, Val Loss: 3.5296\n",
      "Epoch [212/500] Train Loss: 3.8151, Val Loss: 3.5272\n",
      "Epoch [213/500] Train Loss: 3.8108, Val Loss: 3.5202\n",
      "Epoch [214/500] Train Loss: 3.8080, Val Loss: 3.5220\n",
      "Epoch [215/500] Train Loss: 3.8028, Val Loss: 3.5162\n",
      "Epoch [216/500] Train Loss: 3.8000, Val Loss: 3.5101\n",
      "Epoch [217/500] Train Loss: 3.7923, Val Loss: 3.5051\n",
      "Epoch [218/500] Train Loss: 3.7869, Val Loss: 3.5047\n",
      "Epoch [219/500] Train Loss: 3.7797, Val Loss: 3.4968\n",
      "Epoch [220/500] Train Loss: 3.7755, Val Loss: 3.4929\n",
      "Epoch [221/500] Train Loss: 3.7716, Val Loss: 3.4900\n",
      "Epoch [222/500] Train Loss: 3.7689, Val Loss: 3.4928\n",
      "Epoch [223/500] Train Loss: 3.7652, Val Loss: 3.4824\n",
      "Epoch [224/500] Train Loss: 3.7634, Val Loss: 3.4858\n",
      "Epoch [225/500] Train Loss: 3.7621, Val Loss: 3.4775\n",
      "Epoch [226/500] Train Loss: 3.7582, Val Loss: 3.4849\n",
      "Epoch [227/500] Train Loss: 3.7599, Val Loss: 3.4805\n",
      "Epoch [228/500] Train Loss: 3.7529, Val Loss: 3.4729\n",
      "Epoch [229/500] Train Loss: 3.7528, Val Loss: 3.4741\n",
      "Epoch [230/500] Train Loss: 3.7500, Val Loss: 3.4706\n",
      "Epoch [231/500] Train Loss: 3.7460, Val Loss: 3.4701\n",
      "Epoch [232/500] Train Loss: 3.7457, Val Loss: 3.4650\n",
      "Epoch [233/500] Train Loss: 3.7413, Val Loss: 3.4624\n",
      "Epoch [234/500] Train Loss: 3.7379, Val Loss: 3.4629\n",
      "Epoch [235/500] Train Loss: 3.7365, Val Loss: 3.4642\n",
      "Epoch [236/500] Train Loss: 3.7372, Val Loss: 3.4651\n",
      "Epoch [237/500] Train Loss: 3.7336, Val Loss: 3.4539\n",
      "Epoch [238/500] Train Loss: 3.7294, Val Loss: 3.4502\n",
      "Epoch [239/500] Train Loss: 3.7235, Val Loss: 3.4459\n",
      "Epoch [240/500] Train Loss: 3.7201, Val Loss: 3.4420\n",
      "Epoch [241/500] Train Loss: 3.7183, Val Loss: 3.4393\n",
      "Epoch [242/500] Train Loss: 3.7116, Val Loss: 3.4375\n",
      "Epoch [243/500] Train Loss: 3.7098, Val Loss: 3.4335\n",
      "Epoch [244/500] Train Loss: 3.7051, Val Loss: 3.4357\n",
      "Epoch [245/500] Train Loss: 3.6978, Val Loss: 3.4241\n",
      "Epoch [246/500] Train Loss: 3.6938, Val Loss: 3.4212\n",
      "Epoch [247/500] Train Loss: 3.6840, Val Loss: 3.4106\n",
      "Epoch [248/500] Train Loss: 3.6745, Val Loss: 3.4019\n",
      "Epoch [249/500] Train Loss: 3.6505, Val Loss: 3.4041\n",
      "Epoch [250/500] Train Loss: 3.5890, Val Loss: 3.3120\n",
      "Epoch [251/500] Train Loss: 3.4955, Val Loss: 3.2284\n",
      "Epoch [252/500] Train Loss: 3.4133, Val Loss: 3.1515\n",
      "Epoch [253/500] Train Loss: 3.3323, Val Loss: 3.0638\n",
      "Epoch [254/500] Train Loss: 3.2356, Val Loss: 2.9573\n",
      "Epoch [255/500] Train Loss: 3.1141, Val Loss: 2.8388\n",
      "Epoch [256/500] Train Loss: 2.9714, Val Loss: 2.6868\n",
      "Epoch [257/500] Train Loss: 2.8089, Val Loss: 2.5431\n",
      "Epoch [258/500] Train Loss: 2.6419, Val Loss: 2.3901\n",
      "Epoch [259/500] Train Loss: 2.4457, Val Loss: 2.1774\n",
      "Epoch [260/500] Train Loss: 2.2159, Val Loss: 1.9991\n",
      "Epoch [261/500] Train Loss: 2.0268, Val Loss: 1.8418\n",
      "Epoch [262/500] Train Loss: 1.8373, Val Loss: 1.6553\n",
      "Epoch [263/500] Train Loss: 1.6574, Val Loss: 1.4874\n",
      "Epoch [264/500] Train Loss: 1.4854, Val Loss: 1.3372\n",
      "Epoch [265/500] Train Loss: 1.3354, Val Loss: 1.1942\n",
      "Epoch [266/500] Train Loss: 1.1880, Val Loss: 1.0677\n",
      "Epoch [267/500] Train Loss: 1.0610, Val Loss: 0.9572\n",
      "Epoch [268/500] Train Loss: 0.9482, Val Loss: 0.8624\n",
      "Epoch [269/500] Train Loss: 0.8436, Val Loss: 0.7791\n",
      "Epoch [270/500] Train Loss: 0.7570, Val Loss: 0.7073\n",
      "Epoch [271/500] Train Loss: 0.6789, Val Loss: 0.6303\n",
      "Epoch [272/500] Train Loss: 0.6147, Val Loss: 0.5793\n",
      "Epoch [273/500] Train Loss: 0.5685, Val Loss: 0.5612\n",
      "Epoch [274/500] Train Loss: 0.5190, Val Loss: 0.5033\n",
      "Epoch [275/500] Train Loss: 0.4717, Val Loss: 0.4626\n",
      "Epoch [276/500] Train Loss: 0.4321, Val Loss: 0.4380\n",
      "Epoch [277/500] Train Loss: 0.4042, Val Loss: 0.4149\n",
      "Epoch [278/500] Train Loss: 0.3776, Val Loss: 0.3781\n",
      "Epoch [279/500] Train Loss: 0.3531, Val Loss: 0.3526\n",
      "Epoch [280/500] Train Loss: 0.3304, Val Loss: 0.3326\n",
      "Epoch [281/500] Train Loss: 0.3107, Val Loss: 0.3147\n",
      "Epoch [282/500] Train Loss: 0.2957, Val Loss: 0.3042\n",
      "Epoch [283/500] Train Loss: 0.2765, Val Loss: 0.2832\n",
      "Epoch [284/500] Train Loss: 0.2606, Val Loss: 0.2731\n",
      "Epoch [285/500] Train Loss: 0.2471, Val Loss: 0.2550\n",
      "Epoch [286/500] Train Loss: 0.2345, Val Loss: 0.2373\n",
      "Epoch [287/500] Train Loss: 0.2209, Val Loss: 0.2236\n",
      "Epoch [288/500] Train Loss: 0.2090, Val Loss: 0.2132\n",
      "Epoch [289/500] Train Loss: 0.1997, Val Loss: 0.2017\n",
      "Epoch [290/500] Train Loss: 0.1903, Val Loss: 0.1946\n",
      "Epoch [291/500] Train Loss: 0.1838, Val Loss: 0.1841\n",
      "Epoch [292/500] Train Loss: 0.1752, Val Loss: 0.1764\n",
      "Epoch [293/500] Train Loss: 0.1687, Val Loss: 0.1688\n",
      "Epoch [294/500] Train Loss: 0.1625, Val Loss: 0.1614\n",
      "Epoch [295/500] Train Loss: 0.1557, Val Loss: 0.1554\n",
      "Epoch [296/500] Train Loss: 0.1506, Val Loss: 0.1583\n",
      "Epoch [297/500] Train Loss: 0.1493, Val Loss: 0.1562\n",
      "Epoch [298/500] Train Loss: 0.1424, Val Loss: 0.1389\n",
      "Epoch [299/500] Train Loss: 0.1371, Val Loss: 0.1337\n",
      "Epoch [300/500] Train Loss: 0.1309, Val Loss: 0.1284\n",
      "Epoch [301/500] Train Loss: 0.1266, Val Loss: 0.1243\n",
      "Epoch [302/500] Train Loss: 0.1247, Val Loss: 0.1235\n",
      "Epoch [303/500] Train Loss: 0.1192, Val Loss: 0.1200\n",
      "Epoch [304/500] Train Loss: 0.1167, Val Loss: 0.1116\n",
      "Epoch [305/500] Train Loss: 0.1139, Val Loss: 0.1084\n",
      "Epoch [306/500] Train Loss: 0.1095, Val Loss: 0.1051\n",
      "Epoch [307/500] Train Loss: 0.1092, Val Loss: 0.1032\n",
      "Epoch [308/500] Train Loss: 0.1079, Val Loss: 0.0987\n",
      "Epoch [309/500] Train Loss: 0.1056, Val Loss: 0.0948\n",
      "Epoch [310/500] Train Loss: 0.1029, Val Loss: 0.1089\n",
      "Epoch [311/500] Train Loss: 0.1084, Val Loss: 0.0941\n",
      "Epoch [312/500] Train Loss: 0.0971, Val Loss: 0.0874\n",
      "Epoch [313/500] Train Loss: 0.0934, Val Loss: 0.0843\n",
      "Epoch [314/500] Train Loss: 0.0888, Val Loss: 0.0822\n",
      "Epoch [315/500] Train Loss: 0.0870, Val Loss: 0.0815\n",
      "Epoch [316/500] Train Loss: 0.0856, Val Loss: 0.0777\n",
      "Epoch [317/500] Train Loss: 0.0842, Val Loss: 0.0805\n",
      "Epoch [318/500] Train Loss: 0.0829, Val Loss: 0.0751\n",
      "Epoch [319/500] Train Loss: 0.0797, Val Loss: 0.0738\n",
      "Epoch [320/500] Train Loss: 0.0794, Val Loss: 0.0726\n",
      "Epoch [321/500] Train Loss: 0.0782, Val Loss: 0.0674\n",
      "Epoch [322/500] Train Loss: 0.0740, Val Loss: 0.0671\n",
      "Epoch [323/500] Train Loss: 0.0728, Val Loss: 0.0639\n",
      "Epoch [324/500] Train Loss: 0.0708, Val Loss: 0.0623\n",
      "Epoch [325/500] Train Loss: 0.0697, Val Loss: 0.0607\n",
      "Epoch [326/500] Train Loss: 0.0731, Val Loss: 0.0638\n",
      "Epoch [327/500] Train Loss: 0.0721, Val Loss: 0.0582\n",
      "Epoch [328/500] Train Loss: 0.0657, Val Loss: 0.0559\n",
      "Epoch [329/500] Train Loss: 0.0641, Val Loss: 0.0547\n",
      "Epoch [330/500] Train Loss: 0.0634, Val Loss: 0.0542\n",
      "Epoch [331/500] Train Loss: 0.0621, Val Loss: 0.0546\n",
      "Epoch [332/500] Train Loss: 0.0596, Val Loss: 0.0507\n",
      "Epoch [333/500] Train Loss: 0.0591, Val Loss: 0.0518\n",
      "Epoch [334/500] Train Loss: 0.0581, Val Loss: 0.0476\n",
      "Epoch [335/500] Train Loss: 0.0558, Val Loss: 0.0529\n",
      "Epoch [336/500] Train Loss: 0.0582, Val Loss: 0.0465\n",
      "Epoch [337/500] Train Loss: 0.0552, Val Loss: 0.0441\n",
      "Epoch [338/500] Train Loss: 0.0532, Val Loss: 0.0432\n",
      "Epoch [339/500] Train Loss: 0.0532, Val Loss: 0.0475\n",
      "Epoch [340/500] Train Loss: 0.0524, Val Loss: 0.0417\n",
      "Epoch [341/500] Train Loss: 0.0496, Val Loss: 0.0400\n",
      "Epoch [342/500] Train Loss: 0.0497, Val Loss: 0.0428\n",
      "Epoch [343/500] Train Loss: 0.0491, Val Loss: 0.0385\n",
      "Epoch [344/500] Train Loss: 0.0473, Val Loss: 0.0383\n",
      "Epoch [345/500] Train Loss: 0.0466, Val Loss: 0.0387\n",
      "Epoch [346/500] Train Loss: 0.0454, Val Loss: 0.0356\n",
      "Epoch [347/500] Train Loss: 0.0465, Val Loss: 0.0380\n",
      "Epoch [348/500] Train Loss: 0.0477, Val Loss: 0.0365\n",
      "Epoch [349/500] Train Loss: 0.0440, Val Loss: 0.0345\n",
      "Epoch [350/500] Train Loss: 0.0460, Val Loss: 0.0380\n",
      "Epoch [351/500] Train Loss: 0.0462, Val Loss: 0.0373\n",
      "Epoch [352/500] Train Loss: 0.0436, Val Loss: 0.0355\n",
      "Epoch [353/500] Train Loss: 0.0409, Val Loss: 0.0319\n",
      "Epoch [354/500] Train Loss: 0.0402, Val Loss: 0.0309\n",
      "Epoch [355/500] Train Loss: 0.0394, Val Loss: 0.0309\n",
      "Epoch [356/500] Train Loss: 0.0388, Val Loss: 0.0293\n",
      "Epoch [357/500] Train Loss: 0.0385, Val Loss: 0.0352\n",
      "Epoch [358/500] Train Loss: 0.0392, Val Loss: 0.0284\n",
      "Epoch [359/500] Train Loss: 0.0374, Val Loss: 0.0287\n",
      "Epoch [360/500] Train Loss: 0.0373, Val Loss: 0.0307\n",
      "Epoch [361/500] Train Loss: 0.0373, Val Loss: 0.0289\n",
      "Epoch [362/500] Train Loss: 0.0349, Val Loss: 0.0286\n",
      "Epoch [363/500] Train Loss: 0.0348, Val Loss: 0.0270\n",
      "Epoch [364/500] Train Loss: 0.0345, Val Loss: 0.0289\n",
      "Epoch [365/500] Train Loss: 0.0339, Val Loss: 0.0313\n",
      "Epoch [366/500] Train Loss: 0.0354, Val Loss: 0.0270\n",
      "Epoch [367/500] Train Loss: 0.0330, Val Loss: 0.0253\n",
      "Epoch [368/500] Train Loss: 0.0344, Val Loss: 0.0405\n",
      "Epoch [369/500] Train Loss: 0.0386, Val Loss: 0.0266\n",
      "Epoch [370/500] Train Loss: 0.0351, Val Loss: 0.0299\n",
      "Epoch [371/500] Train Loss: 0.0327, Val Loss: 0.0241\n",
      "Epoch [372/500] Train Loss: 0.0316, Val Loss: 0.0230\n",
      "Epoch [373/500] Train Loss: 0.0321, Val Loss: 0.0225\n",
      "Epoch [374/500] Train Loss: 0.0308, Val Loss: 0.0240\n",
      "Epoch [375/500] Train Loss: 0.0295, Val Loss: 0.0222\n",
      "Epoch [376/500] Train Loss: 0.0299, Val Loss: 0.0214\n",
      "Epoch [377/500] Train Loss: 0.0289, Val Loss: 0.0229\n",
      "Epoch [378/500] Train Loss: 0.0290, Val Loss: 0.0208\n",
      "Epoch [379/500] Train Loss: 0.0280, Val Loss: 0.0215\n",
      "Epoch [380/500] Train Loss: 0.0283, Val Loss: 0.0274\n",
      "Epoch [381/500] Train Loss: 0.0309, Val Loss: 0.0335\n",
      "Epoch [382/500] Train Loss: 0.0314, Val Loss: 0.0368\n",
      "Epoch [383/500] Train Loss: 0.0305, Val Loss: 0.0204\n",
      "Epoch [384/500] Train Loss: 0.0283, Val Loss: 0.0201\n",
      "Epoch [385/500] Train Loss: 0.0261, Val Loss: 0.0202\n",
      "Epoch [386/500] Train Loss: 0.0252, Val Loss: 0.0209\n",
      "Epoch [387/500] Train Loss: 0.0251, Val Loss: 0.0198\n",
      "Epoch [388/500] Train Loss: 0.0252, Val Loss: 0.0202\n",
      "Epoch [389/500] Train Loss: 0.0256, Val Loss: 0.0202\n",
      "Epoch [390/500] Train Loss: 0.0246, Val Loss: 0.0199\n",
      "Epoch [391/500] Train Loss: 0.0261, Val Loss: 0.0194\n",
      "Epoch [392/500] Train Loss: 0.0241, Val Loss: 0.0183\n",
      "Epoch [393/500] Train Loss: 0.0235, Val Loss: 0.0179\n",
      "Epoch [394/500] Train Loss: 0.0237, Val Loss: 0.0189\n",
      "Epoch [395/500] Train Loss: 0.0245, Val Loss: 0.0171\n",
      "Epoch [396/500] Train Loss: 0.0242, Val Loss: 0.0180\n",
      "Epoch [397/500] Train Loss: 0.0228, Val Loss: 0.0175\n",
      "Epoch [398/500] Train Loss: 0.0223, Val Loss: 0.0165\n",
      "Epoch [399/500] Train Loss: 0.0238, Val Loss: 0.0167\n",
      "Epoch [400/500] Train Loss: 0.0220, Val Loss: 0.0165\n",
      "Epoch [401/500] Train Loss: 0.0218, Val Loss: 0.0246\n",
      "Epoch [402/500] Train Loss: 0.0224, Val Loss: 0.0163\n",
      "Epoch [403/500] Train Loss: 0.0212, Val Loss: 0.0158\n",
      "Epoch [404/500] Train Loss: 0.0211, Val Loss: 0.0159\n",
      "Epoch [405/500] Train Loss: 0.0216, Val Loss: 0.0163\n",
      "Epoch [406/500] Train Loss: 0.0210, Val Loss: 0.0218\n",
      "Epoch [407/500] Train Loss: 0.0224, Val Loss: 0.0166\n",
      "Epoch [408/500] Train Loss: 0.0215, Val Loss: 0.0189\n",
      "Epoch [409/500] Train Loss: 0.0203, Val Loss: 0.0180\n",
      "Epoch [410/500] Train Loss: 0.0210, Val Loss: 0.0167\n",
      "Epoch [411/500] Train Loss: 0.0206, Val Loss: 0.0156\n",
      "Epoch [412/500] Train Loss: 0.0200, Val Loss: 0.0147\n",
      "Epoch [413/500] Train Loss: 0.0187, Val Loss: 0.0149\n",
      "Epoch [414/500] Train Loss: 0.0184, Val Loss: 0.0166\n",
      "Epoch [415/500] Train Loss: 0.0218, Val Loss: 0.0217\n",
      "Epoch [416/500] Train Loss: 0.0204, Val Loss: 0.0164\n",
      "Epoch [417/500] Train Loss: 0.0214, Val Loss: 0.0145\n",
      "Epoch [418/500] Train Loss: 0.0193, Val Loss: 0.0175\n",
      "Epoch [419/500] Train Loss: 0.0224, Val Loss: 0.0232\n",
      "Epoch [420/500] Train Loss: 0.0199, Val Loss: 0.0135\n",
      "Epoch [421/500] Train Loss: 0.0175, Val Loss: 0.0157\n",
      "Epoch [422/500] Train Loss: 0.0189, Val Loss: 0.0165\n",
      "Epoch [423/500] Train Loss: 0.0179, Val Loss: 0.0129\n",
      "Epoch [424/500] Train Loss: 0.0165, Val Loss: 0.0140\n",
      "Epoch [425/500] Train Loss: 0.0171, Val Loss: 0.0144\n",
      "Epoch [426/500] Train Loss: 0.0167, Val Loss: 0.0130\n",
      "Epoch [427/500] Train Loss: 0.0174, Val Loss: 0.0151\n",
      "Epoch [428/500] Train Loss: 0.0172, Val Loss: 0.0123\n",
      "Epoch [429/500] Train Loss: 0.0164, Val Loss: 0.0137\n",
      "Epoch [430/500] Train Loss: 0.0167, Val Loss: 0.0122\n",
      "Epoch [431/500] Train Loss: 0.0170, Val Loss: 0.0130\n",
      "Epoch [432/500] Train Loss: 0.0176, Val Loss: 0.0117\n",
      "Epoch [433/500] Train Loss: 0.0154, Val Loss: 0.0125\n",
      "Epoch [434/500] Train Loss: 0.0156, Val Loss: 0.0125\n",
      "Epoch [435/500] Train Loss: 0.0165, Val Loss: 0.0156\n",
      "Epoch [436/500] Train Loss: 0.0179, Val Loss: 0.0156\n",
      "Epoch [437/500] Train Loss: 0.0184, Val Loss: 0.0220\n",
      "Epoch [438/500] Train Loss: 0.0192, Val Loss: 0.0120\n",
      "Epoch [439/500] Train Loss: 0.0158, Val Loss: 0.0132\n",
      "Epoch [440/500] Train Loss: 0.0145, Val Loss: 0.0110\n",
      "Epoch [441/500] Train Loss: 0.0144, Val Loss: 0.0114\n",
      "Epoch [442/500] Train Loss: 0.0143, Val Loss: 0.0117\n",
      "Epoch [443/500] Train Loss: 0.0146, Val Loss: 0.0113\n",
      "Epoch [444/500] Train Loss: 0.0147, Val Loss: 0.0138\n",
      "Epoch [445/500] Train Loss: 0.0160, Val Loss: 0.0117\n",
      "Epoch [446/500] Train Loss: 0.0154, Val Loss: 0.0108\n",
      "Epoch [447/500] Train Loss: 0.0143, Val Loss: 0.0105\n",
      "Epoch [448/500] Train Loss: 0.0135, Val Loss: 0.0104\n",
      "Epoch [449/500] Train Loss: 0.0134, Val Loss: 0.0111\n",
      "Epoch [450/500] Train Loss: 0.0132, Val Loss: 0.0105\n",
      "Epoch [451/500] Train Loss: 0.0127, Val Loss: 0.0103\n",
      "Epoch [452/500] Train Loss: 0.0126, Val Loss: 0.0105\n",
      "Epoch [453/500] Train Loss: 0.0129, Val Loss: 0.0109\n",
      "Epoch [454/500] Train Loss: 0.0131, Val Loss: 0.0149\n",
      "Epoch [455/500] Train Loss: 0.0162, Val Loss: 0.0105\n",
      "Epoch [456/500] Train Loss: 0.0315, Val Loss: 0.0150\n",
      "Epoch [457/500] Train Loss: 0.0252, Val Loss: 0.0180\n",
      "Epoch [458/500] Train Loss: 0.0145, Val Loss: 0.0115\n",
      "Epoch [459/500] Train Loss: 0.0134, Val Loss: 0.0098\n",
      "Epoch [460/500] Train Loss: 0.0127, Val Loss: 0.0102\n",
      "Epoch [461/500] Train Loss: 0.0123, Val Loss: 0.0094\n",
      "Epoch [462/500] Train Loss: 0.0121, Val Loss: 0.0093\n",
      "Epoch [463/500] Train Loss: 0.0117, Val Loss: 0.0098\n",
      "Epoch [464/500] Train Loss: 0.0124, Val Loss: 0.0109\n",
      "Epoch [465/500] Train Loss: 0.0118, Val Loss: 0.0091\n",
      "Epoch [466/500] Train Loss: 0.0117, Val Loss: 0.0092\n",
      "Epoch [467/500] Train Loss: 0.0112, Val Loss: 0.0093\n",
      "Epoch [468/500] Train Loss: 0.0116, Val Loss: 0.0101\n",
      "Epoch [469/500] Train Loss: 0.0114, Val Loss: 0.0093\n",
      "Epoch [470/500] Train Loss: 0.0112, Val Loss: 0.0091\n",
      "Epoch [471/500] Train Loss: 0.0111, Val Loss: 0.0095\n",
      "Epoch [472/500] Train Loss: 0.0130, Val Loss: 0.0101\n",
      "Epoch [473/500] Train Loss: 0.0138, Val Loss: 0.0092\n",
      "Epoch [474/500] Train Loss: 0.0143, Val Loss: 0.0156\n",
      "Epoch [475/500] Train Loss: 0.0132, Val Loss: 0.0128\n",
      "Epoch [476/500] Train Loss: 0.0128, Val Loss: 0.0089\n",
      "Epoch [477/500] Train Loss: 0.0108, Val Loss: 0.0085\n",
      "Epoch [478/500] Train Loss: 0.0101, Val Loss: 0.0082\n",
      "Epoch [479/500] Train Loss: 0.0101, Val Loss: 0.0104\n",
      "Epoch [480/500] Train Loss: 0.0114, Val Loss: 0.0126\n",
      "Epoch [481/500] Train Loss: 0.0120, Val Loss: 0.0082\n",
      "Epoch [482/500] Train Loss: 0.0102, Val Loss: 0.0085\n",
      "Epoch [483/500] Train Loss: 0.0097, Val Loss: 0.0093\n",
      "Epoch [484/500] Train Loss: 0.0117, Val Loss: 0.0080\n",
      "Epoch [485/500] Train Loss: 0.0107, Val Loss: 0.0088\n",
      "Epoch [486/500] Train Loss: 0.0098, Val Loss: 0.0100\n",
      "Epoch [487/500] Train Loss: 0.0103, Val Loss: 0.0092\n",
      "Epoch [488/500] Train Loss: 0.0102, Val Loss: 0.0080\n",
      "Epoch [489/500] Train Loss: 0.0097, Val Loss: 0.0089\n",
      "Epoch [490/500] Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [491/500] Train Loss: 0.0104, Val Loss: 0.0108\n",
      "Epoch [492/500] Train Loss: 0.0116, Val Loss: 0.0109\n",
      "Epoch [493/500] Train Loss: 0.0100, Val Loss: 0.0076\n",
      "Epoch [494/500] Train Loss: 0.0102, Val Loss: 0.0088\n",
      "Epoch [495/500] Train Loss: 0.0097, Val Loss: 0.0079\n",
      "Epoch [496/500] Train Loss: 0.0105, Val Loss: 0.0268\n",
      "Epoch [497/500] Train Loss: 0.0184, Val Loss: 0.0196\n",
      "Epoch [498/500] Train Loss: 0.0181, Val Loss: 0.0078\n",
      "Epoch [499/500] Train Loss: 0.0089, Val Loss: 0.0070\n",
      "Epoch [500/500] Train Loss: 0.0087, Val Loss: 0.0071\n",
      "\n",
      "=== Fold 9 ===\n",
      "Epoch [1/500] Train Loss: 40.9752, Val Loss: 42.1604\n",
      "Epoch [2/500] Train Loss: 40.9540, Val Loss: 42.1389\n",
      "Epoch [3/500] Train Loss: 40.9326, Val Loss: 42.1170\n",
      "Epoch [4/500] Train Loss: 40.9103, Val Loss: 42.0929\n",
      "Epoch [5/500] Train Loss: 40.8838, Val Loss: 42.0613\n",
      "Epoch [6/500] Train Loss: 40.8470, Val Loss: 42.0150\n",
      "Epoch [7/500] Train Loss: 40.7866, Val Loss: 41.9220\n",
      "Epoch [8/500] Train Loss: 40.6372, Val Loss: 41.6696\n",
      "Epoch [9/500] Train Loss: 40.2566, Val Loss: 41.0678\n",
      "Epoch [10/500] Train Loss: 39.4022, Val Loss: 39.7835\n",
      "Epoch [11/500] Train Loss: 37.6590, Val Loss: 37.3802\n",
      "Epoch [12/500] Train Loss: 34.8700, Val Loss: 34.6357\n",
      "Epoch [13/500] Train Loss: 32.9638, Val Loss: 34.4888\n",
      "Epoch [14/500] Train Loss: 32.7932, Val Loss: 34.0969\n",
      "Epoch [15/500] Train Loss: 32.5861, Val Loss: 33.9786\n",
      "Epoch [16/500] Train Loss: 32.4899, Val Loss: 33.8913\n",
      "Epoch [17/500] Train Loss: 32.3722, Val Loss: 33.7678\n",
      "Epoch [18/500] Train Loss: 32.2586, Val Loss: 33.6562\n",
      "Epoch [19/500] Train Loss: 32.1462, Val Loss: 33.5357\n",
      "Epoch [20/500] Train Loss: 32.0315, Val Loss: 33.4176\n",
      "Epoch [21/500] Train Loss: 31.9142, Val Loss: 33.2884\n",
      "Epoch [22/500] Train Loss: 31.7908, Val Loss: 33.1573\n",
      "Epoch [23/500] Train Loss: 31.6609, Val Loss: 33.0159\n",
      "Epoch [24/500] Train Loss: 31.5289, Val Loss: 32.8698\n",
      "Epoch [25/500] Train Loss: 31.3838, Val Loss: 32.7079\n",
      "Epoch [26/500] Train Loss: 31.2248, Val Loss: 32.5419\n",
      "Epoch [27/500] Train Loss: 31.0538, Val Loss: 32.3468\n",
      "Epoch [28/500] Train Loss: 30.8625, Val Loss: 32.1318\n",
      "Epoch [29/500] Train Loss: 30.6469, Val Loss: 31.8813\n",
      "Epoch [30/500] Train Loss: 30.4125, Val Loss: 31.6169\n",
      "Epoch [31/500] Train Loss: 30.1513, Val Loss: 31.3054\n",
      "Epoch [32/500] Train Loss: 29.8384, Val Loss: 30.9522\n",
      "Epoch [33/500] Train Loss: 29.5044, Val Loss: 30.5745\n",
      "Epoch [34/500] Train Loss: 29.1595, Val Loss: 30.2298\n",
      "Epoch [35/500] Train Loss: 28.8687, Val Loss: 29.9477\n",
      "Epoch [36/500] Train Loss: 28.6321, Val Loss: 29.7598\n",
      "Epoch [37/500] Train Loss: 28.4593, Val Loss: 29.6125\n",
      "Epoch [38/500] Train Loss: 28.3294, Val Loss: 29.5016\n",
      "Epoch [39/500] Train Loss: 28.2281, Val Loss: 29.4184\n",
      "Epoch [40/500] Train Loss: 28.1574, Val Loss: 29.3640\n",
      "Epoch [41/500] Train Loss: 28.1140, Val Loss: 29.3357\n",
      "Epoch [42/500] Train Loss: 28.0885, Val Loss: 29.3120\n",
      "Epoch [43/500] Train Loss: 28.0729, Val Loss: 29.3010\n",
      "Epoch [44/500] Train Loss: 28.0651, Val Loss: 29.2951\n",
      "Epoch [45/500] Train Loss: 28.0614, Val Loss: 29.2915\n",
      "Epoch [46/500] Train Loss: 28.0582, Val Loss: 29.2888\n",
      "Epoch [47/500] Train Loss: 28.0555, Val Loss: 29.2863\n",
      "Epoch [48/500] Train Loss: 28.0534, Val Loss: 29.2835\n",
      "Epoch [49/500] Train Loss: 28.0517, Val Loss: 29.2814\n",
      "Epoch [50/500] Train Loss: 28.0498, Val Loss: 29.2797\n",
      "Epoch [51/500] Train Loss: 28.0482, Val Loss: 29.2782\n",
      "Epoch [52/500] Train Loss: 28.0471, Val Loss: 29.2762\n",
      "Epoch [53/500] Train Loss: 28.0458, Val Loss: 29.2756\n",
      "Epoch [54/500] Train Loss: 28.0447, Val Loss: 29.2741\n",
      "Epoch [55/500] Train Loss: 28.0436, Val Loss: 29.2733\n",
      "Epoch [56/500] Train Loss: 28.0427, Val Loss: 29.2715\n",
      "Epoch [57/500] Train Loss: 28.0419, Val Loss: 29.2704\n",
      "Epoch [58/500] Train Loss: 28.0413, Val Loss: 29.2690\n",
      "Epoch [59/500] Train Loss: 28.0401, Val Loss: 29.2681\n",
      "Epoch [60/500] Train Loss: 28.0395, Val Loss: 29.2673\n",
      "Epoch [61/500] Train Loss: 28.0388, Val Loss: 29.2674\n",
      "Epoch [62/500] Train Loss: 28.0381, Val Loss: 29.2655\n",
      "Epoch [63/500] Train Loss: 28.0372, Val Loss: 29.2650\n",
      "Epoch [64/500] Train Loss: 28.0364, Val Loss: 29.2638\n",
      "Epoch [65/500] Train Loss: 28.0362, Val Loss: 29.2631\n",
      "Epoch [66/500] Train Loss: 28.0351, Val Loss: 29.2623\n",
      "Epoch [67/500] Train Loss: 28.0345, Val Loss: 29.2616\n",
      "Epoch [68/500] Train Loss: 28.0337, Val Loss: 29.2607\n",
      "Epoch [69/500] Train Loss: 28.0335, Val Loss: 29.2602\n",
      "Epoch [70/500] Train Loss: 28.0327, Val Loss: 29.2594\n",
      "Epoch [71/500] Train Loss: 28.0318, Val Loss: 29.2588\n",
      "Epoch [72/500] Train Loss: 28.0313, Val Loss: 29.2581\n",
      "Epoch [73/500] Train Loss: 28.0306, Val Loss: 29.2575\n",
      "Epoch [74/500] Train Loss: 28.0301, Val Loss: 29.2568\n",
      "Epoch [75/500] Train Loss: 28.0294, Val Loss: 29.2561\n",
      "Epoch [76/500] Train Loss: 28.0290, Val Loss: 29.2555\n",
      "Epoch [77/500] Train Loss: 28.0286, Val Loss: 29.2549\n",
      "Epoch [78/500] Train Loss: 28.0278, Val Loss: 29.2545\n",
      "Epoch [79/500] Train Loss: 28.0273, Val Loss: 29.2542\n",
      "Epoch [80/500] Train Loss: 28.0270, Val Loss: 29.2535\n",
      "Epoch [81/500] Train Loss: 28.0264, Val Loss: 29.2529\n",
      "Epoch [82/500] Train Loss: 28.0260, Val Loss: 29.2533\n",
      "Epoch [83/500] Train Loss: 28.0255, Val Loss: 29.2519\n",
      "Epoch [84/500] Train Loss: 28.0252, Val Loss: 29.2514\n",
      "Epoch [85/500] Train Loss: 28.0248, Val Loss: 29.2509\n",
      "Epoch [86/500] Train Loss: 28.0243, Val Loss: 29.2505\n",
      "Epoch [87/500] Train Loss: 28.0238, Val Loss: 29.2503\n",
      "Epoch [88/500] Train Loss: 28.0238, Val Loss: 29.2507\n",
      "Epoch [89/500] Train Loss: 28.0235, Val Loss: 29.2493\n",
      "Epoch [90/500] Train Loss: 28.0228, Val Loss: 29.2489\n",
      "Epoch [91/500] Train Loss: 28.0226, Val Loss: 29.2485\n",
      "Epoch [92/500] Train Loss: 28.0222, Val Loss: 29.2481\n",
      "Epoch [93/500] Train Loss: 28.0219, Val Loss: 29.2481\n",
      "Epoch [94/500] Train Loss: 28.0218, Val Loss: 29.2474\n",
      "Epoch [95/500] Train Loss: 28.0214, Val Loss: 29.2471\n",
      "Epoch [96/500] Train Loss: 28.0212, Val Loss: 29.2468\n",
      "Epoch [97/500] Train Loss: 28.0209, Val Loss: 29.2464\n",
      "Epoch [98/500] Train Loss: 28.0206, Val Loss: 29.2463\n",
      "Epoch [99/500] Train Loss: 28.0205, Val Loss: 29.2459\n",
      "Epoch [100/500] Train Loss: 28.0203, Val Loss: 29.2455\n",
      "Epoch [101/500] Train Loss: 28.0200, Val Loss: 29.2452\n",
      "Epoch [102/500] Train Loss: 28.0200, Val Loss: 29.2452\n",
      "Epoch [103/500] Train Loss: 28.0196, Val Loss: 29.2446\n",
      "Epoch [104/500] Train Loss: 28.0193, Val Loss: 29.2446\n",
      "Epoch [105/500] Train Loss: 28.0191, Val Loss: 29.2442\n",
      "Epoch [106/500] Train Loss: 28.0189, Val Loss: 29.2440\n",
      "Epoch [107/500] Train Loss: 28.0187, Val Loss: 29.2436\n",
      "Epoch [108/500] Train Loss: 28.0186, Val Loss: 29.2437\n",
      "Epoch [109/500] Train Loss: 28.0185, Val Loss: 29.2432\n",
      "Epoch [110/500] Train Loss: 28.0184, Val Loss: 29.2432\n",
      "Epoch [111/500] Train Loss: 28.0182, Val Loss: 29.2428\n",
      "Epoch [112/500] Train Loss: 28.0179, Val Loss: 29.2426\n",
      "Epoch [113/500] Train Loss: 28.0177, Val Loss: 29.2424\n",
      "Epoch [114/500] Train Loss: 28.0176, Val Loss: 29.2422\n",
      "Epoch [115/500] Train Loss: 28.0174, Val Loss: 29.2420\n",
      "Epoch [116/500] Train Loss: 28.0173, Val Loss: 29.2420\n",
      "Epoch [117/500] Train Loss: 28.0171, Val Loss: 29.2417\n",
      "Epoch [118/500] Train Loss: 28.0171, Val Loss: 29.2415\n",
      "Epoch [119/500] Train Loss: 28.0169, Val Loss: 29.2414\n",
      "Epoch [120/500] Train Loss: 28.0167, Val Loss: 29.2414\n",
      "Epoch [121/500] Train Loss: 28.0167, Val Loss: 29.2412\n",
      "Epoch [122/500] Train Loss: 28.0166, Val Loss: 29.2411\n",
      "Epoch [123/500] Train Loss: 28.0164, Val Loss: 29.2409\n",
      "Epoch [124/500] Train Loss: 28.0164, Val Loss: 29.2412\n",
      "Epoch [125/500] Train Loss: 28.0164, Val Loss: 29.2406\n",
      "Epoch [126/500] Train Loss: 28.0161, Val Loss: 29.2405\n",
      "Epoch [127/500] Train Loss: 28.0161, Val Loss: 29.2405\n",
      "Epoch [128/500] Train Loss: 28.0159, Val Loss: 29.2404\n",
      "Epoch [129/500] Train Loss: 28.0158, Val Loss: 29.2407\n",
      "Epoch [130/500] Train Loss: 28.0157, Val Loss: 29.2402\n",
      "Epoch [131/500] Train Loss: 28.0156, Val Loss: 29.2401\n",
      "Epoch [132/500] Train Loss: 28.0154, Val Loss: 29.2401\n",
      "Epoch [133/500] Train Loss: 28.0154, Val Loss: 29.2400\n",
      "Epoch [134/500] Train Loss: 28.0154, Val Loss: 29.2398\n",
      "Epoch [135/500] Train Loss: 28.0152, Val Loss: 29.2397\n",
      "Epoch [136/500] Train Loss: 28.0151, Val Loss: 29.2398\n",
      "Epoch [137/500] Train Loss: 28.0151, Val Loss: 29.2396\n",
      "Epoch [138/500] Train Loss: 28.0150, Val Loss: 29.2398\n",
      "Epoch [139/500] Train Loss: 28.0150, Val Loss: 29.2394\n",
      "Epoch [140/500] Train Loss: 28.0148, Val Loss: 29.2393\n",
      "Epoch [141/500] Train Loss: 28.0148, Val Loss: 29.2393\n",
      "Epoch [142/500] Train Loss: 28.0147, Val Loss: 29.2392\n",
      "Epoch [143/500] Train Loss: 28.0147, Val Loss: 29.2391\n",
      "Epoch [144/500] Train Loss: 28.0146, Val Loss: 29.2394\n",
      "Epoch [145/500] Train Loss: 28.0145, Val Loss: 29.2392\n",
      "Epoch [146/500] Train Loss: 28.0145, Val Loss: 29.2390\n",
      "Epoch [147/500] Train Loss: 28.0144, Val Loss: 29.2390\n",
      "Epoch [148/500] Train Loss: 28.0145, Val Loss: 29.2391\n",
      "Epoch [149/500] Train Loss: 28.0144, Val Loss: 29.2388\n",
      "Epoch [150/500] Train Loss: 28.0144, Val Loss: 29.2388\n",
      "Epoch [151/500] Train Loss: 28.0142, Val Loss: 29.2387\n",
      "Epoch [152/500] Train Loss: 28.0142, Val Loss: 29.2387\n",
      "Epoch [153/500] Train Loss: 28.0141, Val Loss: 29.2385\n",
      "Epoch [154/500] Train Loss: 28.0140, Val Loss: 29.2386\n",
      "Epoch [155/500] Train Loss: 28.0140, Val Loss: 29.2385\n",
      "Epoch [156/500] Train Loss: 28.0140, Val Loss: 29.2386\n",
      "Epoch [157/500] Train Loss: 28.0139, Val Loss: 29.2384\n",
      "Epoch [158/500] Train Loss: 28.0139, Val Loss: 29.2383\n",
      "Epoch [159/500] Train Loss: 28.0139, Val Loss: 29.2385\n",
      "Epoch [160/500] Train Loss: 28.0138, Val Loss: 29.2383\n",
      "Epoch [161/500] Train Loss: 28.0138, Val Loss: 29.2382\n",
      "Epoch [162/500] Train Loss: 28.0137, Val Loss: 29.2382\n",
      "Epoch [163/500] Train Loss: 28.0137, Val Loss: 29.2381\n",
      "Epoch [164/500] Train Loss: 28.0137, Val Loss: 29.2383\n",
      "Epoch [165/500] Train Loss: 28.0137, Val Loss: 29.2384\n",
      "Epoch [166/500] Train Loss: 28.0137, Val Loss: 29.2381\n",
      "Epoch [167/500] Train Loss: 28.0137, Val Loss: 29.2384\n",
      "Epoch [168/500] Train Loss: 28.0137, Val Loss: 29.2381\n",
      "Epoch [169/500] Train Loss: 28.0135, Val Loss: 29.2380\n",
      "Epoch [170/500] Train Loss: 28.0134, Val Loss: 29.2379\n",
      "Epoch [171/500] Train Loss: 28.0134, Val Loss: 29.2381\n",
      "Epoch [172/500] Train Loss: 28.0134, Val Loss: 29.2378\n",
      "Epoch [173/500] Train Loss: 28.0134, Val Loss: 29.2379\n",
      "Epoch [174/500] Train Loss: 28.0133, Val Loss: 29.2378\n",
      "Epoch [175/500] Train Loss: 28.0133, Val Loss: 29.2378\n",
      "Epoch [176/500] Train Loss: 28.0132, Val Loss: 29.2377\n",
      "Epoch [177/500] Train Loss: 28.0132, Val Loss: 29.2377\n",
      "Epoch [178/500] Train Loss: 28.0132, Val Loss: 29.2377\n",
      "Epoch [179/500] Train Loss: 28.0132, Val Loss: 29.2377\n",
      "Epoch [180/500] Train Loss: 28.0132, Val Loss: 29.2378\n",
      "Epoch [181/500] Train Loss: 28.0131, Val Loss: 29.2376\n",
      "Epoch [182/500] Train Loss: 28.0131, Val Loss: 29.2376\n",
      "Epoch [183/500] Train Loss: 28.0130, Val Loss: 29.2377\n",
      "Epoch [184/500] Train Loss: 28.0130, Val Loss: 29.2376\n",
      "Epoch [185/500] Train Loss: 28.0130, Val Loss: 29.2375\n",
      "Epoch [186/500] Train Loss: 28.0130, Val Loss: 29.2375\n",
      "Epoch [187/500] Train Loss: 28.0130, Val Loss: 29.2374\n",
      "Epoch [188/500] Train Loss: 28.0129, Val Loss: 29.2374\n",
      "Epoch [189/500] Train Loss: 28.0129, Val Loss: 29.2374\n",
      "Epoch [190/500] Train Loss: 28.0129, Val Loss: 29.2375\n",
      "Epoch [191/500] Train Loss: 28.0129, Val Loss: 29.2373\n",
      "Epoch [192/500] Train Loss: 28.0129, Val Loss: 29.2373\n",
      "Epoch [193/500] Train Loss: 28.0128, Val Loss: 29.2375\n",
      "Epoch [194/500] Train Loss: 28.0128, Val Loss: 29.2374\n",
      "Epoch [195/500] Train Loss: 28.0128, Val Loss: 29.2373\n",
      "Epoch [196/500] Train Loss: 28.0127, Val Loss: 29.2373\n",
      "Epoch [197/500] Train Loss: 28.0127, Val Loss: 29.2373\n",
      "Epoch [198/500] Train Loss: 28.0127, Val Loss: 29.2373\n",
      "Epoch [199/500] Train Loss: 28.0128, Val Loss: 29.2375\n",
      "Epoch [200/500] Train Loss: 28.0127, Val Loss: 29.2372\n",
      "Epoch [201/500] Train Loss: 28.0127, Val Loss: 29.2373\n",
      "Epoch [202/500] Train Loss: 28.0126, Val Loss: 29.2372\n",
      "Epoch [203/500] Train Loss: 28.0127, Val Loss: 29.2372\n",
      "Epoch [204/500] Train Loss: 28.0126, Val Loss: 29.2372\n",
      "Epoch [205/500] Train Loss: 28.0125, Val Loss: 29.2372\n",
      "Epoch [206/500] Train Loss: 28.0126, Val Loss: 29.2372\n",
      "Epoch [207/500] Train Loss: 28.0126, Val Loss: 29.2371\n",
      "Epoch [208/500] Train Loss: 28.0125, Val Loss: 29.2372\n",
      "Epoch [209/500] Train Loss: 28.0125, Val Loss: 29.2372\n",
      "Epoch [210/500] Train Loss: 28.0125, Val Loss: 29.2371\n",
      "Epoch [211/500] Train Loss: 28.0124, Val Loss: 29.2371\n",
      "Epoch [212/500] Train Loss: 28.0124, Val Loss: 29.2371\n",
      "Epoch [213/500] Train Loss: 28.0124, Val Loss: 29.2370\n",
      "Epoch [214/500] Train Loss: 28.0124, Val Loss: 29.2370\n",
      "Epoch [215/500] Train Loss: 28.0124, Val Loss: 29.2370\n",
      "Epoch [216/500] Train Loss: 28.0124, Val Loss: 29.2371\n",
      "Epoch [217/500] Train Loss: 28.0123, Val Loss: 29.2370\n",
      "Epoch [218/500] Train Loss: 28.0123, Val Loss: 29.2370\n",
      "Epoch [219/500] Train Loss: 28.0123, Val Loss: 29.2369\n",
      "Epoch [220/500] Train Loss: 28.0123, Val Loss: 29.2370\n",
      "Epoch [221/500] Train Loss: 28.0123, Val Loss: 29.2371\n",
      "Epoch [222/500] Train Loss: 28.0122, Val Loss: 29.2370\n",
      "Epoch [223/500] Train Loss: 28.0123, Val Loss: 29.2372\n",
      "Epoch [224/500] Train Loss: 28.0124, Val Loss: 29.2375\n",
      "Epoch [225/500] Train Loss: 28.0124, Val Loss: 29.2370\n",
      "Epoch [226/500] Train Loss: 28.0124, Val Loss: 29.2369\n",
      "Epoch [227/500] Train Loss: 28.0122, Val Loss: 29.2369\n",
      "Epoch [228/500] Train Loss: 28.0122, Val Loss: 29.2370\n",
      "Epoch [229/500] Train Loss: 28.0121, Val Loss: 29.2368\n",
      "Epoch [230/500] Train Loss: 28.0120, Val Loss: 29.2368\n",
      "Epoch [231/500] Train Loss: 28.0119, Val Loss: 29.2369\n",
      "Epoch [232/500] Train Loss: 28.0117, Val Loss: 29.2367\n",
      "Epoch [233/500] Train Loss: 28.0116, Val Loss: 29.2369\n",
      "Epoch [234/500] Train Loss: 28.0116, Val Loss: 29.2366\n",
      "Epoch [235/500] Train Loss: 28.0114, Val Loss: 29.2366\n",
      "Epoch [236/500] Train Loss: 28.0112, Val Loss: 29.2362\n",
      "Epoch [237/500] Train Loss: 28.0111, Val Loss: 29.2362\n",
      "Epoch [238/500] Train Loss: 28.0109, Val Loss: 29.2360\n",
      "Epoch [239/500] Train Loss: 28.0108, Val Loss: 29.2358\n",
      "Epoch [240/500] Train Loss: 28.0108, Val Loss: 29.2358\n",
      "Epoch [241/500] Train Loss: 28.0107, Val Loss: 29.2355\n",
      "Epoch [242/500] Train Loss: 28.0107, Val Loss: 29.2354\n",
      "Epoch [243/500] Train Loss: 28.0106, Val Loss: 29.2356\n",
      "Epoch [244/500] Train Loss: 28.0106, Val Loss: 29.2357\n",
      "Epoch [245/500] Train Loss: 28.0106, Val Loss: 29.2356\n",
      "Epoch [246/500] Train Loss: 28.0105, Val Loss: 29.2354\n",
      "Epoch [247/500] Train Loss: 28.0106, Val Loss: 29.2356\n",
      "Epoch [248/500] Train Loss: 28.0105, Val Loss: 29.2354\n",
      "Epoch [249/500] Train Loss: 28.0104, Val Loss: 29.2356\n",
      "Epoch [250/500] Train Loss: 28.0105, Val Loss: 29.2359\n",
      "Epoch [251/500] Train Loss: 28.0105, Val Loss: 29.2356\n",
      "Epoch [252/500] Train Loss: 28.0105, Val Loss: 29.2354\n",
      "Epoch [253/500] Train Loss: 28.0104, Val Loss: 29.2359\n",
      "Epoch [254/500] Train Loss: 28.0104, Val Loss: 29.2355\n",
      "Epoch [255/500] Train Loss: 28.0103, Val Loss: 29.2353\n",
      "Epoch [256/500] Train Loss: 28.0103, Val Loss: 29.2354\n",
      "Epoch [257/500] Train Loss: 28.0103, Val Loss: 29.2353\n",
      "Epoch [258/500] Train Loss: 28.0103, Val Loss: 29.2352\n",
      "Epoch [259/500] Train Loss: 28.0103, Val Loss: 29.2352\n",
      "Epoch [260/500] Train Loss: 28.0103, Val Loss: 29.2352\n",
      "Epoch [261/500] Train Loss: 28.0103, Val Loss: 29.2352\n",
      "Epoch [262/500] Train Loss: 28.0102, Val Loss: 29.2351\n",
      "Epoch [263/500] Train Loss: 28.0103, Val Loss: 29.2350\n",
      "Epoch [264/500] Train Loss: 28.0102, Val Loss: 29.2351\n",
      "Epoch [265/500] Train Loss: 28.0102, Val Loss: 29.2350\n",
      "Epoch [266/500] Train Loss: 28.0102, Val Loss: 29.2352\n",
      "Epoch [267/500] Train Loss: 28.0102, Val Loss: 29.2353\n",
      "Epoch [268/500] Train Loss: 28.0103, Val Loss: 29.2352\n",
      "Epoch [269/500] Train Loss: 28.0103, Val Loss: 29.2350\n",
      "Epoch [270/500] Train Loss: 28.0102, Val Loss: 29.2350\n",
      "Epoch [271/500] Train Loss: 28.0102, Val Loss: 29.2350\n",
      "Epoch [272/500] Train Loss: 28.0101, Val Loss: 29.2349\n",
      "Epoch [273/500] Train Loss: 28.0101, Val Loss: 29.2350\n",
      "Epoch [274/500] Train Loss: 28.0101, Val Loss: 29.2349\n",
      "Epoch [275/500] Train Loss: 28.0101, Val Loss: 29.2349\n",
      "Epoch [276/500] Train Loss: 28.0101, Val Loss: 29.2350\n",
      "Epoch [277/500] Train Loss: 28.0101, Val Loss: 29.2349\n",
      "Epoch [278/500] Train Loss: 28.0101, Val Loss: 29.2351\n",
      "Epoch [279/500] Train Loss: 28.0101, Val Loss: 29.2349\n",
      "Epoch [280/500] Train Loss: 28.0101, Val Loss: 29.2350\n",
      "Epoch [281/500] Train Loss: 28.0101, Val Loss: 29.2349\n",
      "Epoch [282/500] Train Loss: 28.0100, Val Loss: 29.2350\n",
      "Epoch [283/500] Train Loss: 28.0101, Val Loss: 29.2349\n",
      "Epoch [284/500] Train Loss: 28.0100, Val Loss: 29.2349\n",
      "Epoch [285/500] Train Loss: 28.0100, Val Loss: 29.2349\n",
      "Epoch [286/500] Train Loss: 28.0100, Val Loss: 29.2348\n",
      "Epoch [287/500] Train Loss: 28.0100, Val Loss: 29.2350\n",
      "Epoch [288/500] Train Loss: 28.0101, Val Loss: 29.2348\n",
      "Epoch [289/500] Train Loss: 28.0100, Val Loss: 29.2349\n",
      "Epoch [290/500] Train Loss: 28.0100, Val Loss: 29.2349\n",
      "Epoch [291/500] Train Loss: 28.0100, Val Loss: 29.2348\n",
      "Epoch [292/500] Train Loss: 28.0100, Val Loss: 29.2351\n",
      "Epoch [293/500] Train Loss: 28.0101, Val Loss: 29.2347\n",
      "Epoch [294/500] Train Loss: 28.0099, Val Loss: 29.2348\n",
      "Epoch [295/500] Train Loss: 28.0099, Val Loss: 29.2348\n",
      "Epoch [296/500] Train Loss: 28.0099, Val Loss: 29.2350\n",
      "Epoch [297/500] Train Loss: 28.0100, Val Loss: 29.2348\n",
      "Epoch [298/500] Train Loss: 28.0100, Val Loss: 29.2351\n",
      "Epoch [299/500] Train Loss: 28.0100, Val Loss: 29.2347\n",
      "Epoch [300/500] Train Loss: 28.0099, Val Loss: 29.2348\n",
      "Epoch [301/500] Train Loss: 28.0098, Val Loss: 29.2347\n",
      "Epoch [302/500] Train Loss: 28.0098, Val Loss: 29.2349\n",
      "Epoch [303/500] Train Loss: 28.0098, Val Loss: 29.2347\n",
      "Epoch [304/500] Train Loss: 28.0098, Val Loss: 29.2347\n",
      "Epoch [305/500] Train Loss: 28.0098, Val Loss: 29.2347\n",
      "Epoch [306/500] Train Loss: 28.0097, Val Loss: 29.2347\n",
      "Epoch [307/500] Train Loss: 28.0097, Val Loss: 29.2346\n",
      "Epoch [308/500] Train Loss: 28.0097, Val Loss: 29.2347\n",
      "Epoch [309/500] Train Loss: 28.0097, Val Loss: 29.2346\n",
      "Epoch [310/500] Train Loss: 28.0097, Val Loss: 29.2346\n",
      "Epoch [311/500] Train Loss: 28.0098, Val Loss: 29.2345\n",
      "Epoch [312/500] Train Loss: 28.0098, Val Loss: 29.2348\n",
      "Epoch [313/500] Train Loss: 28.0098, Val Loss: 29.2346\n",
      "Epoch [314/500] Train Loss: 28.0097, Val Loss: 29.2346\n",
      "Epoch [315/500] Train Loss: 28.0097, Val Loss: 29.2346\n",
      "Epoch [316/500] Train Loss: 28.0097, Val Loss: 29.2345\n",
      "Epoch [317/500] Train Loss: 28.0098, Val Loss: 29.2347\n",
      "Epoch [318/500] Train Loss: 28.0097, Val Loss: 29.2345\n",
      "Epoch [319/500] Train Loss: 28.0096, Val Loss: 29.2345\n",
      "Epoch [320/500] Train Loss: 28.0098, Val Loss: 29.2348\n",
      "Epoch [321/500] Train Loss: 28.0097, Val Loss: 29.2345\n",
      "Epoch [322/500] Train Loss: 28.0096, Val Loss: 29.2344\n",
      "Epoch [323/500] Train Loss: 28.0096, Val Loss: 29.2348\n",
      "Epoch [324/500] Train Loss: 28.0098, Val Loss: 29.2347\n",
      "Epoch [325/500] Train Loss: 28.0096, Val Loss: 29.2345\n",
      "Epoch [326/500] Train Loss: 28.0096, Val Loss: 29.2344\n",
      "Epoch [327/500] Train Loss: 28.0096, Val Loss: 29.2344\n",
      "Epoch [328/500] Train Loss: 28.0096, Val Loss: 29.2346\n",
      "Epoch [329/500] Train Loss: 28.0096, Val Loss: 29.2345\n",
      "Epoch [330/500] Train Loss: 28.0097, Val Loss: 29.2347\n",
      "Epoch [331/500] Train Loss: 28.0096, Val Loss: 29.2344\n",
      "Epoch [332/500] Train Loss: 28.0096, Val Loss: 29.2347\n",
      "Epoch [333/500] Train Loss: 28.0096, Val Loss: 29.2345\n",
      "Epoch [334/500] Train Loss: 28.0096, Val Loss: 29.2344\n",
      "Epoch [335/500] Train Loss: 28.0095, Val Loss: 29.2344\n",
      "Epoch [336/500] Train Loss: 28.0095, Val Loss: 29.2344\n",
      "Epoch [337/500] Train Loss: 28.0095, Val Loss: 29.2344\n",
      "Epoch [338/500] Train Loss: 28.0095, Val Loss: 29.2344\n",
      "Epoch [339/500] Train Loss: 28.0096, Val Loss: 29.2344\n",
      "Epoch [340/500] Train Loss: 28.0096, Val Loss: 29.2344\n",
      "Epoch [341/500] Train Loss: 28.0095, Val Loss: 29.2344\n",
      "Epoch [342/500] Train Loss: 28.0096, Val Loss: 29.2344\n",
      "Epoch [343/500] Train Loss: 28.0095, Val Loss: 29.2344\n",
      "Epoch [344/500] Train Loss: 28.0095, Val Loss: 29.2343\n",
      "Epoch [345/500] Train Loss: 28.0095, Val Loss: 29.2344\n",
      "Epoch [346/500] Train Loss: 28.0095, Val Loss: 29.2344\n",
      "Epoch [347/500] Train Loss: 28.0095, Val Loss: 29.2344\n",
      "Epoch [348/500] Train Loss: 28.0095, Val Loss: 29.2343\n",
      "Epoch [349/500] Train Loss: 28.0095, Val Loss: 29.2343\n",
      "Epoch [350/500] Train Loss: 28.0095, Val Loss: 29.2344\n",
      "Epoch [351/500] Train Loss: 28.0095, Val Loss: 29.2343\n",
      "Epoch [352/500] Train Loss: 28.0095, Val Loss: 29.2343\n",
      "Epoch [353/500] Train Loss: 28.0095, Val Loss: 29.2343\n",
      "Epoch [354/500] Train Loss: 28.0095, Val Loss: 29.2343\n",
      "Epoch [355/500] Train Loss: 28.0095, Val Loss: 29.2344\n",
      "Epoch [356/500] Train Loss: 28.0095, Val Loss: 29.2343\n",
      "Epoch [357/500] Train Loss: 28.0095, Val Loss: 29.2344\n",
      "Epoch [358/500] Train Loss: 28.0095, Val Loss: 29.2344\n",
      "Epoch [359/500] Train Loss: 28.0095, Val Loss: 29.2343\n",
      "Epoch [360/500] Train Loss: 28.0095, Val Loss: 29.2343\n",
      "Epoch [361/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [362/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [363/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [364/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [365/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [366/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [367/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [368/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [369/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [370/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [371/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [372/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [373/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [374/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [375/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [376/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [377/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [378/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [379/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [380/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [381/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [382/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [383/500] Train Loss: 28.0094, Val Loss: 29.2344\n",
      "Epoch [384/500] Train Loss: 28.0095, Val Loss: 29.2343\n",
      "Epoch [385/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [386/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [387/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [388/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [389/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [390/500] Train Loss: 28.0094, Val Loss: 29.2344\n",
      "Epoch [391/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [392/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [393/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [394/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [395/500] Train Loss: 28.0093, Val Loss: 29.2343\n",
      "Epoch [396/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [397/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [398/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [399/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [400/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [401/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [402/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [403/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [404/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [405/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [406/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [407/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [408/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [409/500] Train Loss: 28.0093, Val Loss: 29.2344\n",
      "Epoch [410/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [411/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [412/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [413/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [414/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [415/500] Train Loss: 28.0094, Val Loss: 29.2342\n",
      "Epoch [416/500] Train Loss: 28.0094, Val Loss: 29.2343\n",
      "Epoch [417/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [418/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [419/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [420/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [421/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [422/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [423/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [424/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [425/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [426/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [427/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [428/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [429/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [430/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [431/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [432/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [433/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [434/500] Train Loss: 28.0093, Val Loss: 29.2343\n",
      "Epoch [435/500] Train Loss: 28.0093, Val Loss: 29.2343\n",
      "Epoch [436/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [437/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [438/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [439/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [440/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [441/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [442/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [443/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [444/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [445/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [446/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [447/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [448/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [449/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [450/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [451/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [452/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [453/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [454/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [455/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [456/500] Train Loss: 28.0092, Val Loss: 29.2341\n",
      "Epoch [457/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [458/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [459/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [460/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [461/500] Train Loss: 28.0092, Val Loss: 29.2341\n",
      "Epoch [462/500] Train Loss: 28.0092, Val Loss: 29.2342\n",
      "Epoch [463/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [464/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [465/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [466/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [467/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [468/500] Train Loss: 28.0092, Val Loss: 29.2341\n",
      "Epoch [469/500] Train Loss: 28.0092, Val Loss: 29.2341\n",
      "Epoch [470/500] Train Loss: 28.0092, Val Loss: 29.2341\n",
      "Epoch [471/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [472/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [473/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [474/500] Train Loss: 28.0093, Val Loss: 29.2343\n",
      "Epoch [475/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [476/500] Train Loss: 28.0092, Val Loss: 29.2341\n",
      "Epoch [477/500] Train Loss: 28.0092, Val Loss: 29.2341\n",
      "Epoch [478/500] Train Loss: 28.0092, Val Loss: 29.2342\n",
      "Epoch [479/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [480/500] Train Loss: 28.0092, Val Loss: 29.2341\n",
      "Epoch [481/500] Train Loss: 28.0092, Val Loss: 29.2342\n",
      "Epoch [482/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [483/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [484/500] Train Loss: 28.0092, Val Loss: 29.2341\n",
      "Epoch [485/500] Train Loss: 28.0092, Val Loss: 29.2342\n",
      "Epoch [486/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [487/500] Train Loss: 28.0092, Val Loss: 29.2341\n",
      "Epoch [488/500] Train Loss: 28.0092, Val Loss: 29.2341\n",
      "Epoch [489/500] Train Loss: 28.0092, Val Loss: 29.2341\n",
      "Epoch [490/500] Train Loss: 28.0092, Val Loss: 29.2342\n",
      "Epoch [491/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [492/500] Train Loss: 28.0093, Val Loss: 29.2343\n",
      "Epoch [493/500] Train Loss: 28.0093, Val Loss: 29.2341\n",
      "Epoch [494/500] Train Loss: 28.0093, Val Loss: 29.2342\n",
      "Epoch [495/500] Train Loss: 28.0092, Val Loss: 29.2341\n",
      "Epoch [496/500] Train Loss: 28.0092, Val Loss: 29.2341\n",
      "Epoch [497/500] Train Loss: 28.0092, Val Loss: 29.2341\n",
      "Epoch [498/500] Train Loss: 28.0092, Val Loss: 29.2341\n",
      "Epoch [499/500] Train Loss: 28.0092, Val Loss: 29.2341\n",
      "Epoch [500/500] Train Loss: 28.0092, Val Loss: 29.2341\n",
      "\n",
      "=== Fold 10 ===\n",
      "Epoch [1/500] Train Loss: 40.2707, Val Loss: 40.4223\n",
      "Epoch [2/500] Train Loss: 40.2011, Val Loss: 40.3482\n",
      "Epoch [3/500] Train Loss: 40.1222, Val Loss: 40.2538\n",
      "Epoch [4/500] Train Loss: 40.0092, Val Loss: 40.1106\n",
      "Epoch [5/500] Train Loss: 39.8361, Val Loss: 39.8778\n",
      "Epoch [6/500] Train Loss: 39.5469, Val Loss: 39.4528\n",
      "Epoch [7/500] Train Loss: 38.9694, Val Loss: 38.5644\n",
      "Epoch [8/500] Train Loss: 37.7585, Val Loss: 36.5688\n",
      "Epoch [9/500] Train Loss: 34.9747, Val Loss: 32.2017\n",
      "Epoch [10/500] Train Loss: 29.2243, Val Loss: 23.7819\n",
      "Epoch [11/500] Train Loss: 19.3321, Val Loss: 12.1194\n",
      "Epoch [12/500] Train Loss: 10.4836, Val Loss: 8.9676\n",
      "Epoch [13/500] Train Loss: 9.7594, Val Loss: 8.3544\n",
      "Epoch [14/500] Train Loss: 9.1766, Val Loss: 8.2583\n",
      "Epoch [15/500] Train Loss: 9.0714, Val Loss: 8.1588\n",
      "Epoch [16/500] Train Loss: 8.9668, Val Loss: 8.1122\n",
      "Epoch [17/500] Train Loss: 8.8969, Val Loss: 8.0623\n",
      "Epoch [18/500] Train Loss: 8.8445, Val Loss: 8.0366\n",
      "Epoch [19/500] Train Loss: 8.7970, Val Loss: 8.0049\n",
      "Epoch [20/500] Train Loss: 8.7554, Val Loss: 7.9727\n",
      "Epoch [21/500] Train Loss: 8.7143, Val Loss: 7.9453\n",
      "Epoch [22/500] Train Loss: 8.6763, Val Loss: 7.9112\n",
      "Epoch [23/500] Train Loss: 8.6405, Val Loss: 7.8819\n",
      "Epoch [24/500] Train Loss: 8.6024, Val Loss: 7.8500\n",
      "Epoch [25/500] Train Loss: 8.5671, Val Loss: 7.8208\n",
      "Epoch [26/500] Train Loss: 8.5328, Val Loss: 7.7928\n",
      "Epoch [27/500] Train Loss: 8.5019, Val Loss: 7.7617\n",
      "Epoch [28/500] Train Loss: 8.4744, Val Loss: 7.7394\n",
      "Epoch [29/500] Train Loss: 8.4470, Val Loss: 7.7139\n",
      "Epoch [30/500] Train Loss: 8.4211, Val Loss: 7.6911\n",
      "Epoch [31/500] Train Loss: 8.3974, Val Loss: 7.6681\n",
      "Epoch [32/500] Train Loss: 8.3755, Val Loss: 7.6496\n",
      "Epoch [33/500] Train Loss: 8.3550, Val Loss: 7.6331\n",
      "Epoch [34/500] Train Loss: 8.3343, Val Loss: 7.6137\n",
      "Epoch [35/500] Train Loss: 8.3166, Val Loss: 7.5969\n",
      "Epoch [36/500] Train Loss: 8.2985, Val Loss: 7.5767\n",
      "Epoch [37/500] Train Loss: 8.2845, Val Loss: 7.5594\n",
      "Epoch [38/500] Train Loss: 8.2688, Val Loss: 7.5489\n",
      "Epoch [39/500] Train Loss: 8.2540, Val Loss: 7.5349\n",
      "Epoch [40/500] Train Loss: 8.2413, Val Loss: 7.5213\n",
      "Epoch [41/500] Train Loss: 8.2305, Val Loss: 7.4989\n",
      "Epoch [42/500] Train Loss: 8.2183, Val Loss: 7.4906\n",
      "Epoch [43/500] Train Loss: 8.2061, Val Loss: 7.4774\n",
      "Epoch [44/500] Train Loss: 8.1964, Val Loss: 7.4639\n",
      "Epoch [45/500] Train Loss: 8.1864, Val Loss: 7.4556\n",
      "Epoch [46/500] Train Loss: 8.1763, Val Loss: 7.4376\n",
      "Epoch [47/500] Train Loss: 8.1688, Val Loss: 7.4310\n",
      "Epoch [48/500] Train Loss: 8.1619, Val Loss: 7.4230\n",
      "Epoch [49/500] Train Loss: 8.1529, Val Loss: 7.4069\n",
      "Epoch [50/500] Train Loss: 8.1465, Val Loss: 7.4076\n",
      "Epoch [51/500] Train Loss: 8.1392, Val Loss: 7.3919\n",
      "Epoch [52/500] Train Loss: 8.1330, Val Loss: 7.3839\n",
      "Epoch [53/500] Train Loss: 8.1268, Val Loss: 7.3839\n",
      "Epoch [54/500] Train Loss: 8.1201, Val Loss: 7.3759\n",
      "Epoch [55/500] Train Loss: 8.1150, Val Loss: 7.3708\n",
      "Epoch [56/500] Train Loss: 8.1099, Val Loss: 7.3588\n",
      "Epoch [57/500] Train Loss: 8.1050, Val Loss: 7.3508\n",
      "Epoch [58/500] Train Loss: 8.0997, Val Loss: 7.3438\n",
      "Epoch [59/500] Train Loss: 8.0969, Val Loss: 7.3450\n",
      "Epoch [60/500] Train Loss: 8.0918, Val Loss: 7.3340\n",
      "Epoch [61/500] Train Loss: 8.0869, Val Loss: 7.3363\n",
      "Epoch [62/500] Train Loss: 8.0839, Val Loss: 7.3368\n",
      "Epoch [63/500] Train Loss: 8.0802, Val Loss: 7.3281\n",
      "Epoch [64/500] Train Loss: 8.0766, Val Loss: 7.3211\n",
      "Epoch [65/500] Train Loss: 8.0740, Val Loss: 7.3187\n",
      "Epoch [66/500] Train Loss: 8.0720, Val Loss: 7.3131\n",
      "Epoch [67/500] Train Loss: 8.0682, Val Loss: 7.3132\n",
      "Epoch [68/500] Train Loss: 8.0648, Val Loss: 7.3115\n",
      "Epoch [69/500] Train Loss: 8.0625, Val Loss: 7.3045\n",
      "Epoch [70/500] Train Loss: 8.0594, Val Loss: 7.3029\n",
      "Epoch [71/500] Train Loss: 8.0570, Val Loss: 7.3035\n",
      "Epoch [72/500] Train Loss: 8.0555, Val Loss: 7.3083\n",
      "Epoch [73/500] Train Loss: 8.0543, Val Loss: 7.2984\n",
      "Epoch [74/500] Train Loss: 8.0514, Val Loss: 7.2989\n",
      "Epoch [75/500] Train Loss: 8.0497, Val Loss: 7.2942\n",
      "Epoch [76/500] Train Loss: 8.0469, Val Loss: 7.2923\n",
      "Epoch [77/500] Train Loss: 8.0451, Val Loss: 7.2914\n",
      "Epoch [78/500] Train Loss: 8.0427, Val Loss: 7.2876\n",
      "Epoch [79/500] Train Loss: 8.0390, Val Loss: 7.2850\n",
      "Epoch [80/500] Train Loss: 8.0364, Val Loss: 7.2756\n",
      "Epoch [81/500] Train Loss: 8.0345, Val Loss: 7.2778\n",
      "Epoch [82/500] Train Loss: 8.0320, Val Loss: 7.2754\n",
      "Epoch [83/500] Train Loss: 8.0288, Val Loss: 7.2698\n",
      "Epoch [84/500] Train Loss: 8.0257, Val Loss: 7.2708\n",
      "Epoch [85/500] Train Loss: 8.0217, Val Loss: 7.2787\n",
      "Epoch [86/500] Train Loss: 8.0194, Val Loss: 7.2631\n",
      "Epoch [87/500] Train Loss: 8.0165, Val Loss: 7.2608\n",
      "Epoch [88/500] Train Loss: 8.0111, Val Loss: 7.2576\n",
      "Epoch [89/500] Train Loss: 8.0048, Val Loss: 7.2478\n",
      "Epoch [90/500] Train Loss: 8.0008, Val Loss: 7.2317\n",
      "Epoch [91/500] Train Loss: 7.9912, Val Loss: 7.2353\n",
      "Epoch [92/500] Train Loss: 7.9841, Val Loss: 7.2375\n",
      "Epoch [93/500] Train Loss: 7.9758, Val Loss: 7.2194\n",
      "Epoch [94/500] Train Loss: 7.9628, Val Loss: 7.2074\n",
      "Epoch [95/500] Train Loss: 7.9483, Val Loss: 7.1834\n",
      "Epoch [96/500] Train Loss: 7.9314, Val Loss: 7.1574\n",
      "Epoch [97/500] Train Loss: 7.9034, Val Loss: 7.1487\n",
      "Epoch [98/500] Train Loss: 7.8740, Val Loss: 7.1015\n",
      "Epoch [99/500] Train Loss: 7.8327, Val Loss: 7.0551\n",
      "Epoch [100/500] Train Loss: 7.7696, Val Loss: 6.9672\n",
      "Epoch [101/500] Train Loss: 7.6827, Val Loss: 6.8610\n",
      "Epoch [102/500] Train Loss: 7.5571, Val Loss: 6.6993\n",
      "Epoch [103/500] Train Loss: 7.3519, Val Loss: 6.4159\n",
      "Epoch [104/500] Train Loss: 7.0339, Val Loss: 6.0249\n",
      "Epoch [105/500] Train Loss: 6.5990, Val Loss: 5.4982\n",
      "Epoch [106/500] Train Loss: 6.0202, Val Loss: 4.8711\n",
      "Epoch [107/500] Train Loss: 5.4818, Val Loss: 4.4176\n",
      "Epoch [108/500] Train Loss: 5.1350, Val Loss: 4.1505\n",
      "Epoch [109/500] Train Loss: 4.9418, Val Loss: 4.0423\n",
      "Epoch [110/500] Train Loss: 4.8516, Val Loss: 3.9837\n",
      "Epoch [111/500] Train Loss: 4.7928, Val Loss: 3.9464\n",
      "Epoch [112/500] Train Loss: 4.7488, Val Loss: 3.9180\n",
      "Epoch [113/500] Train Loss: 4.7090, Val Loss: 3.8847\n",
      "Epoch [114/500] Train Loss: 4.6718, Val Loss: 3.8497\n",
      "Epoch [115/500] Train Loss: 4.6308, Val Loss: 3.8140\n",
      "Epoch [116/500] Train Loss: 4.5936, Val Loss: 3.7861\n",
      "Epoch [117/500] Train Loss: 4.5631, Val Loss: 3.7621\n",
      "Epoch [118/500] Train Loss: 4.5325, Val Loss: 3.7363\n",
      "Epoch [119/500] Train Loss: 4.5068, Val Loss: 3.7142\n",
      "Epoch [120/500] Train Loss: 4.4808, Val Loss: 3.6929\n",
      "Epoch [121/500] Train Loss: 4.4574, Val Loss: 3.6727\n",
      "Epoch [122/500] Train Loss: 4.4349, Val Loss: 3.6545\n",
      "Epoch [123/500] Train Loss: 4.4134, Val Loss: 3.6504\n",
      "Epoch [124/500] Train Loss: 4.3927, Val Loss: 3.6175\n",
      "Epoch [125/500] Train Loss: 4.3728, Val Loss: 3.5994\n",
      "Epoch [126/500] Train Loss: 4.3506, Val Loss: 3.5820\n",
      "Epoch [127/500] Train Loss: 4.3290, Val Loss: 3.5654\n",
      "Epoch [128/500] Train Loss: 4.3113, Val Loss: 3.5471\n",
      "Epoch [129/500] Train Loss: 4.2884, Val Loss: 3.5335\n",
      "Epoch [130/500] Train Loss: 4.2674, Val Loss: 3.5127\n",
      "Epoch [131/500] Train Loss: 4.2383, Val Loss: 3.4865\n",
      "Epoch [132/500] Train Loss: 4.2126, Val Loss: 3.4796\n",
      "Epoch [133/500] Train Loss: 4.1907, Val Loss: 3.4552\n",
      "Epoch [134/500] Train Loss: 4.1661, Val Loss: 3.4384\n",
      "Epoch [135/500] Train Loss: 4.1443, Val Loss: 3.4265\n",
      "Epoch [136/500] Train Loss: 4.1257, Val Loss: 3.4080\n",
      "Epoch [137/500] Train Loss: 4.1071, Val Loss: 3.4029\n",
      "Epoch [138/500] Train Loss: 4.0932, Val Loss: 3.3732\n",
      "Epoch [139/500] Train Loss: 4.0710, Val Loss: 3.3656\n",
      "Epoch [140/500] Train Loss: 4.0544, Val Loss: 3.3450\n",
      "Epoch [141/500] Train Loss: 4.0285, Val Loss: 3.3222\n",
      "Epoch [142/500] Train Loss: 3.9988, Val Loss: 3.2955\n",
      "Epoch [143/500] Train Loss: 3.9736, Val Loss: 3.2725\n",
      "Epoch [144/500] Train Loss: 3.9489, Val Loss: 3.2507\n",
      "Epoch [145/500] Train Loss: 3.9286, Val Loss: 3.2393\n",
      "Epoch [146/500] Train Loss: 3.9137, Val Loss: 3.2232\n",
      "Epoch [147/500] Train Loss: 3.8976, Val Loss: 3.2122\n",
      "Epoch [148/500] Train Loss: 3.8873, Val Loss: 3.2006\n",
      "Epoch [149/500] Train Loss: 3.8742, Val Loss: 3.1897\n",
      "Epoch [150/500] Train Loss: 3.8609, Val Loss: 3.1824\n",
      "Epoch [151/500] Train Loss: 3.8501, Val Loss: 3.1664\n",
      "Epoch [152/500] Train Loss: 3.8379, Val Loss: 3.1604\n",
      "Epoch [153/500] Train Loss: 3.8250, Val Loss: 3.1538\n",
      "Epoch [154/500] Train Loss: 3.8134, Val Loss: 3.1407\n",
      "Epoch [155/500] Train Loss: 3.8004, Val Loss: 3.1316\n",
      "Epoch [156/500] Train Loss: 3.7902, Val Loss: 3.1243\n",
      "Epoch [157/500] Train Loss: 3.7756, Val Loss: 3.1164\n",
      "Epoch [158/500] Train Loss: 3.7642, Val Loss: 3.1074\n",
      "Epoch [159/500] Train Loss: 3.7516, Val Loss: 3.1000\n",
      "Epoch [160/500] Train Loss: 3.7419, Val Loss: 3.0929\n",
      "Epoch [161/500] Train Loss: 3.7258, Val Loss: 3.0843\n",
      "Epoch [162/500] Train Loss: 3.7216, Val Loss: 3.0956\n",
      "Epoch [163/500] Train Loss: 3.7084, Val Loss: 3.0794\n",
      "Epoch [164/500] Train Loss: 3.6966, Val Loss: 3.0610\n",
      "Epoch [165/500] Train Loss: 3.6795, Val Loss: 3.0552\n",
      "Epoch [166/500] Train Loss: 3.6669, Val Loss: 3.0403\n",
      "Epoch [167/500] Train Loss: 3.6512, Val Loss: 3.0336\n",
      "Epoch [168/500] Train Loss: 3.6391, Val Loss: 3.0298\n",
      "Epoch [169/500] Train Loss: 3.6270, Val Loss: 3.0221\n",
      "Epoch [170/500] Train Loss: 3.6147, Val Loss: 3.0187\n",
      "Epoch [171/500] Train Loss: 3.5973, Val Loss: 3.0065\n",
      "Epoch [172/500] Train Loss: 3.5856, Val Loss: 3.0016\n",
      "Epoch [173/500] Train Loss: 3.5747, Val Loss: 2.9921\n",
      "Epoch [174/500] Train Loss: 3.5558, Val Loss: 2.9825\n",
      "Epoch [175/500] Train Loss: 3.5467, Val Loss: 2.9989\n",
      "Epoch [176/500] Train Loss: 3.5391, Val Loss: 2.9772\n",
      "Epoch [177/500] Train Loss: 3.5228, Val Loss: 2.9669\n",
      "Epoch [178/500] Train Loss: 3.5035, Val Loss: 2.9792\n",
      "Epoch [179/500] Train Loss: 3.5126, Val Loss: 2.9682\n",
      "Epoch [180/500] Train Loss: 3.4821, Val Loss: 2.9447\n",
      "Epoch [181/500] Train Loss: 3.4699, Val Loss: 2.9463\n",
      "Epoch [182/500] Train Loss: 3.4544, Val Loss: 2.9419\n",
      "Epoch [183/500] Train Loss: 3.4351, Val Loss: 2.9272\n",
      "Epoch [184/500] Train Loss: 3.4256, Val Loss: 2.9204\n",
      "Epoch [185/500] Train Loss: 3.4261, Val Loss: 2.9286\n",
      "Epoch [186/500] Train Loss: 3.4075, Val Loss: 2.9065\n",
      "Epoch [187/500] Train Loss: 3.3902, Val Loss: 2.9345\n",
      "Epoch [188/500] Train Loss: 3.3829, Val Loss: 2.8872\n",
      "Epoch [189/500] Train Loss: 3.3655, Val Loss: 2.9050\n",
      "Epoch [190/500] Train Loss: 3.3521, Val Loss: 2.8936\n",
      "Epoch [191/500] Train Loss: 3.3401, Val Loss: 2.8764\n",
      "Epoch [192/500] Train Loss: 3.3316, Val Loss: 2.8941\n",
      "Epoch [193/500] Train Loss: 3.3077, Val Loss: 2.8859\n",
      "Epoch [194/500] Train Loss: 3.3046, Val Loss: 2.8807\n",
      "Epoch [195/500] Train Loss: 3.2975, Val Loss: 2.8652\n",
      "Epoch [196/500] Train Loss: 3.2972, Val Loss: 2.8878\n",
      "Epoch [197/500] Train Loss: 3.2673, Val Loss: 2.8606\n",
      "Epoch [198/500] Train Loss: 3.2667, Val Loss: 2.8517\n",
      "Epoch [199/500] Train Loss: 3.2550, Val Loss: 2.8918\n",
      "Epoch [200/500] Train Loss: 3.2362, Val Loss: 2.9103\n",
      "Epoch [201/500] Train Loss: 3.2341, Val Loss: 2.8657\n",
      "Epoch [202/500] Train Loss: 3.2196, Val Loss: 2.8592\n",
      "Epoch [203/500] Train Loss: 3.1947, Val Loss: 2.8510\n",
      "Epoch [204/500] Train Loss: 3.1852, Val Loss: 2.8816\n",
      "Epoch [205/500] Train Loss: 3.1787, Val Loss: 2.8630\n",
      "Epoch [206/500] Train Loss: 3.1646, Val Loss: 2.8478\n",
      "Epoch [207/500] Train Loss: 3.1492, Val Loss: 2.8600\n",
      "Epoch [208/500] Train Loss: 3.1440, Val Loss: 2.8454\n",
      "Epoch [209/500] Train Loss: 3.1399, Val Loss: 2.8421\n",
      "Epoch [210/500] Train Loss: 3.1498, Val Loss: 2.8877\n",
      "Epoch [211/500] Train Loss: 3.1280, Val Loss: 2.8427\n",
      "Epoch [212/500] Train Loss: 3.1054, Val Loss: 2.8394\n",
      "Epoch [213/500] Train Loss: 3.0837, Val Loss: 2.8712\n",
      "Epoch [214/500] Train Loss: 3.0786, Val Loss: 2.8538\n",
      "Epoch [215/500] Train Loss: 3.0763, Val Loss: 2.8542\n",
      "Epoch [216/500] Train Loss: 3.0694, Val Loss: 2.8338\n",
      "Epoch [217/500] Train Loss: 3.0536, Val Loss: 2.8755\n",
      "Epoch [218/500] Train Loss: 3.0705, Val Loss: 2.8526\n",
      "Epoch [219/500] Train Loss: 3.0379, Val Loss: 2.8417\n",
      "Epoch [220/500] Train Loss: 3.0340, Val Loss: 2.8610\n",
      "Epoch [221/500] Train Loss: 3.0099, Val Loss: 2.8544\n",
      "Epoch [222/500] Train Loss: 2.9844, Val Loss: 2.8443\n",
      "Epoch [223/500] Train Loss: 2.9817, Val Loss: 2.8316\n",
      "Epoch [224/500] Train Loss: 2.9721, Val Loss: 2.8237\n",
      "Epoch [225/500] Train Loss: 2.9554, Val Loss: 2.8424\n",
      "Epoch [226/500] Train Loss: 2.9410, Val Loss: 2.8086\n",
      "Epoch [227/500] Train Loss: 2.9640, Val Loss: 2.8016\n",
      "Epoch [228/500] Train Loss: 2.9347, Val Loss: 2.8620\n",
      "Epoch [229/500] Train Loss: 2.9259, Val Loss: 2.8192\n",
      "Epoch [230/500] Train Loss: 2.9283, Val Loss: 2.8049\n",
      "Epoch [231/500] Train Loss: 2.9078, Val Loss: 2.8148\n",
      "Epoch [232/500] Train Loss: 2.8895, Val Loss: 2.7930\n",
      "Epoch [233/500] Train Loss: 2.8777, Val Loss: 2.8604\n",
      "Epoch [234/500] Train Loss: 2.8826, Val Loss: 2.8102\n",
      "Epoch [235/500] Train Loss: 2.8718, Val Loss: 2.8603\n",
      "Epoch [236/500] Train Loss: 2.8716, Val Loss: 2.8447\n",
      "Epoch [237/500] Train Loss: 2.8891, Val Loss: 2.8356\n",
      "Epoch [238/500] Train Loss: 2.8406, Val Loss: 2.8044\n",
      "Epoch [239/500] Train Loss: 2.8386, Val Loss: 2.7812\n",
      "Epoch [240/500] Train Loss: 2.8230, Val Loss: 2.7789\n",
      "Epoch [241/500] Train Loss: 2.8099, Val Loss: 2.7864\n",
      "Epoch [242/500] Train Loss: 2.8061, Val Loss: 2.7995\n",
      "Epoch [243/500] Train Loss: 2.8023, Val Loss: 2.7837\n",
      "Epoch [244/500] Train Loss: 2.7914, Val Loss: 2.8052\n",
      "Epoch [245/500] Train Loss: 2.7846, Val Loss: 2.8147\n",
      "Epoch [246/500] Train Loss: 2.7946, Val Loss: 2.8419\n",
      "Epoch [247/500] Train Loss: 2.7747, Val Loss: 2.8066\n",
      "Epoch [248/500] Train Loss: 2.7607, Val Loss: 2.7852\n",
      "Epoch [249/500] Train Loss: 2.7584, Val Loss: 2.7928\n",
      "Epoch [250/500] Train Loss: 2.7800, Val Loss: 2.7703\n",
      "Epoch [251/500] Train Loss: 2.7817, Val Loss: 2.8242\n",
      "Epoch [252/500] Train Loss: 2.7847, Val Loss: 2.7692\n",
      "Epoch [253/500] Train Loss: 2.7664, Val Loss: 2.7487\n",
      "Epoch [254/500] Train Loss: 2.7456, Val Loss: 2.8445\n",
      "Epoch [255/500] Train Loss: 2.7512, Val Loss: 2.7809\n",
      "Epoch [256/500] Train Loss: 2.7260, Val Loss: 2.8362\n",
      "Epoch [257/500] Train Loss: 2.7271, Val Loss: 2.7520\n",
      "Epoch [258/500] Train Loss: 2.7084, Val Loss: 2.7518\n",
      "Epoch [259/500] Train Loss: 2.6928, Val Loss: 2.7525\n",
      "Epoch [260/500] Train Loss: 2.6891, Val Loss: 2.7406\n",
      "Epoch [261/500] Train Loss: 2.7044, Val Loss: 2.7413\n",
      "Epoch [262/500] Train Loss: 2.6965, Val Loss: 2.7926\n",
      "Epoch [263/500] Train Loss: 2.6821, Val Loss: 2.7214\n",
      "Epoch [264/500] Train Loss: 2.6691, Val Loss: 2.7548\n",
      "Epoch [265/500] Train Loss: 2.6691, Val Loss: 2.7484\n",
      "Epoch [266/500] Train Loss: 2.6747, Val Loss: 2.7962\n",
      "Epoch [267/500] Train Loss: 2.6707, Val Loss: 2.7162\n",
      "Epoch [268/500] Train Loss: 2.6483, Val Loss: 2.7392\n",
      "Epoch [269/500] Train Loss: 2.6507, Val Loss: 2.7274\n",
      "Epoch [270/500] Train Loss: 2.6383, Val Loss: 2.7438\n",
      "Epoch [271/500] Train Loss: 2.6491, Val Loss: 2.7476\n",
      "Epoch [272/500] Train Loss: 2.6333, Val Loss: 2.7329\n",
      "Epoch [273/500] Train Loss: 2.6462, Val Loss: 2.6948\n",
      "Epoch [274/500] Train Loss: 2.6185, Val Loss: 2.7148\n",
      "Epoch [275/500] Train Loss: 2.6211, Val Loss: 2.6799\n",
      "Epoch [276/500] Train Loss: 2.6084, Val Loss: 2.7469\n",
      "Epoch [277/500] Train Loss: 2.6058, Val Loss: 2.6821\n",
      "Epoch [278/500] Train Loss: 2.6107, Val Loss: 2.7272\n",
      "Epoch [279/500] Train Loss: 2.5931, Val Loss: 2.7298\n",
      "Epoch [280/500] Train Loss: 2.5959, Val Loss: 2.7820\n",
      "Epoch [281/500] Train Loss: 2.6204, Val Loss: 2.7072\n",
      "Epoch [282/500] Train Loss: 2.6105, Val Loss: 2.7042\n",
      "Epoch [283/500] Train Loss: 2.5889, Val Loss: 2.7645\n",
      "Epoch [284/500] Train Loss: 2.5848, Val Loss: 2.6720\n",
      "Epoch [285/500] Train Loss: 2.5657, Val Loss: 2.6955\n",
      "Epoch [286/500] Train Loss: 2.5618, Val Loss: 2.7629\n",
      "Epoch [287/500] Train Loss: 2.5699, Val Loss: 2.7009\n",
      "Epoch [288/500] Train Loss: 2.5576, Val Loss: 2.7194\n",
      "Epoch [289/500] Train Loss: 2.5511, Val Loss: 2.7678\n",
      "Epoch [290/500] Train Loss: 2.5566, Val Loss: 2.7649\n",
      "Epoch [291/500] Train Loss: 2.5650, Val Loss: 2.7303\n",
      "Epoch [292/500] Train Loss: 2.5563, Val Loss: 2.7176\n",
      "Epoch [293/500] Train Loss: 2.5342, Val Loss: 2.7084\n",
      "Epoch [294/500] Train Loss: 2.5421, Val Loss: 2.7556\n",
      "Epoch [295/500] Train Loss: 2.5405, Val Loss: 2.7488\n",
      "Epoch [296/500] Train Loss: 2.5572, Val Loss: 2.7525\n",
      "Epoch [297/500] Train Loss: 2.5524, Val Loss: 2.7994\n",
      "Epoch [298/500] Train Loss: 2.5301, Val Loss: 2.7844\n",
      "Epoch [299/500] Train Loss: 2.5417, Val Loss: 2.7641\n",
      "Epoch [300/500] Train Loss: 2.5233, Val Loss: 2.7672\n",
      "Epoch [301/500] Train Loss: 2.5027, Val Loss: 2.7443\n",
      "Epoch [302/500] Train Loss: 2.4953, Val Loss: 2.7362\n",
      "Epoch [303/500] Train Loss: 2.5018, Val Loss: 2.7657\n",
      "Epoch [304/500] Train Loss: 2.4919, Val Loss: 2.7365\n",
      "Epoch [305/500] Train Loss: 2.4856, Val Loss: 2.7433\n",
      "Epoch [306/500] Train Loss: 2.4805, Val Loss: 2.7465\n",
      "Epoch [307/500] Train Loss: 2.4710, Val Loss: 2.7333\n",
      "Epoch [308/500] Train Loss: 2.4741, Val Loss: 2.7491\n",
      "Epoch [309/500] Train Loss: 2.4842, Val Loss: 2.7476\n",
      "Epoch [310/500] Train Loss: 2.4622, Val Loss: 2.7600\n",
      "Epoch [311/500] Train Loss: 2.4657, Val Loss: 2.7649\n",
      "Epoch [312/500] Train Loss: 2.4552, Val Loss: 2.7688\n",
      "Epoch [313/500] Train Loss: 2.4348, Val Loss: 2.7691\n",
      "Epoch [314/500] Train Loss: 2.4336, Val Loss: 2.7941\n",
      "Epoch [315/500] Train Loss: 2.4306, Val Loss: 2.7758\n",
      "Epoch [316/500] Train Loss: 2.4185, Val Loss: 2.7615\n",
      "Epoch [317/500] Train Loss: 2.4122, Val Loss: 2.7777\n",
      "Epoch [318/500] Train Loss: 2.4082, Val Loss: 2.7775\n",
      "Epoch [319/500] Train Loss: 2.4085, Val Loss: 2.7796\n",
      "Epoch [320/500] Train Loss: 2.3998, Val Loss: 2.7678\n",
      "Epoch [321/500] Train Loss: 2.3964, Val Loss: 2.7772\n",
      "Epoch [322/500] Train Loss: 2.3861, Val Loss: 2.7888\n",
      "Epoch [323/500] Train Loss: 2.3939, Val Loss: 2.7648\n",
      "Epoch [324/500] Train Loss: 2.3873, Val Loss: 2.7762\n",
      "Epoch [325/500] Train Loss: 2.3735, Val Loss: 2.7827\n",
      "Epoch [326/500] Train Loss: 2.3633, Val Loss: 2.7739\n",
      "Epoch [327/500] Train Loss: 2.3606, Val Loss: 2.7857\n",
      "Epoch [328/500] Train Loss: 2.3611, Val Loss: 2.7855\n",
      "Epoch [329/500] Train Loss: 2.3732, Val Loss: 2.7741\n",
      "Epoch [330/500] Train Loss: 2.3523, Val Loss: 2.7578\n",
      "Epoch [331/500] Train Loss: 2.3460, Val Loss: 2.7471\n",
      "Epoch [332/500] Train Loss: 2.3330, Val Loss: 2.7520\n",
      "Epoch [333/500] Train Loss: 2.3275, Val Loss: 2.7474\n",
      "Epoch [334/500] Train Loss: 2.3296, Val Loss: 2.7589\n",
      "Epoch [335/500] Train Loss: 2.3287, Val Loss: 2.7500\n",
      "Epoch [336/500] Train Loss: 2.3238, Val Loss: 2.7663\n",
      "Epoch [337/500] Train Loss: 2.3357, Val Loss: 2.7393\n",
      "Epoch [338/500] Train Loss: 2.3352, Val Loss: 2.7362\n",
      "Epoch [339/500] Train Loss: 2.3040, Val Loss: 2.7245\n",
      "Epoch [340/500] Train Loss: 2.3118, Val Loss: 2.7278\n",
      "Epoch [341/500] Train Loss: 2.2992, Val Loss: 2.7239\n",
      "Epoch [342/500] Train Loss: 2.2919, Val Loss: 2.7222\n",
      "Epoch [343/500] Train Loss: 2.2873, Val Loss: 2.7525\n",
      "Epoch [344/500] Train Loss: 2.2848, Val Loss: 2.7290\n",
      "Epoch [345/500] Train Loss: 2.2756, Val Loss: 2.7254\n",
      "Epoch [346/500] Train Loss: 2.2973, Val Loss: 2.7382\n",
      "Epoch [347/500] Train Loss: 2.2752, Val Loss: 2.7288\n",
      "Epoch [348/500] Train Loss: 2.2652, Val Loss: 2.7253\n",
      "Epoch [349/500] Train Loss: 2.2750, Val Loss: 2.7793\n",
      "Epoch [350/500] Train Loss: 2.2895, Val Loss: 2.6976\n",
      "Epoch [351/500] Train Loss: 2.2608, Val Loss: 2.7288\n",
      "Epoch [352/500] Train Loss: 2.2528, Val Loss: 2.7065\n",
      "Epoch [353/500] Train Loss: 2.2467, Val Loss: 2.7054\n",
      "Epoch [354/500] Train Loss: 2.2531, Val Loss: 2.7214\n",
      "Epoch [355/500] Train Loss: 2.2463, Val Loss: 2.7030\n",
      "Epoch [356/500] Train Loss: 2.2480, Val Loss: 2.6976\n",
      "Epoch [357/500] Train Loss: 2.2354, Val Loss: 2.6900\n",
      "Epoch [358/500] Train Loss: 2.2382, Val Loss: 2.7182\n",
      "Epoch [359/500] Train Loss: 2.2287, Val Loss: 2.7094\n",
      "Epoch [360/500] Train Loss: 2.2348, Val Loss: 2.7080\n",
      "Epoch [361/500] Train Loss: 2.2410, Val Loss: 2.7301\n",
      "Epoch [362/500] Train Loss: 2.2312, Val Loss: 2.6882\n",
      "Epoch [363/500] Train Loss: 2.2153, Val Loss: 2.6827\n",
      "Epoch [364/500] Train Loss: 2.2204, Val Loss: 2.6840\n",
      "Epoch [365/500] Train Loss: 2.2167, Val Loss: 2.6902\n",
      "Epoch [366/500] Train Loss: 2.2152, Val Loss: 2.6937\n",
      "Epoch [367/500] Train Loss: 2.2124, Val Loss: 2.6712\n",
      "Epoch [368/500] Train Loss: 2.1997, Val Loss: 2.6806\n",
      "Epoch [369/500] Train Loss: 2.1956, Val Loss: 2.6845\n",
      "Epoch [370/500] Train Loss: 2.2138, Val Loss: 2.7067\n",
      "Epoch [371/500] Train Loss: 2.2020, Val Loss: 2.6766\n",
      "Epoch [372/500] Train Loss: 2.1866, Val Loss: 2.6742\n",
      "Epoch [373/500] Train Loss: 2.1749, Val Loss: 2.6618\n",
      "Epoch [374/500] Train Loss: 2.1666, Val Loss: 2.6767\n",
      "Epoch [375/500] Train Loss: 2.1783, Val Loss: 2.6697\n",
      "Epoch [376/500] Train Loss: 2.1827, Val Loss: 2.6842\n",
      "Epoch [377/500] Train Loss: 2.1654, Val Loss: 2.6557\n",
      "Epoch [378/500] Train Loss: 2.1659, Val Loss: 2.6802\n",
      "Epoch [379/500] Train Loss: 2.1570, Val Loss: 2.6655\n",
      "Epoch [380/500] Train Loss: 2.1526, Val Loss: 2.6653\n",
      "Epoch [381/500] Train Loss: 2.1553, Val Loss: 2.6641\n",
      "Epoch [382/500] Train Loss: 2.1478, Val Loss: 2.6697\n",
      "Epoch [383/500] Train Loss: 2.1567, Val Loss: 2.6573\n",
      "Epoch [384/500] Train Loss: 2.1505, Val Loss: 2.6593\n",
      "Epoch [385/500] Train Loss: 2.1579, Val Loss: 2.6563\n",
      "Epoch [386/500] Train Loss: 2.1329, Val Loss: 2.6900\n",
      "Epoch [387/500] Train Loss: 2.1592, Val Loss: 2.6529\n",
      "Epoch [388/500] Train Loss: 2.1302, Val Loss: 2.6791\n",
      "Epoch [389/500] Train Loss: 2.1731, Val Loss: 2.6985\n",
      "Epoch [390/500] Train Loss: 2.1992, Val Loss: 2.6385\n",
      "Epoch [391/500] Train Loss: 2.1301, Val Loss: 2.6356\n",
      "Epoch [392/500] Train Loss: 2.1213, Val Loss: 2.6313\n",
      "Epoch [393/500] Train Loss: 2.1142, Val Loss: 2.6433\n",
      "Epoch [394/500] Train Loss: 2.1097, Val Loss: 2.6432\n",
      "Epoch [395/500] Train Loss: 2.1152, Val Loss: 2.6397\n",
      "Epoch [396/500] Train Loss: 2.1067, Val Loss: 2.6295\n",
      "Epoch [397/500] Train Loss: 2.1135, Val Loss: 2.6254\n",
      "Epoch [398/500] Train Loss: 2.1055, Val Loss: 2.6318\n",
      "Epoch [399/500] Train Loss: 2.0987, Val Loss: 2.6313\n",
      "Epoch [400/500] Train Loss: 2.1133, Val Loss: 2.6549\n",
      "Epoch [401/500] Train Loss: 2.1027, Val Loss: 2.6237\n",
      "Epoch [402/500] Train Loss: 2.0882, Val Loss: 2.6212\n",
      "Epoch [403/500] Train Loss: 2.0898, Val Loss: 2.6062\n",
      "Epoch [404/500] Train Loss: 2.1145, Val Loss: 2.6005\n",
      "Epoch [405/500] Train Loss: 2.0895, Val Loss: 2.6413\n",
      "Epoch [406/500] Train Loss: 2.0756, Val Loss: 2.6006\n",
      "Epoch [407/500] Train Loss: 2.0973, Val Loss: 2.6121\n",
      "Epoch [408/500] Train Loss: 2.0717, Val Loss: 2.5889\n",
      "Epoch [409/500] Train Loss: 2.0689, Val Loss: 2.5900\n",
      "Epoch [410/500] Train Loss: 2.0776, Val Loss: 2.5946\n",
      "Epoch [411/500] Train Loss: 2.0597, Val Loss: 2.6037\n",
      "Epoch [412/500] Train Loss: 2.0630, Val Loss: 2.5850\n",
      "Epoch [413/500] Train Loss: 2.0546, Val Loss: 2.5875\n",
      "Epoch [414/500] Train Loss: 2.0505, Val Loss: 2.5912\n",
      "Epoch [415/500] Train Loss: 2.0453, Val Loss: 2.5726\n",
      "Epoch [416/500] Train Loss: 2.0451, Val Loss: 2.6004\n",
      "Epoch [417/500] Train Loss: 2.0659, Val Loss: 2.5992\n",
      "Epoch [418/500] Train Loss: 2.0693, Val Loss: 2.6171\n",
      "Epoch [419/500] Train Loss: 2.0591, Val Loss: 2.5670\n",
      "Epoch [420/500] Train Loss: 2.0409, Val Loss: 2.5771\n",
      "Epoch [421/500] Train Loss: 2.0308, Val Loss: 2.5461\n",
      "Epoch [422/500] Train Loss: 2.0336, Val Loss: 2.5501\n",
      "Epoch [423/500] Train Loss: 2.0273, Val Loss: 2.5681\n",
      "Epoch [424/500] Train Loss: 2.0204, Val Loss: 2.5523\n",
      "Epoch [425/500] Train Loss: 2.0186, Val Loss: 2.5612\n",
      "Epoch [426/500] Train Loss: 2.0235, Val Loss: 2.5457\n",
      "Epoch [427/500] Train Loss: 2.0111, Val Loss: 2.5453\n",
      "Epoch [428/500] Train Loss: 2.0147, Val Loss: 2.5621\n",
      "Epoch [429/500] Train Loss: 2.0115, Val Loss: 2.5351\n",
      "Epoch [430/500] Train Loss: 2.0289, Val Loss: 2.5567\n",
      "Epoch [431/500] Train Loss: 2.0162, Val Loss: 2.5481\n",
      "Epoch [432/500] Train Loss: 2.0009, Val Loss: 2.6122\n",
      "Epoch [433/500] Train Loss: 2.0727, Val Loss: 2.5513\n",
      "Epoch [434/500] Train Loss: 2.0349, Val Loss: 2.5554\n",
      "Epoch [435/500] Train Loss: 2.0225, Val Loss: 2.5489\n",
      "Epoch [436/500] Train Loss: 2.0234, Val Loss: 2.5777\n",
      "Epoch [437/500] Train Loss: 1.9983, Val Loss: 2.5302\n",
      "Epoch [438/500] Train Loss: 1.9821, Val Loss: 2.5335\n",
      "Epoch [439/500] Train Loss: 1.9835, Val Loss: 2.5318\n",
      "Epoch [440/500] Train Loss: 1.9836, Val Loss: 2.5264\n",
      "Epoch [441/500] Train Loss: 1.9891, Val Loss: 2.5292\n",
      "Epoch [442/500] Train Loss: 1.9743, Val Loss: 2.5097\n",
      "Epoch [443/500] Train Loss: 1.9830, Val Loss: 2.5075\n",
      "Epoch [444/500] Train Loss: 1.9781, Val Loss: 2.5132\n",
      "Epoch [445/500] Train Loss: 1.9701, Val Loss: 2.5177\n",
      "Epoch [446/500] Train Loss: 1.9751, Val Loss: 2.5213\n",
      "Epoch [447/500] Train Loss: 1.9693, Val Loss: 2.5055\n",
      "Epoch [448/500] Train Loss: 1.9597, Val Loss: 2.5101\n",
      "Epoch [449/500] Train Loss: 1.9532, Val Loss: 2.4921\n",
      "Epoch [450/500] Train Loss: 1.9520, Val Loss: 2.4901\n",
      "Epoch [451/500] Train Loss: 1.9470, Val Loss: 2.4828\n",
      "Epoch [452/500] Train Loss: 1.9472, Val Loss: 2.4857\n",
      "Epoch [453/500] Train Loss: 1.9451, Val Loss: 2.5039\n",
      "Epoch [454/500] Train Loss: 1.9494, Val Loss: 2.5111\n",
      "Epoch [455/500] Train Loss: 1.9456, Val Loss: 2.5012\n",
      "Epoch [456/500] Train Loss: 1.9700, Val Loss: 2.5049\n",
      "Epoch [457/500] Train Loss: 1.9411, Val Loss: 2.4942\n",
      "Epoch [458/500] Train Loss: 1.9548, Val Loss: 2.4670\n",
      "Epoch [459/500] Train Loss: 1.9424, Val Loss: 2.5525\n",
      "Epoch [460/500] Train Loss: 1.9760, Val Loss: 2.5066\n",
      "Epoch [461/500] Train Loss: 1.9454, Val Loss: 2.4848\n",
      "Epoch [462/500] Train Loss: 1.9273, Val Loss: 2.4927\n",
      "Epoch [463/500] Train Loss: 1.9310, Val Loss: 2.4788\n",
      "Epoch [464/500] Train Loss: 1.9151, Val Loss: 2.4552\n",
      "Epoch [465/500] Train Loss: 1.9266, Val Loss: 2.4708\n",
      "Epoch [466/500] Train Loss: 1.9297, Val Loss: 2.4847\n",
      "Epoch [467/500] Train Loss: 1.9212, Val Loss: 2.4620\n",
      "Epoch [468/500] Train Loss: 1.9279, Val Loss: 2.4582\n",
      "Epoch [469/500] Train Loss: 1.9171, Val Loss: 2.4513\n",
      "Epoch [470/500] Train Loss: 1.9203, Val Loss: 2.4780\n",
      "Epoch [471/500] Train Loss: 1.9213, Val Loss: 2.4800\n",
      "Epoch [472/500] Train Loss: 1.9120, Val Loss: 2.4562\n",
      "Epoch [473/500] Train Loss: 1.9083, Val Loss: 2.4510\n",
      "Epoch [474/500] Train Loss: 1.9051, Val Loss: 2.4485\n",
      "Epoch [475/500] Train Loss: 1.8986, Val Loss: 2.4464\n",
      "Epoch [476/500] Train Loss: 1.9146, Val Loss: 2.4635\n",
      "Epoch [477/500] Train Loss: 1.8896, Val Loss: 2.4678\n",
      "Epoch [478/500] Train Loss: 1.9017, Val Loss: 2.4545\n",
      "Epoch [479/500] Train Loss: 1.9125, Val Loss: 2.4402\n",
      "Epoch [480/500] Train Loss: 1.8923, Val Loss: 2.4472\n",
      "Epoch [481/500] Train Loss: 1.8892, Val Loss: 2.4464\n",
      "Epoch [482/500] Train Loss: 1.8938, Val Loss: 2.4385\n",
      "Epoch [483/500] Train Loss: 1.8851, Val Loss: 2.4304\n",
      "Epoch [484/500] Train Loss: 1.8914, Val Loss: 2.4463\n",
      "Epoch [485/500] Train Loss: 1.8752, Val Loss: 2.4221\n",
      "Epoch [486/500] Train Loss: 1.8770, Val Loss: 2.4243\n",
      "Epoch [487/500] Train Loss: 1.8795, Val Loss: 2.4237\n",
      "Epoch [488/500] Train Loss: 1.8767, Val Loss: 2.4314\n",
      "Epoch [489/500] Train Loss: 1.8715, Val Loss: 2.4229\n",
      "Epoch [490/500] Train Loss: 1.8613, Val Loss: 2.4196\n",
      "Epoch [491/500] Train Loss: 1.8599, Val Loss: 2.4163\n",
      "Epoch [492/500] Train Loss: 1.8605, Val Loss: 2.4333\n",
      "Epoch [493/500] Train Loss: 1.8768, Val Loss: 2.4233\n",
      "Epoch [494/500] Train Loss: 1.8619, Val Loss: 2.4108\n",
      "Epoch [495/500] Train Loss: 1.8598, Val Loss: 2.4336\n",
      "Epoch [496/500] Train Loss: 1.8540, Val Loss: 2.4218\n",
      "Epoch [497/500] Train Loss: 1.8676, Val Loss: 2.4290\n",
      "Epoch [498/500] Train Loss: 1.8638, Val Loss: 2.4387\n",
      "Epoch [499/500] Train Loss: 1.8666, Val Loss: 2.4169\n",
      "Epoch [500/500] Train Loss: 1.8537, Val Loss: 2.4456\n",
      "\n",
      "=== Fold 11 ===\n",
      "Epoch [1/500] Train Loss: 39.7788, Val Loss: 43.0748\n",
      "Epoch [2/500] Train Loss: 39.7498, Val Loss: 43.0410\n",
      "Epoch [3/500] Train Loss: 39.7191, Val Loss: 43.0045\n",
      "Epoch [4/500] Train Loss: 39.6845, Val Loss: 42.9614\n",
      "Epoch [5/500] Train Loss: 39.6401, Val Loss: 42.8817\n",
      "Epoch [6/500] Train Loss: 39.5232, Val Loss: 42.6709\n",
      "Epoch [7/500] Train Loss: 39.2455, Val Loss: 42.2032\n",
      "Epoch [8/500] Train Loss: 38.6765, Val Loss: 41.2561\n",
      "Epoch [9/500] Train Loss: 37.4784, Val Loss: 39.1976\n",
      "Epoch [10/500] Train Loss: 34.8450, Val Loss: 34.2891\n",
      "Epoch [11/500] Train Loss: 28.8723, Val Loss: 25.0670\n",
      "Epoch [12/500] Train Loss: 22.1063, Val Loss: 20.4800\n",
      "Epoch [13/500] Train Loss: 20.8132, Val Loss: 19.9983\n",
      "Epoch [14/500] Train Loss: 20.1669, Val Loss: 19.9339\n",
      "Epoch [15/500] Train Loss: 19.8950, Val Loss: 19.6536\n",
      "Epoch [16/500] Train Loss: 19.6816, Val Loss: 19.5174\n",
      "Epoch [17/500] Train Loss: 19.5043, Val Loss: 19.3966\n",
      "Epoch [18/500] Train Loss: 19.3380, Val Loss: 19.2447\n",
      "Epoch [19/500] Train Loss: 19.1728, Val Loss: 19.0822\n",
      "Epoch [20/500] Train Loss: 19.0144, Val Loss: 18.9527\n",
      "Epoch [21/500] Train Loss: 18.8652, Val Loss: 18.8230\n",
      "Epoch [22/500] Train Loss: 18.7169, Val Loss: 18.7113\n",
      "Epoch [23/500] Train Loss: 18.5853, Val Loss: 18.5971\n",
      "Epoch [24/500] Train Loss: 18.4712, Val Loss: 18.5175\n",
      "Epoch [25/500] Train Loss: 18.3762, Val Loss: 18.4488\n",
      "Epoch [26/500] Train Loss: 18.3005, Val Loss: 18.3936\n",
      "Epoch [27/500] Train Loss: 18.2409, Val Loss: 18.3571\n",
      "Epoch [28/500] Train Loss: 18.1962, Val Loss: 18.3290\n",
      "Epoch [29/500] Train Loss: 18.1627, Val Loss: 18.2999\n",
      "Epoch [30/500] Train Loss: 18.1375, Val Loss: 18.2776\n",
      "Epoch [31/500] Train Loss: 18.1170, Val Loss: 18.2615\n",
      "Epoch [32/500] Train Loss: 18.0995, Val Loss: 18.2362\n",
      "Epoch [33/500] Train Loss: 18.0821, Val Loss: 18.2225\n",
      "Epoch [34/500] Train Loss: 18.0663, Val Loss: 18.2153\n",
      "Epoch [35/500] Train Loss: 18.0517, Val Loss: 18.1988\n",
      "Epoch [36/500] Train Loss: 18.0411, Val Loss: 18.1828\n",
      "Epoch [37/500] Train Loss: 18.0335, Val Loss: 18.1714\n",
      "Epoch [38/500] Train Loss: 18.0239, Val Loss: 18.1726\n",
      "Epoch [39/500] Train Loss: 18.0161, Val Loss: 18.1664\n",
      "Epoch [40/500] Train Loss: 18.0091, Val Loss: 18.1594\n",
      "Epoch [41/500] Train Loss: 18.0006, Val Loss: 18.1602\n",
      "Epoch [42/500] Train Loss: 17.9924, Val Loss: 18.1522\n",
      "Epoch [43/500] Train Loss: 17.9847, Val Loss: 18.1461\n",
      "Epoch [44/500] Train Loss: 17.9787, Val Loss: 18.1453\n",
      "Epoch [45/500] Train Loss: 17.9683, Val Loss: 18.1364\n",
      "Epoch [46/500] Train Loss: 17.9597, Val Loss: 18.1288\n",
      "Epoch [47/500] Train Loss: 17.9501, Val Loss: 18.1282\n",
      "Epoch [48/500] Train Loss: 17.9350, Val Loss: 18.1091\n",
      "Epoch [49/500] Train Loss: 17.9142, Val Loss: 18.0929\n",
      "Epoch [50/500] Train Loss: 17.8706, Val Loss: 18.0176\n",
      "Epoch [51/500] Train Loss: 17.4920, Val Loss: 16.3844\n",
      "Epoch [52/500] Train Loss: 14.7914, Val Loss: 12.9074\n",
      "Epoch [53/500] Train Loss: 11.6772, Val Loss: 9.6718\n",
      "Epoch [54/500] Train Loss: 9.1951, Val Loss: 8.9279\n",
      "Epoch [55/500] Train Loss: 8.4924, Val Loss: 8.0924\n",
      "Epoch [56/500] Train Loss: 8.2350, Val Loss: 7.9281\n",
      "Epoch [57/500] Train Loss: 8.1391, Val Loss: 7.9088\n",
      "Epoch [58/500] Train Loss: 8.0657, Val Loss: 7.8604\n",
      "Epoch [59/500] Train Loss: 8.0366, Val Loss: 7.8471\n",
      "Epoch [60/500] Train Loss: 8.0210, Val Loss: 7.8049\n",
      "Epoch [61/500] Train Loss: 8.0047, Val Loss: 7.8084\n",
      "Epoch [62/500] Train Loss: 7.9979, Val Loss: 7.8239\n",
      "Epoch [63/500] Train Loss: 7.9897, Val Loss: 7.7926\n",
      "Epoch [64/500] Train Loss: 7.9856, Val Loss: 7.7878\n",
      "Epoch [65/500] Train Loss: 7.9839, Val Loss: 7.8016\n",
      "Epoch [66/500] Train Loss: 7.9816, Val Loss: 7.7864\n",
      "Epoch [67/500] Train Loss: 7.9782, Val Loss: 7.7998\n",
      "Epoch [68/500] Train Loss: 7.9748, Val Loss: 7.8103\n",
      "Epoch [69/500] Train Loss: 7.9706, Val Loss: 7.7909\n",
      "Epoch [70/500] Train Loss: 7.9691, Val Loss: 7.7879\n",
      "Epoch [71/500] Train Loss: 7.9687, Val Loss: 7.7759\n",
      "Epoch [72/500] Train Loss: 7.9681, Val Loss: 7.7917\n",
      "Epoch [73/500] Train Loss: 7.9657, Val Loss: 7.7957\n",
      "Epoch [74/500] Train Loss: 7.9629, Val Loss: 7.7901\n",
      "Epoch [75/500] Train Loss: 7.9607, Val Loss: 7.7946\n",
      "Epoch [76/500] Train Loss: 7.9624, Val Loss: 7.8061\n",
      "Epoch [77/500] Train Loss: 7.9575, Val Loss: 7.7789\n",
      "Epoch [78/500] Train Loss: 7.9577, Val Loss: 7.7859\n",
      "Epoch [79/500] Train Loss: 7.9575, Val Loss: 7.7732\n",
      "Epoch [80/500] Train Loss: 7.9531, Val Loss: 7.7926\n",
      "Epoch [81/500] Train Loss: 7.9549, Val Loss: 7.8081\n",
      "Epoch [82/500] Train Loss: 7.9509, Val Loss: 7.7734\n",
      "Epoch [83/500] Train Loss: 7.9491, Val Loss: 7.7866\n",
      "Epoch [84/500] Train Loss: 7.9475, Val Loss: 7.7780\n",
      "Epoch [85/500] Train Loss: 7.9468, Val Loss: 7.7799\n",
      "Epoch [86/500] Train Loss: 7.9484, Val Loss: 7.7745\n",
      "Epoch [87/500] Train Loss: 7.9504, Val Loss: 7.7903\n",
      "Epoch [88/500] Train Loss: 7.9435, Val Loss: 7.7695\n",
      "Epoch [89/500] Train Loss: 7.9406, Val Loss: 7.7642\n",
      "Epoch [90/500] Train Loss: 7.9391, Val Loss: 7.7756\n",
      "Epoch [91/500] Train Loss: 7.9344, Val Loss: 7.7796\n",
      "Epoch [92/500] Train Loss: 7.9309, Val Loss: 7.7772\n",
      "Epoch [93/500] Train Loss: 7.9297, Val Loss: 7.7604\n",
      "Epoch [94/500] Train Loss: 7.9270, Val Loss: 7.7932\n",
      "Epoch [95/500] Train Loss: 7.9235, Val Loss: 7.7524\n",
      "Epoch [96/500] Train Loss: 7.9205, Val Loss: 7.7590\n",
      "Epoch [97/500] Train Loss: 7.9135, Val Loss: 7.7511\n",
      "Epoch [98/500] Train Loss: 7.9082, Val Loss: 7.7391\n",
      "Epoch [99/500] Train Loss: 7.9121, Val Loss: 7.7533\n",
      "Epoch [100/500] Train Loss: 7.8974, Val Loss: 7.7300\n",
      "Epoch [101/500] Train Loss: 7.8874, Val Loss: 7.7246\n",
      "Epoch [102/500] Train Loss: 7.8804, Val Loss: 7.7292\n",
      "Epoch [103/500] Train Loss: 7.8730, Val Loss: 7.7012\n",
      "Epoch [104/500] Train Loss: 7.8579, Val Loss: 7.7158\n",
      "Epoch [105/500] Train Loss: 7.8421, Val Loss: 7.6719\n",
      "Epoch [106/500] Train Loss: 7.8221, Val Loss: 7.6829\n",
      "Epoch [107/500] Train Loss: 7.8019, Val Loss: 7.6664\n",
      "Epoch [108/500] Train Loss: 7.7756, Val Loss: 7.6034\n",
      "Epoch [109/500] Train Loss: 7.7442, Val Loss: 7.5933\n",
      "Epoch [110/500] Train Loss: 7.6996, Val Loss: 7.5417\n",
      "Epoch [111/500] Train Loss: 7.6385, Val Loss: 7.4816\n",
      "Epoch [112/500] Train Loss: 7.5702, Val Loss: 7.4156\n",
      "Epoch [113/500] Train Loss: 7.4720, Val Loss: 7.3154\n",
      "Epoch [114/500] Train Loss: 7.3388, Val Loss: 7.1907\n",
      "Epoch [115/500] Train Loss: 7.1675, Val Loss: 7.0151\n",
      "Epoch [116/500] Train Loss: 6.9546, Val Loss: 6.7996\n",
      "Epoch [117/500] Train Loss: 6.6780, Val Loss: 6.5429\n",
      "Epoch [118/500] Train Loss: 6.3520, Val Loss: 6.2150\n",
      "Epoch [119/500] Train Loss: 5.9528, Val Loss: 5.7922\n",
      "Epoch [120/500] Train Loss: 5.3895, Val Loss: 5.2042\n",
      "Epoch [121/500] Train Loss: 4.8141, Val Loss: 4.7992\n",
      "Epoch [122/500] Train Loss: 4.4514, Val Loss: 4.5704\n",
      "Epoch [123/500] Train Loss: 4.2775, Val Loss: 4.4744\n",
      "Epoch [124/500] Train Loss: 4.1906, Val Loss: 4.4286\n",
      "Epoch [125/500] Train Loss: 4.1374, Val Loss: 4.3742\n",
      "Epoch [126/500] Train Loss: 4.1058, Val Loss: 4.3297\n",
      "Epoch [127/500] Train Loss: 4.0793, Val Loss: 4.3433\n",
      "Epoch [128/500] Train Loss: 4.0663, Val Loss: 4.3469\n",
      "Epoch [129/500] Train Loss: 4.0544, Val Loss: 4.3055\n",
      "Epoch [130/500] Train Loss: 4.0401, Val Loss: 4.3058\n",
      "Epoch [131/500] Train Loss: 4.0342, Val Loss: 4.3044\n",
      "Epoch [132/500] Train Loss: 4.0248, Val Loss: 4.2717\n",
      "Epoch [133/500] Train Loss: 4.0195, Val Loss: 4.2683\n",
      "Epoch [134/500] Train Loss: 4.0161, Val Loss: 4.2858\n",
      "Epoch [135/500] Train Loss: 4.0090, Val Loss: 4.2665\n",
      "Epoch [136/500] Train Loss: 4.0067, Val Loss: 4.2631\n",
      "Epoch [137/500] Train Loss: 4.0048, Val Loss: 4.2364\n",
      "Epoch [138/500] Train Loss: 4.0024, Val Loss: 4.2627\n",
      "Epoch [139/500] Train Loss: 4.0003, Val Loss: 4.2176\n",
      "Epoch [140/500] Train Loss: 3.9891, Val Loss: 4.2544\n",
      "Epoch [141/500] Train Loss: 3.9857, Val Loss: 4.2193\n",
      "Epoch [142/500] Train Loss: 3.9818, Val Loss: 4.2178\n",
      "Epoch [143/500] Train Loss: 3.9803, Val Loss: 4.1972\n",
      "Epoch [144/500] Train Loss: 3.9768, Val Loss: 4.1955\n",
      "Epoch [145/500] Train Loss: 3.9721, Val Loss: 4.1972\n",
      "Epoch [146/500] Train Loss: 3.9681, Val Loss: 4.1906\n",
      "Epoch [147/500] Train Loss: 3.9652, Val Loss: 4.1963\n",
      "Epoch [148/500] Train Loss: 3.9642, Val Loss: 4.1958\n",
      "Epoch [149/500] Train Loss: 3.9601, Val Loss: 4.1860\n",
      "Epoch [150/500] Train Loss: 3.9579, Val Loss: 4.1743\n",
      "Epoch [151/500] Train Loss: 3.9599, Val Loss: 4.1908\n",
      "Epoch [152/500] Train Loss: 3.9547, Val Loss: 4.1766\n",
      "Epoch [153/500] Train Loss: 3.9531, Val Loss: 4.1678\n",
      "Epoch [154/500] Train Loss: 3.9450, Val Loss: 4.1881\n",
      "Epoch [155/500] Train Loss: 3.9487, Val Loss: 4.1786\n",
      "Epoch [156/500] Train Loss: 3.9452, Val Loss: 4.1729\n",
      "Epoch [157/500] Train Loss: 3.9407, Val Loss: 4.1862\n",
      "Epoch [158/500] Train Loss: 3.9399, Val Loss: 4.1656\n",
      "Epoch [159/500] Train Loss: 3.9364, Val Loss: 4.1728\n",
      "Epoch [160/500] Train Loss: 3.9336, Val Loss: 4.1676\n",
      "Epoch [161/500] Train Loss: 3.9345, Val Loss: 4.1655\n",
      "Epoch [162/500] Train Loss: 3.9324, Val Loss: 4.1619\n",
      "Epoch [163/500] Train Loss: 3.9284, Val Loss: 4.1644\n",
      "Epoch [164/500] Train Loss: 3.9288, Val Loss: 4.1719\n",
      "Epoch [165/500] Train Loss: 3.9261, Val Loss: 4.1621\n",
      "Epoch [166/500] Train Loss: 3.9229, Val Loss: 4.1586\n",
      "Epoch [167/500] Train Loss: 3.9218, Val Loss: 4.1623\n",
      "Epoch [168/500] Train Loss: 3.9197, Val Loss: 4.1589\n",
      "Epoch [169/500] Train Loss: 3.9196, Val Loss: 4.1576\n",
      "Epoch [170/500] Train Loss: 3.9180, Val Loss: 4.1656\n",
      "Epoch [171/500] Train Loss: 3.9163, Val Loss: 4.1549\n",
      "Epoch [172/500] Train Loss: 3.9135, Val Loss: 4.1539\n",
      "Epoch [173/500] Train Loss: 3.9130, Val Loss: 4.1537\n",
      "Epoch [174/500] Train Loss: 3.9101, Val Loss: 4.1511\n",
      "Epoch [175/500] Train Loss: 3.9099, Val Loss: 4.1472\n",
      "Epoch [176/500] Train Loss: 3.9107, Val Loss: 4.1514\n",
      "Epoch [177/500] Train Loss: 3.9078, Val Loss: 4.1503\n",
      "Epoch [178/500] Train Loss: 3.9072, Val Loss: 4.1476\n",
      "Epoch [179/500] Train Loss: 3.9041, Val Loss: 4.1447\n",
      "Epoch [180/500] Train Loss: 3.9028, Val Loss: 4.1493\n",
      "Epoch [181/500] Train Loss: 3.9042, Val Loss: 4.1417\n",
      "Epoch [182/500] Train Loss: 3.8986, Val Loss: 4.1382\n",
      "Epoch [183/500] Train Loss: 3.8978, Val Loss: 4.1369\n",
      "Epoch [184/500] Train Loss: 3.8969, Val Loss: 4.1387\n",
      "Epoch [185/500] Train Loss: 3.8952, Val Loss: 4.1389\n",
      "Epoch [186/500] Train Loss: 3.8932, Val Loss: 4.1321\n",
      "Epoch [187/500] Train Loss: 3.8933, Val Loss: 4.1368\n",
      "Epoch [188/500] Train Loss: 3.8903, Val Loss: 4.1312\n",
      "Epoch [189/500] Train Loss: 3.8891, Val Loss: 4.1279\n",
      "Epoch [190/500] Train Loss: 3.8905, Val Loss: 4.1307\n",
      "Epoch [191/500] Train Loss: 3.8866, Val Loss: 4.1303\n",
      "Epoch [192/500] Train Loss: 3.8845, Val Loss: 4.1273\n",
      "Epoch [193/500] Train Loss: 3.8825, Val Loss: 4.1264\n",
      "Epoch [194/500] Train Loss: 3.8807, Val Loss: 4.1246\n",
      "Epoch [195/500] Train Loss: 3.8782, Val Loss: 4.1242\n",
      "Epoch [196/500] Train Loss: 3.8763, Val Loss: 4.1212\n",
      "Epoch [197/500] Train Loss: 3.8740, Val Loss: 4.1266\n",
      "Epoch [198/500] Train Loss: 3.8720, Val Loss: 4.1200\n",
      "Epoch [199/500] Train Loss: 3.8711, Val Loss: 4.1175\n",
      "Epoch [200/500] Train Loss: 3.8669, Val Loss: 4.1170\n",
      "Epoch [201/500] Train Loss: 3.8669, Val Loss: 4.1200\n",
      "Epoch [202/500] Train Loss: 3.8637, Val Loss: 4.1169\n",
      "Epoch [203/500] Train Loss: 3.8611, Val Loss: 4.1071\n",
      "Epoch [204/500] Train Loss: 3.8590, Val Loss: 4.1133\n",
      "Epoch [205/500] Train Loss: 3.8550, Val Loss: 4.1034\n",
      "Epoch [206/500] Train Loss: 3.8543, Val Loss: 4.1058\n",
      "Epoch [207/500] Train Loss: 3.8503, Val Loss: 4.1181\n",
      "Epoch [208/500] Train Loss: 3.8554, Val Loss: 4.1042\n",
      "Epoch [209/500] Train Loss: 3.8449, Val Loss: 4.0973\n",
      "Epoch [210/500] Train Loss: 3.8420, Val Loss: 4.1043\n",
      "Epoch [211/500] Train Loss: 3.8417, Val Loss: 4.0928\n",
      "Epoch [212/500] Train Loss: 3.8385, Val Loss: 4.0894\n",
      "Epoch [213/500] Train Loss: 3.8359, Val Loss: 4.0906\n",
      "Epoch [214/500] Train Loss: 3.8303, Val Loss: 4.0899\n",
      "Epoch [215/500] Train Loss: 3.8246, Val Loss: 4.0827\n",
      "Epoch [216/500] Train Loss: 3.8185, Val Loss: 4.0728\n",
      "Epoch [217/500] Train Loss: 3.8126, Val Loss: 4.0727\n",
      "Epoch [218/500] Train Loss: 3.8097, Val Loss: 4.0707\n",
      "Epoch [219/500] Train Loss: 3.8056, Val Loss: 4.0692\n",
      "Epoch [220/500] Train Loss: 3.8037, Val Loss: 4.0672\n",
      "Epoch [221/500] Train Loss: 3.7975, Val Loss: 4.0608\n",
      "Epoch [222/500] Train Loss: 3.7917, Val Loss: 4.0561\n",
      "Epoch [223/500] Train Loss: 3.7880, Val Loss: 4.0526\n",
      "Epoch [224/500] Train Loss: 3.7854, Val Loss: 4.0482\n",
      "Epoch [225/500] Train Loss: 3.7776, Val Loss: 4.0369\n",
      "Epoch [226/500] Train Loss: 3.7739, Val Loss: 4.0237\n",
      "Epoch [227/500] Train Loss: 3.7662, Val Loss: 4.0222\n",
      "Epoch [228/500] Train Loss: 3.7614, Val Loss: 4.0219\n",
      "Epoch [229/500] Train Loss: 3.7513, Val Loss: 4.0100\n",
      "Epoch [230/500] Train Loss: 3.7471, Val Loss: 4.0093\n",
      "Epoch [231/500] Train Loss: 3.7360, Val Loss: 3.9891\n",
      "Epoch [232/500] Train Loss: 3.7259, Val Loss: 3.9727\n",
      "Epoch [233/500] Train Loss: 3.7183, Val Loss: 3.9629\n",
      "Epoch [234/500] Train Loss: 3.7078, Val Loss: 3.9617\n",
      "Epoch [235/500] Train Loss: 3.6974, Val Loss: 3.9423\n",
      "Epoch [236/500] Train Loss: 3.6824, Val Loss: 3.9287\n",
      "Epoch [237/500] Train Loss: 3.6678, Val Loss: 3.9152\n",
      "Epoch [238/500] Train Loss: 3.6522, Val Loss: 3.8884\n",
      "Epoch [239/500] Train Loss: 3.6312, Val Loss: 3.8591\n",
      "Epoch [240/500] Train Loss: 3.6079, Val Loss: 3.8233\n",
      "Epoch [241/500] Train Loss: 3.5807, Val Loss: 3.7819\n",
      "Epoch [242/500] Train Loss: 3.5433, Val Loss: 3.7302\n",
      "Epoch [243/500] Train Loss: 3.4876, Val Loss: 3.6620\n",
      "Epoch [244/500] Train Loss: 3.4188, Val Loss: 3.5639\n",
      "Epoch [245/500] Train Loss: 3.3108, Val Loss: 3.4129\n",
      "Epoch [246/500] Train Loss: 3.1607, Val Loss: 3.2026\n",
      "Epoch [247/500] Train Loss: 2.8977, Val Loss: 2.8682\n",
      "Epoch [248/500] Train Loss: 2.3849, Val Loss: 2.2726\n",
      "Epoch [249/500] Train Loss: 1.6458, Val Loss: 1.4763\n",
      "Epoch [250/500] Train Loss: 0.9905, Val Loss: 0.8550\n",
      "Epoch [251/500] Train Loss: 0.6311, Val Loss: 0.4999\n",
      "Epoch [252/500] Train Loss: 0.4401, Val Loss: 0.3607\n",
      "Epoch [253/500] Train Loss: 0.3348, Val Loss: 0.2788\n",
      "Epoch [254/500] Train Loss: 0.2729, Val Loss: 0.2385\n",
      "Epoch [255/500] Train Loss: 0.2281, Val Loss: 0.2177\n",
      "Epoch [256/500] Train Loss: 0.1941, Val Loss: 0.1884\n",
      "Epoch [257/500] Train Loss: 0.1712, Val Loss: 0.1721\n",
      "Epoch [258/500] Train Loss: 0.1490, Val Loss: 0.1508\n",
      "Epoch [259/500] Train Loss: 0.1335, Val Loss: 0.1369\n",
      "Epoch [260/500] Train Loss: 0.1210, Val Loss: 0.1285\n",
      "Epoch [261/500] Train Loss: 0.1093, Val Loss: 0.1169\n",
      "Epoch [262/500] Train Loss: 0.1019, Val Loss: 0.1121\n",
      "Epoch [263/500] Train Loss: 0.0935, Val Loss: 0.1013\n",
      "Epoch [264/500] Train Loss: 0.0872, Val Loss: 0.0963\n",
      "Epoch [265/500] Train Loss: 0.0808, Val Loss: 0.0904\n",
      "Epoch [266/500] Train Loss: 0.0755, Val Loss: 0.0858\n",
      "Epoch [267/500] Train Loss: 0.0716, Val Loss: 0.0827\n",
      "Epoch [268/500] Train Loss: 0.0684, Val Loss: 0.0804\n",
      "Epoch [269/500] Train Loss: 0.0649, Val Loss: 0.0747\n",
      "Epoch [270/500] Train Loss: 0.0608, Val Loss: 0.0685\n",
      "Epoch [271/500] Train Loss: 0.0580, Val Loss: 0.0672\n",
      "Epoch [272/500] Train Loss: 0.0556, Val Loss: 0.0638\n",
      "Epoch [273/500] Train Loss: 0.0528, Val Loss: 0.0624\n",
      "Epoch [274/500] Train Loss: 0.0507, Val Loss: 0.0625\n",
      "Epoch [275/500] Train Loss: 0.0507, Val Loss: 0.0577\n",
      "Epoch [276/500] Train Loss: 0.0478, Val Loss: 0.0540\n",
      "Epoch [277/500] Train Loss: 0.0440, Val Loss: 0.0490\n",
      "Epoch [278/500] Train Loss: 0.0418, Val Loss: 0.0475\n",
      "Epoch [279/500] Train Loss: 0.0401, Val Loss: 0.0446\n",
      "Epoch [280/500] Train Loss: 0.0384, Val Loss: 0.0428\n",
      "Epoch [281/500] Train Loss: 0.0369, Val Loss: 0.0415\n",
      "Epoch [282/500] Train Loss: 0.0355, Val Loss: 0.0387\n",
      "Epoch [283/500] Train Loss: 0.0339, Val Loss: 0.0366\n",
      "Epoch [284/500] Train Loss: 0.0325, Val Loss: 0.0352\n",
      "Epoch [285/500] Train Loss: 0.0321, Val Loss: 0.0341\n",
      "Epoch [286/500] Train Loss: 0.0315, Val Loss: 0.0343\n",
      "Epoch [287/500] Train Loss: 0.0301, Val Loss: 0.0316\n",
      "Epoch [288/500] Train Loss: 0.0290, Val Loss: 0.0305\n",
      "Epoch [289/500] Train Loss: 0.0281, Val Loss: 0.0291\n",
      "Epoch [290/500] Train Loss: 0.0275, Val Loss: 0.0289\n",
      "Epoch [291/500] Train Loss: 0.0267, Val Loss: 0.0277\n",
      "Epoch [292/500] Train Loss: 0.0263, Val Loss: 0.0264\n",
      "Epoch [293/500] Train Loss: 0.0255, Val Loss: 0.0253\n",
      "Epoch [294/500] Train Loss: 0.0246, Val Loss: 0.0246\n",
      "Epoch [295/500] Train Loss: 0.0244, Val Loss: 0.0260\n",
      "Epoch [296/500] Train Loss: 0.0254, Val Loss: 0.0239\n",
      "Epoch [297/500] Train Loss: 0.0231, Val Loss: 0.0229\n",
      "Epoch [298/500] Train Loss: 0.0226, Val Loss: 0.0220\n",
      "Epoch [299/500] Train Loss: 0.0226, Val Loss: 0.0219\n",
      "Epoch [300/500] Train Loss: 0.0217, Val Loss: 0.0213\n",
      "Epoch [301/500] Train Loss: 0.0224, Val Loss: 0.0224\n",
      "Epoch [302/500] Train Loss: 0.0224, Val Loss: 0.0252\n",
      "Epoch [303/500] Train Loss: 0.0223, Val Loss: 0.0211\n",
      "Epoch [304/500] Train Loss: 0.0211, Val Loss: 0.0187\n",
      "Epoch [305/500] Train Loss: 0.0202, Val Loss: 0.0194\n",
      "Epoch [306/500] Train Loss: 0.0189, Val Loss: 0.0182\n",
      "Epoch [307/500] Train Loss: 0.0184, Val Loss: 0.0173\n",
      "Epoch [308/500] Train Loss: 0.0180, Val Loss: 0.0173\n",
      "Epoch [309/500] Train Loss: 0.0183, Val Loss: 0.0175\n",
      "Epoch [310/500] Train Loss: 0.0183, Val Loss: 0.0161\n",
      "Epoch [311/500] Train Loss: 0.0169, Val Loss: 0.0181\n",
      "Epoch [312/500] Train Loss: 0.0174, Val Loss: 0.0158\n",
      "Epoch [313/500] Train Loss: 0.0165, Val Loss: 0.0151\n",
      "Epoch [314/500] Train Loss: 0.0176, Val Loss: 0.0155\n",
      "Epoch [315/500] Train Loss: 0.0163, Val Loss: 0.0145\n",
      "Epoch [316/500] Train Loss: 0.0155, Val Loss: 0.0139\n",
      "Epoch [317/500] Train Loss: 0.0154, Val Loss: 0.0146\n",
      "Epoch [318/500] Train Loss: 0.0159, Val Loss: 0.0141\n",
      "Epoch [319/500] Train Loss: 0.0149, Val Loss: 0.0153\n",
      "Epoch [320/500] Train Loss: 0.0148, Val Loss: 0.0128\n",
      "Epoch [321/500] Train Loss: 0.0144, Val Loss: 0.0130\n",
      "Epoch [322/500] Train Loss: 0.0140, Val Loss: 0.0130\n",
      "Epoch [323/500] Train Loss: 0.0143, Val Loss: 0.0120\n",
      "Epoch [324/500] Train Loss: 0.0141, Val Loss: 0.0119\n",
      "Epoch [325/500] Train Loss: 0.0140, Val Loss: 0.0116\n",
      "Epoch [326/500] Train Loss: 0.0133, Val Loss: 0.0124\n",
      "Epoch [327/500] Train Loss: 0.0135, Val Loss: 0.0117\n",
      "Epoch [328/500] Train Loss: 0.0133, Val Loss: 0.0113\n",
      "Epoch [329/500] Train Loss: 0.0129, Val Loss: 0.0111\n",
      "Epoch [330/500] Train Loss: 0.0134, Val Loss: 0.0121\n",
      "Epoch [331/500] Train Loss: 0.0127, Val Loss: 0.0133\n",
      "Epoch [332/500] Train Loss: 0.0132, Val Loss: 0.0121\n",
      "Epoch [333/500] Train Loss: 0.0129, Val Loss: 0.0105\n",
      "Epoch [334/500] Train Loss: 0.0118, Val Loss: 0.0102\n",
      "Epoch [335/500] Train Loss: 0.0117, Val Loss: 0.0100\n",
      "Epoch [336/500] Train Loss: 0.0118, Val Loss: 0.0103\n",
      "Epoch [337/500] Train Loss: 0.0117, Val Loss: 0.0104\n",
      "Epoch [338/500] Train Loss: 0.0112, Val Loss: 0.0097\n",
      "Epoch [339/500] Train Loss: 0.0115, Val Loss: 0.0138\n",
      "Epoch [340/500] Train Loss: 0.0124, Val Loss: 0.0092\n",
      "Epoch [341/500] Train Loss: 0.0121, Val Loss: 0.0097\n",
      "Epoch [342/500] Train Loss: 0.0113, Val Loss: 0.0092\n",
      "Epoch [343/500] Train Loss: 0.0105, Val Loss: 0.0088\n",
      "Epoch [344/500] Train Loss: 0.0104, Val Loss: 0.0087\n",
      "Epoch [345/500] Train Loss: 0.0104, Val Loss: 0.0096\n",
      "Epoch [346/500] Train Loss: 0.0105, Val Loss: 0.0089\n",
      "Epoch [347/500] Train Loss: 0.0102, Val Loss: 0.0086\n",
      "Epoch [348/500] Train Loss: 0.0099, Val Loss: 0.0084\n",
      "Epoch [349/500] Train Loss: 0.0098, Val Loss: 0.0088\n",
      "Epoch [350/500] Train Loss: 0.0102, Val Loss: 0.0095\n",
      "Epoch [351/500] Train Loss: 0.0097, Val Loss: 0.0082\n",
      "Epoch [352/500] Train Loss: 0.0095, Val Loss: 0.0092\n",
      "Epoch [353/500] Train Loss: 0.0100, Val Loss: 0.0079\n",
      "Epoch [354/500] Train Loss: 0.0095, Val Loss: 0.0076\n",
      "Epoch [355/500] Train Loss: 0.0091, Val Loss: 0.0077\n",
      "Epoch [356/500] Train Loss: 0.0090, Val Loss: 0.0074\n",
      "Epoch [357/500] Train Loss: 0.0089, Val Loss: 0.0079\n",
      "Epoch [358/500] Train Loss: 0.0089, Val Loss: 0.0072\n",
      "Epoch [359/500] Train Loss: 0.0087, Val Loss: 0.0079\n",
      "Epoch [360/500] Train Loss: 0.0093, Val Loss: 0.0073\n",
      "Epoch [361/500] Train Loss: 0.0087, Val Loss: 0.0075\n",
      "Epoch [362/500] Train Loss: 0.0088, Val Loss: 0.0079\n",
      "Epoch [363/500] Train Loss: 0.0085, Val Loss: 0.0072\n",
      "Epoch [364/500] Train Loss: 0.0082, Val Loss: 0.0067\n",
      "Epoch [365/500] Train Loss: 0.0083, Val Loss: 0.0072\n",
      "Epoch [366/500] Train Loss: 0.0081, Val Loss: 0.0071\n",
      "Epoch [367/500] Train Loss: 0.0081, Val Loss: 0.0082\n",
      "Epoch [368/500] Train Loss: 0.0087, Val Loss: 0.0084\n",
      "Epoch [369/500] Train Loss: 0.0082, Val Loss: 0.0077\n",
      "Epoch [370/500] Train Loss: 0.0081, Val Loss: 0.0070\n",
      "Epoch [371/500] Train Loss: 0.0078, Val Loss: 0.0062\n",
      "Epoch [372/500] Train Loss: 0.0077, Val Loss: 0.0078\n",
      "Epoch [373/500] Train Loss: 0.0076, Val Loss: 0.0061\n",
      "Epoch [374/500] Train Loss: 0.0081, Val Loss: 0.0099\n",
      "Epoch [375/500] Train Loss: 0.0097, Val Loss: 0.0070\n",
      "Epoch [376/500] Train Loss: 0.0081, Val Loss: 0.0073\n",
      "Epoch [377/500] Train Loss: 0.0076, Val Loss: 0.0057\n",
      "Epoch [378/500] Train Loss: 0.0073, Val Loss: 0.0058\n",
      "Epoch [379/500] Train Loss: 0.0076, Val Loss: 0.0057\n",
      "Epoch [380/500] Train Loss: 0.0080, Val Loss: 0.0059\n",
      "Epoch [381/500] Train Loss: 0.0074, Val Loss: 0.0080\n",
      "Epoch [382/500] Train Loss: 0.0076, Val Loss: 0.0056\n",
      "Epoch [383/500] Train Loss: 0.0076, Val Loss: 0.0056\n",
      "Epoch [384/500] Train Loss: 0.0069, Val Loss: 0.0060\n",
      "Epoch [385/500] Train Loss: 0.0074, Val Loss: 0.0064\n",
      "Epoch [386/500] Train Loss: 0.0067, Val Loss: 0.0054\n",
      "Epoch [387/500] Train Loss: 0.0066, Val Loss: 0.0059\n",
      "Epoch [388/500] Train Loss: 0.0067, Val Loss: 0.0064\n",
      "Epoch [389/500] Train Loss: 0.0069, Val Loss: 0.0058\n",
      "Epoch [390/500] Train Loss: 0.0069, Val Loss: 0.0062\n",
      "Epoch [391/500] Train Loss: 0.0069, Val Loss: 0.0057\n",
      "Epoch [392/500] Train Loss: 0.0065, Val Loss: 0.0057\n",
      "Epoch [393/500] Train Loss: 0.0063, Val Loss: 0.0051\n",
      "Epoch [394/500] Train Loss: 0.0062, Val Loss: 0.0053\n",
      "Epoch [395/500] Train Loss: 0.0064, Val Loss: 0.0052\n",
      "Epoch [396/500] Train Loss: 0.0064, Val Loss: 0.0054\n",
      "Epoch [397/500] Train Loss: 0.0061, Val Loss: 0.0053\n",
      "Epoch [398/500] Train Loss: 0.0061, Val Loss: 0.0054\n",
      "Epoch [399/500] Train Loss: 0.0060, Val Loss: 0.0051\n",
      "Epoch [400/500] Train Loss: 0.0062, Val Loss: 0.0047\n",
      "Epoch [401/500] Train Loss: 0.0057, Val Loss: 0.0048\n",
      "Epoch [402/500] Train Loss: 0.0062, Val Loss: 0.0046\n",
      "Epoch [403/500] Train Loss: 0.0059, Val Loss: 0.0048\n",
      "Epoch [404/500] Train Loss: 0.0058, Val Loss: 0.0050\n",
      "Epoch [405/500] Train Loss: 0.0058, Val Loss: 0.0048\n",
      "Epoch [406/500] Train Loss: 0.0056, Val Loss: 0.0042\n",
      "Epoch [407/500] Train Loss: 0.0055, Val Loss: 0.0044\n",
      "Epoch [408/500] Train Loss: 0.0059, Val Loss: 0.0069\n",
      "Epoch [409/500] Train Loss: 0.0058, Val Loss: 0.0043\n",
      "Epoch [410/500] Train Loss: 0.0056, Val Loss: 0.0052\n",
      "Epoch [411/500] Train Loss: 0.0062, Val Loss: 0.0044\n",
      "Epoch [412/500] Train Loss: 0.0062, Val Loss: 0.0048\n",
      "Epoch [413/500] Train Loss: 0.0056, Val Loss: 0.0048\n",
      "Epoch [414/500] Train Loss: 0.0060, Val Loss: 0.0047\n",
      "Epoch [415/500] Train Loss: 0.0053, Val Loss: 0.0039\n",
      "Epoch [416/500] Train Loss: 0.0051, Val Loss: 0.0041\n",
      "Epoch [417/500] Train Loss: 0.0054, Val Loss: 0.0039\n",
      "Epoch [418/500] Train Loss: 0.0058, Val Loss: 0.0038\n",
      "Epoch [419/500] Train Loss: 0.0051, Val Loss: 0.0042\n",
      "Epoch [420/500] Train Loss: 0.0058, Val Loss: 0.0042\n",
      "Epoch [421/500] Train Loss: 0.0057, Val Loss: 0.0041\n",
      "Epoch [422/500] Train Loss: 0.0049, Val Loss: 0.0041\n",
      "Epoch [423/500] Train Loss: 0.0051, Val Loss: 0.0041\n",
      "Epoch [424/500] Train Loss: 0.0053, Val Loss: 0.0068\n",
      "Epoch [425/500] Train Loss: 0.0061, Val Loss: 0.0073\n",
      "Epoch [426/500] Train Loss: 0.0067, Val Loss: 0.0039\n",
      "Epoch [427/500] Train Loss: 0.0052, Val Loss: 0.0043\n",
      "Epoch [428/500] Train Loss: 0.0049, Val Loss: 0.0047\n",
      "Epoch [429/500] Train Loss: 0.0053, Val Loss: 0.0046\n",
      "Epoch [430/500] Train Loss: 0.0051, Val Loss: 0.0038\n",
      "Epoch [431/500] Train Loss: 0.0046, Val Loss: 0.0034\n",
      "Epoch [432/500] Train Loss: 0.0045, Val Loss: 0.0034\n",
      "Epoch [433/500] Train Loss: 0.0046, Val Loss: 0.0039\n",
      "Epoch [434/500] Train Loss: 0.0048, Val Loss: 0.0042\n",
      "Epoch [435/500] Train Loss: 0.0046, Val Loss: 0.0033\n",
      "Epoch [436/500] Train Loss: 0.0052, Val Loss: 0.0043\n",
      "Epoch [437/500] Train Loss: 0.0052, Val Loss: 0.0043\n",
      "Epoch [438/500] Train Loss: 0.0051, Val Loss: 0.0036\n",
      "Epoch [439/500] Train Loss: 0.0043, Val Loss: 0.0032\n",
      "Epoch [440/500] Train Loss: 0.0042, Val Loss: 0.0036\n",
      "Epoch [441/500] Train Loss: 0.0044, Val Loss: 0.0034\n",
      "Epoch [442/500] Train Loss: 0.0047, Val Loss: 0.0046\n",
      "Epoch [443/500] Train Loss: 0.0050, Val Loss: 0.0037\n",
      "Epoch [444/500] Train Loss: 0.0047, Val Loss: 0.0031\n",
      "Epoch [445/500] Train Loss: 0.0040, Val Loss: 0.0031\n",
      "Epoch [446/500] Train Loss: 0.0042, Val Loss: 0.0031\n",
      "Epoch [447/500] Train Loss: 0.0049, Val Loss: 0.0030\n",
      "Epoch [448/500] Train Loss: 0.0039, Val Loss: 0.0030\n",
      "Epoch [449/500] Train Loss: 0.0039, Val Loss: 0.0029\n",
      "Epoch [450/500] Train Loss: 0.0040, Val Loss: 0.0030\n",
      "Epoch [451/500] Train Loss: 0.0038, Val Loss: 0.0029\n",
      "Epoch [452/500] Train Loss: 0.0039, Val Loss: 0.0038\n",
      "Epoch [453/500] Train Loss: 0.0041, Val Loss: 0.0035\n",
      "Epoch [454/500] Train Loss: 0.0040, Val Loss: 0.0031\n",
      "Epoch [455/500] Train Loss: 0.0038, Val Loss: 0.0028\n",
      "Epoch [456/500] Train Loss: 0.0038, Val Loss: 0.0028\n",
      "Epoch [457/500] Train Loss: 0.0038, Val Loss: 0.0029\n",
      "Epoch [458/500] Train Loss: 0.0041, Val Loss: 0.0026\n",
      "Epoch [459/500] Train Loss: 0.0037, Val Loss: 0.0029\n",
      "Epoch [460/500] Train Loss: 0.0050, Val Loss: 0.0027\n",
      "Epoch [461/500] Train Loss: 0.0043, Val Loss: 0.0043\n",
      "Epoch [462/500] Train Loss: 0.0040, Val Loss: 0.0029\n",
      "Epoch [463/500] Train Loss: 0.0055, Val Loss: 0.0056\n",
      "Epoch [464/500] Train Loss: 0.0076, Val Loss: 0.0077\n",
      "Epoch [465/500] Train Loss: 0.0045, Val Loss: 0.0026\n",
      "Epoch [466/500] Train Loss: 0.0044, Val Loss: 0.0031\n",
      "Epoch [467/500] Train Loss: 0.0044, Val Loss: 0.0028\n",
      "Epoch [468/500] Train Loss: 0.0038, Val Loss: 0.0027\n",
      "Epoch [469/500] Train Loss: 0.0035, Val Loss: 0.0029\n",
      "Epoch [470/500] Train Loss: 0.0034, Val Loss: 0.0027\n",
      "Epoch [471/500] Train Loss: 0.0036, Val Loss: 0.0046\n",
      "Epoch [472/500] Train Loss: 0.0043, Val Loss: 0.0029\n",
      "Epoch [473/500] Train Loss: 0.0035, Val Loss: 0.0026\n",
      "Epoch [474/500] Train Loss: 0.0033, Val Loss: 0.0023\n",
      "Epoch [475/500] Train Loss: 0.0032, Val Loss: 0.0024\n",
      "Epoch [476/500] Train Loss: 0.0033, Val Loss: 0.0023\n",
      "Epoch [477/500] Train Loss: 0.0033, Val Loss: 0.0023\n",
      "Epoch [478/500] Train Loss: 0.0033, Val Loss: 0.0027\n",
      "Epoch [479/500] Train Loss: 0.0033, Val Loss: 0.0026\n",
      "Epoch [480/500] Train Loss: 0.0032, Val Loss: 0.0023\n",
      "Epoch [481/500] Train Loss: 0.0033, Val Loss: 0.0022\n",
      "Epoch [482/500] Train Loss: 0.0039, Val Loss: 0.0031\n",
      "Epoch [483/500] Train Loss: 0.0035, Val Loss: 0.0032\n",
      "Epoch [484/500] Train Loss: 0.0038, Val Loss: 0.0028\n",
      "Epoch [485/500] Train Loss: 0.0056, Val Loss: 0.0041\n",
      "Epoch [486/500] Train Loss: 0.0072, Val Loss: 0.0057\n",
      "Epoch [487/500] Train Loss: 0.0041, Val Loss: 0.0027\n",
      "Epoch [488/500] Train Loss: 0.0032, Val Loss: 0.0032\n",
      "Epoch [489/500] Train Loss: 0.0036, Val Loss: 0.0026\n",
      "Epoch [490/500] Train Loss: 0.0032, Val Loss: 0.0024\n",
      "Epoch [491/500] Train Loss: 0.0033, Val Loss: 0.0022\n",
      "Epoch [492/500] Train Loss: 0.0030, Val Loss: 0.0020\n",
      "Epoch [493/500] Train Loss: 0.0029, Val Loss: 0.0024\n",
      "Epoch [494/500] Train Loss: 0.0029, Val Loss: 0.0019\n",
      "Epoch [495/500] Train Loss: 0.0029, Val Loss: 0.0024\n",
      "Epoch [496/500] Train Loss: 0.0029, Val Loss: 0.0027\n",
      "Epoch [497/500] Train Loss: 0.0030, Val Loss: 0.0021\n",
      "Epoch [498/500] Train Loss: 0.0029, Val Loss: 0.0021\n",
      "Epoch [499/500] Train Loss: 0.0034, Val Loss: 0.0019\n",
      "Epoch [500/500] Train Loss: 0.0029, Val Loss: 0.0023\n",
      "\n",
      "=== Fold 12 ===\n",
      "Epoch [1/500] Train Loss: 40.9581, Val Loss: 39.4579\n",
      "Epoch [2/500] Train Loss: 40.9353, Val Loss: 39.4372\n",
      "Epoch [3/500] Train Loss: 40.9144, Val Loss: 39.4168\n",
      "Epoch [4/500] Train Loss: 40.8940, Val Loss: 39.3969\n",
      "Epoch [5/500] Train Loss: 40.8716, Val Loss: 39.3718\n",
      "Epoch [6/500] Train Loss: 40.8435, Val Loss: 39.3423\n",
      "Epoch [7/500] Train Loss: 40.8090, Val Loss: 39.3011\n",
      "Epoch [8/500] Train Loss: 40.7556, Val Loss: 39.2350\n",
      "Epoch [9/500] Train Loss: 40.6735, Val Loss: 39.1089\n",
      "Epoch [10/500] Train Loss: 40.4568, Val Loss: 38.7749\n",
      "Epoch [11/500] Train Loss: 39.9570, Val Loss: 38.0585\n",
      "Epoch [12/500] Train Loss: 38.8641, Val Loss: 36.4977\n",
      "Epoch [13/500] Train Loss: 36.4223, Val Loss: 32.8105\n",
      "Epoch [14/500] Train Loss: 31.1732, Val Loss: 26.6168\n",
      "Epoch [15/500] Train Loss: 25.2988, Val Loss: 23.1176\n",
      "Epoch [16/500] Train Loss: 23.1106, Val Loss: 21.6635\n",
      "Epoch [17/500] Train Loss: 21.6553, Val Loss: 20.5887\n",
      "Epoch [18/500] Train Loss: 20.7196, Val Loss: 19.9232\n",
      "Epoch [19/500] Train Loss: 20.1945, Val Loss: 19.5461\n",
      "Epoch [20/500] Train Loss: 19.8680, Val Loss: 19.2986\n",
      "Epoch [21/500] Train Loss: 19.6535, Val Loss: 19.1282\n",
      "Epoch [22/500] Train Loss: 19.5065, Val Loss: 19.0068\n",
      "Epoch [23/500] Train Loss: 19.3980, Val Loss: 18.9112\n",
      "Epoch [24/500] Train Loss: 19.3070, Val Loss: 18.8336\n",
      "Epoch [25/500] Train Loss: 19.2340, Val Loss: 18.7670\n",
      "Epoch [26/500] Train Loss: 19.1652, Val Loss: 18.7049\n",
      "Epoch [27/500] Train Loss: 19.1037, Val Loss: 18.6496\n",
      "Epoch [28/500] Train Loss: 19.0481, Val Loss: 18.5958\n",
      "Epoch [29/500] Train Loss: 18.9943, Val Loss: 18.5444\n",
      "Epoch [30/500] Train Loss: 18.9423, Val Loss: 18.4967\n",
      "Epoch [31/500] Train Loss: 18.8948, Val Loss: 18.4504\n",
      "Epoch [32/500] Train Loss: 18.8486, Val Loss: 18.4076\n",
      "Epoch [33/500] Train Loss: 18.8047, Val Loss: 18.3659\n",
      "Epoch [34/500] Train Loss: 18.7630, Val Loss: 18.3245\n",
      "Epoch [35/500] Train Loss: 18.7192, Val Loss: 18.2840\n",
      "Epoch [36/500] Train Loss: 18.6787, Val Loss: 18.2459\n",
      "Epoch [37/500] Train Loss: 18.6380, Val Loss: 18.2067\n",
      "Epoch [38/500] Train Loss: 18.5991, Val Loss: 18.1688\n",
      "Epoch [39/500] Train Loss: 18.5620, Val Loss: 18.1332\n",
      "Epoch [40/500] Train Loss: 18.5253, Val Loss: 18.1003\n",
      "Epoch [41/500] Train Loss: 18.4906, Val Loss: 18.0678\n",
      "Epoch [42/500] Train Loss: 18.4569, Val Loss: 18.0378\n",
      "Epoch [43/500] Train Loss: 18.4256, Val Loss: 18.0066\n",
      "Epoch [44/500] Train Loss: 18.3925, Val Loss: 17.9749\n",
      "Epoch [45/500] Train Loss: 18.3627, Val Loss: 17.9467\n",
      "Epoch [46/500] Train Loss: 18.3348, Val Loss: 17.9199\n",
      "Epoch [47/500] Train Loss: 18.3087, Val Loss: 17.8962\n",
      "Epoch [48/500] Train Loss: 18.2875, Val Loss: 17.8718\n",
      "Epoch [49/500] Train Loss: 18.2639, Val Loss: 17.8507\n",
      "Epoch [50/500] Train Loss: 18.2432, Val Loss: 17.8328\n",
      "Epoch [51/500] Train Loss: 18.2260, Val Loss: 17.8154\n",
      "Epoch [52/500] Train Loss: 18.2110, Val Loss: 17.8010\n",
      "Epoch [53/500] Train Loss: 18.1955, Val Loss: 17.7873\n",
      "Epoch [54/500] Train Loss: 18.1846, Val Loss: 17.7767\n",
      "Epoch [55/500] Train Loss: 18.1749, Val Loss: 17.7680\n",
      "Epoch [56/500] Train Loss: 18.1634, Val Loss: 17.7584\n",
      "Epoch [57/500] Train Loss: 18.1558, Val Loss: 17.7517\n",
      "Epoch [58/500] Train Loss: 18.1489, Val Loss: 17.7452\n",
      "Epoch [59/500] Train Loss: 18.1428, Val Loss: 17.7398\n",
      "Epoch [60/500] Train Loss: 18.1377, Val Loss: 17.7356\n",
      "Epoch [61/500] Train Loss: 18.1335, Val Loss: 17.7319\n",
      "Epoch [62/500] Train Loss: 18.1296, Val Loss: 17.7306\n",
      "Epoch [63/500] Train Loss: 18.1270, Val Loss: 17.7278\n",
      "Epoch [64/500] Train Loss: 18.1257, Val Loss: 17.7249\n",
      "Epoch [65/500] Train Loss: 18.1217, Val Loss: 17.7219\n",
      "Epoch [66/500] Train Loss: 18.1192, Val Loss: 17.7201\n",
      "Epoch [67/500] Train Loss: 18.1180, Val Loss: 17.7197\n",
      "Epoch [68/500] Train Loss: 18.1152, Val Loss: 17.7170\n",
      "Epoch [69/500] Train Loss: 18.1128, Val Loss: 17.7178\n",
      "Epoch [70/500] Train Loss: 18.1120, Val Loss: 17.7147\n",
      "Epoch [71/500] Train Loss: 18.1101, Val Loss: 17.7132\n",
      "Epoch [72/500] Train Loss: 18.1090, Val Loss: 17.7122\n",
      "Epoch [73/500] Train Loss: 18.1075, Val Loss: 17.7111\n",
      "Epoch [74/500] Train Loss: 18.1056, Val Loss: 17.7102\n",
      "Epoch [75/500] Train Loss: 18.1039, Val Loss: 17.7097\n",
      "Epoch [76/500] Train Loss: 18.1034, Val Loss: 17.7094\n",
      "Epoch [77/500] Train Loss: 18.1013, Val Loss: 17.7083\n",
      "Epoch [78/500] Train Loss: 18.1003, Val Loss: 17.7069\n",
      "Epoch [79/500] Train Loss: 18.0990, Val Loss: 17.7061\n",
      "Epoch [80/500] Train Loss: 18.0979, Val Loss: 17.7057\n",
      "Epoch [81/500] Train Loss: 18.0968, Val Loss: 17.7042\n",
      "Epoch [82/500] Train Loss: 18.0963, Val Loss: 17.7038\n",
      "Epoch [83/500] Train Loss: 18.0948, Val Loss: 17.7039\n",
      "Epoch [84/500] Train Loss: 18.0933, Val Loss: 17.7041\n",
      "Epoch [85/500] Train Loss: 18.0928, Val Loss: 17.7030\n",
      "Epoch [86/500] Train Loss: 18.0929, Val Loss: 17.7001\n",
      "Epoch [87/500] Train Loss: 18.0892, Val Loss: 17.6988\n",
      "Epoch [88/500] Train Loss: 18.0883, Val Loss: 17.6971\n",
      "Epoch [89/500] Train Loss: 18.0868, Val Loss: 17.6962\n",
      "Epoch [90/500] Train Loss: 18.0841, Val Loss: 17.6967\n",
      "Epoch [91/500] Train Loss: 18.0853, Val Loss: 17.6946\n",
      "Epoch [92/500] Train Loss: 18.0824, Val Loss: 17.6938\n",
      "Epoch [93/500] Train Loss: 18.0810, Val Loss: 17.6925\n",
      "Epoch [94/500] Train Loss: 18.0734, Val Loss: 17.6855\n",
      "Epoch [95/500] Train Loss: 18.0743, Val Loss: 17.6813\n",
      "Epoch [96/500] Train Loss: 18.0662, Val Loss: 17.6749\n",
      "Epoch [97/500] Train Loss: 18.0560, Val Loss: 17.6680\n",
      "Epoch [98/500] Train Loss: 18.0515, Val Loss: 17.6596\n",
      "Epoch [99/500] Train Loss: 18.0423, Val Loss: 17.6474\n",
      "Epoch [100/500] Train Loss: 18.0238, Val Loss: 17.6292\n",
      "Epoch [101/500] Train Loss: 18.0059, Val Loss: 17.6020\n",
      "Epoch [102/500] Train Loss: 17.9633, Val Loss: 17.5165\n",
      "Epoch [103/500] Train Loss: 17.8657, Val Loss: 17.4056\n",
      "Epoch [104/500] Train Loss: 17.7376, Val Loss: 17.2074\n",
      "Epoch [105/500] Train Loss: 17.4856, Val Loss: 16.8468\n",
      "Epoch [106/500] Train Loss: 17.0564, Val Loss: 16.2992\n",
      "Epoch [107/500] Train Loss: 16.4457, Val Loss: 15.5317\n",
      "Epoch [108/500] Train Loss: 15.6583, Val Loss: 14.6786\n",
      "Epoch [109/500] Train Loss: 14.9016, Val Loss: 14.1588\n",
      "Epoch [110/500] Train Loss: 14.6295, Val Loss: 13.9742\n",
      "Epoch [111/500] Train Loss: 14.4763, Val Loss: 13.8813\n",
      "Epoch [112/500] Train Loss: 14.3905, Val Loss: 13.8156\n",
      "Epoch [113/500] Train Loss: 14.3379, Val Loss: 13.7786\n",
      "Epoch [114/500] Train Loss: 14.3036, Val Loss: 13.7670\n",
      "Epoch [115/500] Train Loss: 14.2792, Val Loss: 13.7291\n",
      "Epoch [116/500] Train Loss: 14.2574, Val Loss: 13.7136\n",
      "Epoch [117/500] Train Loss: 14.2410, Val Loss: 13.6998\n",
      "Epoch [118/500] Train Loss: 14.2283, Val Loss: 13.6922\n",
      "Epoch [119/500] Train Loss: 14.2174, Val Loss: 13.6909\n",
      "Epoch [120/500] Train Loss: 14.2086, Val Loss: 13.6748\n",
      "Epoch [121/500] Train Loss: 14.2012, Val Loss: 13.6734\n",
      "Epoch [122/500] Train Loss: 14.2003, Val Loss: 13.6651\n",
      "Epoch [123/500] Train Loss: 14.1919, Val Loss: 13.6671\n",
      "Epoch [124/500] Train Loss: 14.1928, Val Loss: 13.6621\n",
      "Epoch [125/500] Train Loss: 14.1868, Val Loss: 13.6545\n",
      "Epoch [126/500] Train Loss: 14.1803, Val Loss: 13.6514\n",
      "Epoch [127/500] Train Loss: 14.1761, Val Loss: 13.6486\n",
      "Epoch [128/500] Train Loss: 14.1742, Val Loss: 13.6532\n",
      "Epoch [129/500] Train Loss: 14.1727, Val Loss: 13.6431\n",
      "Epoch [130/500] Train Loss: 14.1688, Val Loss: 13.6430\n",
      "Epoch [131/500] Train Loss: 14.1666, Val Loss: 13.6443\n",
      "Epoch [132/500] Train Loss: 14.1647, Val Loss: 13.6377\n",
      "Epoch [133/500] Train Loss: 14.1630, Val Loss: 13.6426\n",
      "Epoch [134/500] Train Loss: 14.1650, Val Loss: 13.6350\n",
      "Epoch [135/500] Train Loss: 14.1634, Val Loss: 13.6396\n",
      "Epoch [136/500] Train Loss: 14.1622, Val Loss: 13.6348\n",
      "Epoch [137/500] Train Loss: 14.1619, Val Loss: 13.6447\n",
      "Epoch [138/500] Train Loss: 14.1555, Val Loss: 13.6277\n",
      "Epoch [139/500] Train Loss: 14.1535, Val Loss: 13.6270\n",
      "Epoch [140/500] Train Loss: 14.1509, Val Loss: 13.6308\n",
      "Epoch [141/500] Train Loss: 14.1494, Val Loss: 13.6249\n",
      "Epoch [142/500] Train Loss: 14.1483, Val Loss: 13.6226\n",
      "Epoch [143/500] Train Loss: 14.1469, Val Loss: 13.6208\n",
      "Epoch [144/500] Train Loss: 14.1459, Val Loss: 13.6220\n",
      "Epoch [145/500] Train Loss: 14.1440, Val Loss: 13.6229\n",
      "Epoch [146/500] Train Loss: 14.1436, Val Loss: 13.6175\n",
      "Epoch [147/500] Train Loss: 14.1418, Val Loss: 13.6169\n",
      "Epoch [148/500] Train Loss: 14.1413, Val Loss: 13.6193\n",
      "Epoch [149/500] Train Loss: 14.1398, Val Loss: 13.6148\n",
      "Epoch [150/500] Train Loss: 14.1384, Val Loss: 13.6152\n",
      "Epoch [151/500] Train Loss: 14.1388, Val Loss: 13.6279\n",
      "Epoch [152/500] Train Loss: 14.1401, Val Loss: 13.6117\n",
      "Epoch [153/500] Train Loss: 14.1355, Val Loss: 13.6103\n",
      "Epoch [154/500] Train Loss: 14.1348, Val Loss: 13.6096\n",
      "Epoch [155/500] Train Loss: 14.1347, Val Loss: 13.6100\n",
      "Epoch [156/500] Train Loss: 14.1364, Val Loss: 13.6090\n",
      "Epoch [157/500] Train Loss: 14.1347, Val Loss: 13.6062\n",
      "Epoch [158/500] Train Loss: 14.1321, Val Loss: 13.6049\n",
      "Epoch [159/500] Train Loss: 14.1310, Val Loss: 13.6051\n",
      "Epoch [160/500] Train Loss: 14.1306, Val Loss: 13.6035\n",
      "Epoch [161/500] Train Loss: 14.1299, Val Loss: 13.6024\n",
      "Epoch [162/500] Train Loss: 14.1293, Val Loss: 13.6023\n",
      "Epoch [163/500] Train Loss: 14.1289, Val Loss: 13.6014\n",
      "Epoch [164/500] Train Loss: 14.1284, Val Loss: 13.6030\n",
      "Epoch [165/500] Train Loss: 14.1280, Val Loss: 13.6029\n",
      "Epoch [166/500] Train Loss: 14.1298, Val Loss: 13.6084\n",
      "Epoch [167/500] Train Loss: 14.1283, Val Loss: 13.5974\n",
      "Epoch [168/500] Train Loss: 14.1255, Val Loss: 13.5977\n",
      "Epoch [169/500] Train Loss: 14.1250, Val Loss: 13.5960\n",
      "Epoch [170/500] Train Loss: 14.1244, Val Loss: 13.5954\n",
      "Epoch [171/500] Train Loss: 14.1240, Val Loss: 13.5952\n",
      "Epoch [172/500] Train Loss: 14.1235, Val Loss: 13.5947\n",
      "Epoch [173/500] Train Loss: 14.1230, Val Loss: 13.5948\n",
      "Epoch [174/500] Train Loss: 14.1222, Val Loss: 13.5929\n",
      "Epoch [175/500] Train Loss: 14.1221, Val Loss: 13.5934\n",
      "Epoch [176/500] Train Loss: 14.1222, Val Loss: 13.5923\n",
      "Epoch [177/500] Train Loss: 14.1239, Val Loss: 13.5943\n",
      "Epoch [178/500] Train Loss: 14.1232, Val Loss: 13.5918\n",
      "Epoch [179/500] Train Loss: 14.1223, Val Loss: 13.5906\n",
      "Epoch [180/500] Train Loss: 14.1199, Val Loss: 13.5891\n",
      "Epoch [181/500] Train Loss: 14.1198, Val Loss: 13.5899\n",
      "Epoch [182/500] Train Loss: 14.1197, Val Loss: 13.5896\n",
      "Epoch [183/500] Train Loss: 14.1190, Val Loss: 13.5890\n",
      "Epoch [184/500] Train Loss: 14.1183, Val Loss: 13.5874\n",
      "Epoch [185/500] Train Loss: 14.1179, Val Loss: 13.5869\n",
      "Epoch [186/500] Train Loss: 14.1177, Val Loss: 13.5874\n",
      "Epoch [187/500] Train Loss: 14.1174, Val Loss: 13.5875\n",
      "Epoch [188/500] Train Loss: 14.1185, Val Loss: 13.5865\n",
      "Epoch [189/500] Train Loss: 14.1173, Val Loss: 13.5865\n",
      "Epoch [190/500] Train Loss: 14.1164, Val Loss: 13.5851\n",
      "Epoch [191/500] Train Loss: 14.1161, Val Loss: 13.5845\n",
      "Epoch [192/500] Train Loss: 14.1156, Val Loss: 13.5853\n",
      "Epoch [193/500] Train Loss: 14.1158, Val Loss: 13.5837\n",
      "Epoch [194/500] Train Loss: 14.1153, Val Loss: 13.5838\n",
      "Epoch [195/500] Train Loss: 14.1153, Val Loss: 13.5847\n",
      "Epoch [196/500] Train Loss: 14.1151, Val Loss: 13.5837\n",
      "Epoch [197/500] Train Loss: 14.1149, Val Loss: 13.5821\n",
      "Epoch [198/500] Train Loss: 14.1144, Val Loss: 13.5833\n",
      "Epoch [199/500] Train Loss: 14.1144, Val Loss: 13.5818\n",
      "Epoch [200/500] Train Loss: 14.1137, Val Loss: 13.5820\n",
      "Epoch [201/500] Train Loss: 14.1138, Val Loss: 13.5817\n",
      "Epoch [202/500] Train Loss: 14.1133, Val Loss: 13.5814\n",
      "Epoch [203/500] Train Loss: 14.1137, Val Loss: 13.5814\n",
      "Epoch [204/500] Train Loss: 14.1128, Val Loss: 13.5824\n",
      "Epoch [205/500] Train Loss: 14.1150, Val Loss: 13.5815\n",
      "Epoch [206/500] Train Loss: 14.1132, Val Loss: 13.5811\n",
      "Epoch [207/500] Train Loss: 14.1130, Val Loss: 13.5793\n",
      "Epoch [208/500] Train Loss: 14.1127, Val Loss: 13.5788\n",
      "Epoch [209/500] Train Loss: 14.1116, Val Loss: 13.5783\n",
      "Epoch [210/500] Train Loss: 14.1113, Val Loss: 13.5785\n",
      "Epoch [211/500] Train Loss: 14.1116, Val Loss: 13.5789\n",
      "Epoch [212/500] Train Loss: 14.1120, Val Loss: 13.5791\n",
      "Epoch [213/500] Train Loss: 14.1109, Val Loss: 13.5789\n",
      "Epoch [214/500] Train Loss: 14.1108, Val Loss: 13.5776\n",
      "Epoch [215/500] Train Loss: 14.1104, Val Loss: 13.5770\n",
      "Epoch [216/500] Train Loss: 14.1108, Val Loss: 13.5776\n",
      "Epoch [217/500] Train Loss: 14.1106, Val Loss: 13.5799\n",
      "Epoch [218/500] Train Loss: 14.1113, Val Loss: 13.5793\n",
      "Epoch [219/500] Train Loss: 14.1109, Val Loss: 13.5766\n",
      "Epoch [220/500] Train Loss: 14.1097, Val Loss: 13.5759\n",
      "Epoch [221/500] Train Loss: 14.1099, Val Loss: 13.5758\n",
      "Epoch [222/500] Train Loss: 14.1098, Val Loss: 13.5757\n",
      "Epoch [223/500] Train Loss: 14.1094, Val Loss: 13.5752\n",
      "Epoch [224/500] Train Loss: 14.1089, Val Loss: 13.5748\n",
      "Epoch [225/500] Train Loss: 14.1088, Val Loss: 13.5747\n",
      "Epoch [226/500] Train Loss: 14.1089, Val Loss: 13.5745\n",
      "Epoch [227/500] Train Loss: 14.1088, Val Loss: 13.5744\n",
      "Epoch [228/500] Train Loss: 14.1086, Val Loss: 13.5760\n",
      "Epoch [229/500] Train Loss: 14.1088, Val Loss: 13.5745\n",
      "Epoch [230/500] Train Loss: 14.1087, Val Loss: 13.5740\n",
      "Epoch [231/500] Train Loss: 14.1082, Val Loss: 13.5741\n",
      "Epoch [232/500] Train Loss: 14.1086, Val Loss: 13.5743\n",
      "Epoch [233/500] Train Loss: 14.1080, Val Loss: 13.5732\n",
      "Epoch [234/500] Train Loss: 14.1078, Val Loss: 13.5730\n",
      "Epoch [235/500] Train Loss: 14.1076, Val Loss: 13.5729\n",
      "Epoch [236/500] Train Loss: 14.1074, Val Loss: 13.5725\n",
      "Epoch [237/500] Train Loss: 14.1074, Val Loss: 13.5722\n",
      "Epoch [238/500] Train Loss: 14.1070, Val Loss: 13.5730\n",
      "Epoch [239/500] Train Loss: 14.1072, Val Loss: 13.5721\n",
      "Epoch [240/500] Train Loss: 14.1070, Val Loss: 13.5719\n",
      "Epoch [241/500] Train Loss: 14.1071, Val Loss: 13.5723\n",
      "Epoch [242/500] Train Loss: 14.1070, Val Loss: 13.5718\n",
      "Epoch [243/500] Train Loss: 14.1067, Val Loss: 13.5719\n",
      "Epoch [244/500] Train Loss: 14.1068, Val Loss: 13.5720\n",
      "Epoch [245/500] Train Loss: 14.1068, Val Loss: 13.5712\n",
      "Epoch [246/500] Train Loss: 14.1065, Val Loss: 13.5716\n",
      "Epoch [247/500] Train Loss: 14.1070, Val Loss: 13.5711\n",
      "Epoch [248/500] Train Loss: 14.1066, Val Loss: 13.5706\n",
      "Epoch [249/500] Train Loss: 14.1063, Val Loss: 13.5711\n",
      "Epoch [250/500] Train Loss: 14.1064, Val Loss: 13.5705\n",
      "Epoch [251/500] Train Loss: 14.1060, Val Loss: 13.5705\n",
      "Epoch [252/500] Train Loss: 14.1059, Val Loss: 13.5709\n",
      "Epoch [253/500] Train Loss: 14.1059, Val Loss: 13.5700\n",
      "Epoch [254/500] Train Loss: 14.1059, Val Loss: 13.5711\n",
      "Epoch [255/500] Train Loss: 14.1062, Val Loss: 13.5699\n",
      "Epoch [256/500] Train Loss: 14.1056, Val Loss: 13.5699\n",
      "Epoch [257/500] Train Loss: 14.1056, Val Loss: 13.5701\n",
      "Epoch [258/500] Train Loss: 14.1057, Val Loss: 13.5697\n",
      "Epoch [259/500] Train Loss: 14.1056, Val Loss: 13.5697\n",
      "Epoch [260/500] Train Loss: 14.1056, Val Loss: 13.5696\n",
      "Epoch [261/500] Train Loss: 14.1055, Val Loss: 13.5699\n",
      "Epoch [262/500] Train Loss: 14.1056, Val Loss: 13.5706\n",
      "Epoch [263/500] Train Loss: 14.1057, Val Loss: 13.5699\n",
      "Epoch [264/500] Train Loss: 14.1058, Val Loss: 13.5691\n",
      "Epoch [265/500] Train Loss: 14.1054, Val Loss: 13.5706\n",
      "Epoch [266/500] Train Loss: 14.1062, Val Loss: 13.5754\n",
      "Epoch [267/500] Train Loss: 14.1071, Val Loss: 13.5692\n",
      "Epoch [268/500] Train Loss: 14.1050, Val Loss: 13.5692\n",
      "Epoch [269/500] Train Loss: 14.1050, Val Loss: 13.5692\n",
      "Epoch [270/500] Train Loss: 14.1050, Val Loss: 13.5699\n",
      "Epoch [271/500] Train Loss: 14.1053, Val Loss: 13.5686\n",
      "Epoch [272/500] Train Loss: 14.1050, Val Loss: 13.5687\n",
      "Epoch [273/500] Train Loss: 14.1046, Val Loss: 13.5684\n",
      "Epoch [274/500] Train Loss: 14.1046, Val Loss: 13.5690\n",
      "Epoch [275/500] Train Loss: 14.1048, Val Loss: 13.5684\n",
      "Epoch [276/500] Train Loss: 14.1050, Val Loss: 13.5685\n",
      "Epoch [277/500] Train Loss: 14.1046, Val Loss: 13.5682\n",
      "Epoch [278/500] Train Loss: 14.1046, Val Loss: 13.5682\n",
      "Epoch [279/500] Train Loss: 14.1044, Val Loss: 13.5680\n",
      "Epoch [280/500] Train Loss: 14.1043, Val Loss: 13.5689\n",
      "Epoch [281/500] Train Loss: 14.1048, Val Loss: 13.5689\n",
      "Epoch [282/500] Train Loss: 14.1050, Val Loss: 13.5680\n",
      "Epoch [283/500] Train Loss: 14.1045, Val Loss: 13.5681\n",
      "Epoch [284/500] Train Loss: 14.1042, Val Loss: 13.5678\n",
      "Epoch [285/500] Train Loss: 14.1041, Val Loss: 13.5677\n",
      "Epoch [286/500] Train Loss: 14.1043, Val Loss: 13.5675\n",
      "Epoch [287/500] Train Loss: 14.1043, Val Loss: 13.5675\n",
      "Epoch [288/500] Train Loss: 14.1040, Val Loss: 13.5675\n",
      "Epoch [289/500] Train Loss: 14.1043, Val Loss: 13.5674\n",
      "Epoch [290/500] Train Loss: 14.1042, Val Loss: 13.5681\n",
      "Epoch [291/500] Train Loss: 14.1043, Val Loss: 13.5678\n",
      "Epoch [292/500] Train Loss: 14.1041, Val Loss: 13.5674\n",
      "Epoch [293/500] Train Loss: 14.1042, Val Loss: 13.5675\n",
      "Epoch [294/500] Train Loss: 14.1041, Val Loss: 13.5681\n",
      "Epoch [295/500] Train Loss: 14.1039, Val Loss: 13.5675\n",
      "Epoch [296/500] Train Loss: 14.1038, Val Loss: 13.5671\n",
      "Epoch [297/500] Train Loss: 14.1038, Val Loss: 13.5668\n",
      "Epoch [298/500] Train Loss: 14.1039, Val Loss: 13.5682\n",
      "Epoch [299/500] Train Loss: 14.1040, Val Loss: 13.5668\n",
      "Epoch [300/500] Train Loss: 14.1039, Val Loss: 13.5668\n",
      "Epoch [301/500] Train Loss: 14.1036, Val Loss: 13.5667\n",
      "Epoch [302/500] Train Loss: 14.1036, Val Loss: 13.5668\n",
      "Epoch [303/500] Train Loss: 14.1035, Val Loss: 13.5673\n",
      "Epoch [304/500] Train Loss: 14.1036, Val Loss: 13.5667\n",
      "Epoch [305/500] Train Loss: 14.1035, Val Loss: 13.5670\n",
      "Epoch [306/500] Train Loss: 14.1034, Val Loss: 13.5665\n",
      "Epoch [307/500] Train Loss: 14.1033, Val Loss: 13.5665\n",
      "Epoch [308/500] Train Loss: 14.1034, Val Loss: 13.5669\n",
      "Epoch [309/500] Train Loss: 14.1035, Val Loss: 13.5663\n",
      "Epoch [310/500] Train Loss: 14.1034, Val Loss: 13.5676\n",
      "Epoch [311/500] Train Loss: 14.1037, Val Loss: 13.5662\n",
      "Epoch [312/500] Train Loss: 14.1037, Val Loss: 13.5664\n",
      "Epoch [313/500] Train Loss: 14.1036, Val Loss: 13.5679\n",
      "Epoch [314/500] Train Loss: 14.1043, Val Loss: 13.5672\n",
      "Epoch [315/500] Train Loss: 14.1038, Val Loss: 13.5670\n",
      "Epoch [316/500] Train Loss: 14.1032, Val Loss: 13.5672\n",
      "Epoch [317/500] Train Loss: 14.1036, Val Loss: 13.5660\n",
      "Epoch [318/500] Train Loss: 14.1031, Val Loss: 13.5663\n",
      "Epoch [319/500] Train Loss: 14.1031, Val Loss: 13.5660\n",
      "Epoch [320/500] Train Loss: 14.1029, Val Loss: 13.5660\n",
      "Epoch [321/500] Train Loss: 14.1028, Val Loss: 13.5659\n",
      "Epoch [322/500] Train Loss: 14.1029, Val Loss: 13.5660\n",
      "Epoch [323/500] Train Loss: 14.1030, Val Loss: 13.5662\n",
      "Epoch [324/500] Train Loss: 14.1029, Val Loss: 13.5661\n",
      "Epoch [325/500] Train Loss: 14.1031, Val Loss: 13.5675\n",
      "Epoch [326/500] Train Loss: 14.1039, Val Loss: 13.5661\n",
      "Epoch [327/500] Train Loss: 14.1032, Val Loss: 13.5659\n",
      "Epoch [328/500] Train Loss: 14.1031, Val Loss: 13.5662\n",
      "Epoch [329/500] Train Loss: 14.1028, Val Loss: 13.5657\n",
      "Epoch [330/500] Train Loss: 14.1027, Val Loss: 13.5662\n",
      "Epoch [331/500] Train Loss: 14.1028, Val Loss: 13.5657\n",
      "Epoch [332/500] Train Loss: 14.1028, Val Loss: 13.5664\n",
      "Epoch [333/500] Train Loss: 14.1028, Val Loss: 13.5665\n",
      "Epoch [334/500] Train Loss: 14.1030, Val Loss: 13.5666\n",
      "Epoch [335/500] Train Loss: 14.1027, Val Loss: 13.5656\n",
      "Epoch [336/500] Train Loss: 14.1028, Val Loss: 13.5656\n",
      "Epoch [337/500] Train Loss: 14.1027, Val Loss: 13.5658\n",
      "Epoch [338/500] Train Loss: 14.1025, Val Loss: 13.5665\n",
      "Epoch [339/500] Train Loss: 14.1031, Val Loss: 13.5663\n",
      "Epoch [340/500] Train Loss: 14.1033, Val Loss: 13.5656\n",
      "Epoch [341/500] Train Loss: 14.1025, Val Loss: 13.5656\n",
      "Epoch [342/500] Train Loss: 14.1025, Val Loss: 13.5654\n",
      "Epoch [343/500] Train Loss: 14.1024, Val Loss: 13.5662\n",
      "Epoch [344/500] Train Loss: 14.1030, Val Loss: 13.5653\n",
      "Epoch [345/500] Train Loss: 14.1024, Val Loss: 13.5655\n",
      "Epoch [346/500] Train Loss: 14.1025, Val Loss: 13.5654\n",
      "Epoch [347/500] Train Loss: 14.1025, Val Loss: 13.5653\n",
      "Epoch [348/500] Train Loss: 14.1025, Val Loss: 13.5654\n",
      "Epoch [349/500] Train Loss: 14.1023, Val Loss: 13.5654\n",
      "Epoch [350/500] Train Loss: 14.1023, Val Loss: 13.5652\n",
      "Epoch [351/500] Train Loss: 14.1024, Val Loss: 13.5652\n",
      "Epoch [352/500] Train Loss: 14.1022, Val Loss: 13.5662\n",
      "Epoch [353/500] Train Loss: 14.1025, Val Loss: 13.5651\n",
      "Epoch [354/500] Train Loss: 14.1023, Val Loss: 13.5657\n",
      "Epoch [355/500] Train Loss: 14.1029, Val Loss: 13.5660\n",
      "Epoch [356/500] Train Loss: 14.1030, Val Loss: 13.5650\n",
      "Epoch [357/500] Train Loss: 14.1030, Val Loss: 13.5653\n",
      "Epoch [358/500] Train Loss: 14.1032, Val Loss: 13.5653\n",
      "Epoch [359/500] Train Loss: 14.1027, Val Loss: 13.5657\n",
      "Epoch [360/500] Train Loss: 14.1034, Val Loss: 13.5670\n",
      "Epoch [361/500] Train Loss: 14.1035, Val Loss: 13.5670\n",
      "Epoch [362/500] Train Loss: 14.1034, Val Loss: 13.5658\n",
      "Epoch [363/500] Train Loss: 14.1022, Val Loss: 13.5649\n",
      "Epoch [364/500] Train Loss: 14.1024, Val Loss: 13.5656\n",
      "Epoch [365/500] Train Loss: 14.1026, Val Loss: 13.5650\n",
      "Epoch [366/500] Train Loss: 14.1023, Val Loss: 13.5650\n",
      "Epoch [367/500] Train Loss: 14.1022, Val Loss: 13.5649\n",
      "Epoch [368/500] Train Loss: 14.1022, Val Loss: 13.5654\n",
      "Epoch [369/500] Train Loss: 14.1022, Val Loss: 13.5651\n",
      "Epoch [370/500] Train Loss: 14.1020, Val Loss: 13.5651\n",
      "Epoch [371/500] Train Loss: 14.1022, Val Loss: 13.5649\n",
      "Epoch [372/500] Train Loss: 14.1023, Val Loss: 13.5657\n",
      "Epoch [373/500] Train Loss: 14.1025, Val Loss: 13.5653\n",
      "Epoch [374/500] Train Loss: 14.1023, Val Loss: 13.5650\n",
      "Epoch [375/500] Train Loss: 14.1022, Val Loss: 13.5648\n",
      "Epoch [376/500] Train Loss: 14.1022, Val Loss: 13.5648\n",
      "Epoch [377/500] Train Loss: 14.1021, Val Loss: 13.5650\n",
      "Epoch [378/500] Train Loss: 14.1020, Val Loss: 13.5653\n",
      "Epoch [379/500] Train Loss: 14.1020, Val Loss: 13.5648\n",
      "Epoch [380/500] Train Loss: 14.1019, Val Loss: 13.5650\n",
      "Epoch [381/500] Train Loss: 14.1020, Val Loss: 13.5647\n",
      "Epoch [382/500] Train Loss: 14.1019, Val Loss: 13.5648\n",
      "Epoch [383/500] Train Loss: 14.1022, Val Loss: 13.5647\n",
      "Epoch [384/500] Train Loss: 14.1021, Val Loss: 13.5655\n",
      "Epoch [385/500] Train Loss: 14.1023, Val Loss: 13.5647\n",
      "Epoch [386/500] Train Loss: 14.1021, Val Loss: 13.5649\n",
      "Epoch [387/500] Train Loss: 14.1020, Val Loss: 13.5646\n",
      "Epoch [388/500] Train Loss: 14.1019, Val Loss: 13.5654\n",
      "Epoch [389/500] Train Loss: 14.1025, Val Loss: 13.5658\n",
      "Epoch [390/500] Train Loss: 14.1021, Val Loss: 13.5647\n",
      "Epoch [391/500] Train Loss: 14.1019, Val Loss: 13.5652\n",
      "Epoch [392/500] Train Loss: 14.1020, Val Loss: 13.5646\n",
      "Epoch [393/500] Train Loss: 14.1019, Val Loss: 13.5646\n",
      "Epoch [394/500] Train Loss: 14.1022, Val Loss: 13.5646\n",
      "Epoch [395/500] Train Loss: 14.1018, Val Loss: 13.5646\n",
      "Epoch [396/500] Train Loss: 14.1018, Val Loss: 13.5648\n",
      "Epoch [397/500] Train Loss: 14.1018, Val Loss: 13.5645\n",
      "Epoch [398/500] Train Loss: 14.1019, Val Loss: 13.5655\n",
      "Epoch [399/500] Train Loss: 14.1021, Val Loss: 13.5646\n",
      "Epoch [400/500] Train Loss: 14.1020, Val Loss: 13.5647\n",
      "Epoch [401/500] Train Loss: 14.1018, Val Loss: 13.5651\n",
      "Epoch [402/500] Train Loss: 14.1025, Val Loss: 13.5647\n",
      "Epoch [403/500] Train Loss: 14.1020, Val Loss: 13.5650\n",
      "Epoch [404/500] Train Loss: 14.1022, Val Loss: 13.5648\n",
      "Epoch [405/500] Train Loss: 14.1021, Val Loss: 13.5651\n",
      "Epoch [406/500] Train Loss: 14.1018, Val Loss: 13.5644\n",
      "Epoch [407/500] Train Loss: 14.1017, Val Loss: 13.5647\n",
      "Epoch [408/500] Train Loss: 14.1019, Val Loss: 13.5649\n",
      "Epoch [409/500] Train Loss: 14.1020, Val Loss: 13.5645\n",
      "Epoch [410/500] Train Loss: 14.1019, Val Loss: 13.5645\n",
      "Epoch [411/500] Train Loss: 14.1020, Val Loss: 13.5652\n",
      "Epoch [412/500] Train Loss: 14.1020, Val Loss: 13.5647\n",
      "Epoch [413/500] Train Loss: 14.1018, Val Loss: 13.5644\n",
      "Epoch [414/500] Train Loss: 14.1017, Val Loss: 13.5644\n",
      "Epoch [415/500] Train Loss: 14.1019, Val Loss: 13.5645\n",
      "Epoch [416/500] Train Loss: 14.1017, Val Loss: 13.5646\n",
      "Epoch [417/500] Train Loss: 14.1017, Val Loss: 13.5646\n",
      "Epoch [418/500] Train Loss: 14.1018, Val Loss: 13.5651\n",
      "Epoch [419/500] Train Loss: 14.1018, Val Loss: 13.5643\n",
      "Epoch [420/500] Train Loss: 14.1019, Val Loss: 13.5643\n",
      "Epoch [421/500] Train Loss: 14.1018, Val Loss: 13.5649\n",
      "Epoch [422/500] Train Loss: 14.1017, Val Loss: 13.5643\n",
      "Epoch [423/500] Train Loss: 14.1019, Val Loss: 13.5647\n",
      "Epoch [424/500] Train Loss: 14.1017, Val Loss: 13.5643\n",
      "Epoch [425/500] Train Loss: 14.1016, Val Loss: 13.5648\n",
      "Epoch [426/500] Train Loss: 14.1018, Val Loss: 13.5643\n",
      "Epoch [427/500] Train Loss: 14.1017, Val Loss: 13.5643\n",
      "Epoch [428/500] Train Loss: 14.1017, Val Loss: 13.5650\n",
      "Epoch [429/500] Train Loss: 14.1020, Val Loss: 13.5642\n",
      "Epoch [430/500] Train Loss: 14.1017, Val Loss: 13.5642\n",
      "Epoch [431/500] Train Loss: 14.1016, Val Loss: 13.5643\n",
      "Epoch [432/500] Train Loss: 14.1016, Val Loss: 13.5643\n",
      "Epoch [433/500] Train Loss: 14.1016, Val Loss: 13.5641\n",
      "Epoch [434/500] Train Loss: 14.1017, Val Loss: 13.5642\n",
      "Epoch [435/500] Train Loss: 14.1016, Val Loss: 13.5642\n",
      "Epoch [436/500] Train Loss: 14.1017, Val Loss: 13.5642\n",
      "Epoch [437/500] Train Loss: 14.1021, Val Loss: 13.5642\n",
      "Epoch [438/500] Train Loss: 14.1020, Val Loss: 13.5643\n",
      "Epoch [439/500] Train Loss: 14.1020, Val Loss: 13.5645\n",
      "Epoch [440/500] Train Loss: 14.1017, Val Loss: 13.5641\n",
      "Epoch [441/500] Train Loss: 14.1015, Val Loss: 13.5644\n",
      "Epoch [442/500] Train Loss: 14.1018, Val Loss: 13.5642\n",
      "Epoch [443/500] Train Loss: 14.1017, Val Loss: 13.5643\n",
      "Epoch [444/500] Train Loss: 14.1019, Val Loss: 13.5643\n",
      "Epoch [445/500] Train Loss: 14.1015, Val Loss: 13.5641\n",
      "Epoch [446/500] Train Loss: 14.1015, Val Loss: 13.5642\n",
      "Epoch [447/500] Train Loss: 14.1016, Val Loss: 13.5642\n",
      "Epoch [448/500] Train Loss: 14.1022, Val Loss: 13.5646\n",
      "Epoch [449/500] Train Loss: 14.1019, Val Loss: 13.5642\n",
      "Epoch [450/500] Train Loss: 14.1018, Val Loss: 13.5642\n",
      "Epoch [451/500] Train Loss: 14.1016, Val Loss: 13.5641\n",
      "Epoch [452/500] Train Loss: 14.1015, Val Loss: 13.5641\n",
      "Epoch [453/500] Train Loss: 14.1015, Val Loss: 13.5648\n",
      "Epoch [454/500] Train Loss: 14.1022, Val Loss: 13.5643\n",
      "Epoch [455/500] Train Loss: 14.1021, Val Loss: 13.5640\n",
      "Epoch [456/500] Train Loss: 14.1019, Val Loss: 13.5641\n",
      "Epoch [457/500] Train Loss: 14.1016, Val Loss: 13.5640\n",
      "Epoch [458/500] Train Loss: 14.1017, Val Loss: 13.5644\n",
      "Epoch [459/500] Train Loss: 14.1015, Val Loss: 13.5639\n",
      "Epoch [460/500] Train Loss: 14.1015, Val Loss: 13.5643\n",
      "Epoch [461/500] Train Loss: 14.1014, Val Loss: 13.5641\n",
      "Epoch [462/500] Train Loss: 14.1014, Val Loss: 13.5639\n",
      "Epoch [463/500] Train Loss: 14.1015, Val Loss: 13.5640\n",
      "Epoch [464/500] Train Loss: 14.1015, Val Loss: 13.5639\n",
      "Epoch [465/500] Train Loss: 14.1016, Val Loss: 13.5642\n",
      "Epoch [466/500] Train Loss: 14.1016, Val Loss: 13.5645\n",
      "Epoch [467/500] Train Loss: 14.1019, Val Loss: 13.5639\n",
      "Epoch [468/500] Train Loss: 14.1015, Val Loss: 13.5641\n",
      "Epoch [469/500] Train Loss: 14.1015, Val Loss: 13.5641\n",
      "Epoch [470/500] Train Loss: 14.1014, Val Loss: 13.5640\n",
      "Epoch [471/500] Train Loss: 14.1015, Val Loss: 13.5643\n",
      "Epoch [472/500] Train Loss: 14.1023, Val Loss: 13.5640\n",
      "Epoch [473/500] Train Loss: 14.1017, Val Loss: 13.5649\n",
      "Epoch [474/500] Train Loss: 14.1025, Val Loss: 13.5648\n",
      "Epoch [475/500] Train Loss: 14.1018, Val Loss: 13.5642\n",
      "Epoch [476/500] Train Loss: 14.1014, Val Loss: 13.5640\n",
      "Epoch [477/500] Train Loss: 14.1014, Val Loss: 13.5644\n",
      "Epoch [478/500] Train Loss: 14.1016, Val Loss: 13.5647\n",
      "Epoch [479/500] Train Loss: 14.1017, Val Loss: 13.5639\n",
      "Epoch [480/500] Train Loss: 14.1014, Val Loss: 13.5638\n",
      "Epoch [481/500] Train Loss: 14.1016, Val Loss: 13.5647\n",
      "Epoch [482/500] Train Loss: 14.1015, Val Loss: 13.5638\n",
      "Epoch [483/500] Train Loss: 14.1012, Val Loss: 13.5638\n",
      "Epoch [484/500] Train Loss: 14.1014, Val Loss: 13.5644\n",
      "Epoch [485/500] Train Loss: 14.1019, Val Loss: 13.5637\n",
      "Epoch [486/500] Train Loss: 14.1016, Val Loss: 13.5638\n",
      "Epoch [487/500] Train Loss: 14.1014, Val Loss: 13.5639\n",
      "Epoch [488/500] Train Loss: 14.1013, Val Loss: 13.5639\n",
      "Epoch [489/500] Train Loss: 14.1012, Val Loss: 13.5638\n",
      "Epoch [490/500] Train Loss: 14.1013, Val Loss: 13.5641\n",
      "Epoch [491/500] Train Loss: 14.1013, Val Loss: 13.5638\n",
      "Epoch [492/500] Train Loss: 14.1013, Val Loss: 13.5638\n",
      "Epoch [493/500] Train Loss: 14.1012, Val Loss: 13.5641\n",
      "Epoch [494/500] Train Loss: 14.1014, Val Loss: 13.5642\n",
      "Epoch [495/500] Train Loss: 14.1013, Val Loss: 13.5639\n",
      "Epoch [496/500] Train Loss: 14.1014, Val Loss: 13.5638\n",
      "Epoch [497/500] Train Loss: 14.1014, Val Loss: 13.5638\n",
      "Epoch [498/500] Train Loss: 14.1013, Val Loss: 13.5638\n",
      "Epoch [499/500] Train Loss: 14.1013, Val Loss: 13.5637\n",
      "Epoch [500/500] Train Loss: 14.1013, Val Loss: 13.5642\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "    print(f'\\n=== Fold {fold+1} ===')\n",
    "    \n",
    "    # Create subset loaders\n",
    "    train_subset = Subset(dataset, train_idx)\n",
    "    val_subset = Subset(dataset, val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model, loss, optimizer (re-init for each fold)\n",
    "    model = Autoencoder()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs in train_loader:\n",
    "            targets = inputs.clone()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        train_loss /= len(train_subset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs in val_loader:\n",
    "                targets = inputs.clone()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss /= len(val_subset)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "81dcfb8e-4653-4078-aa0e-802fab0cbcda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0538, 1.0648, 0.0000]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "x = torch.tensor([[char2idx[char] for char in 'ABC']], dtype=torch.float32)\n",
    "y = model(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "7756dd4b-ebcc-404b-9deb-47fcc39bf6b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.]]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_idxs.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2aaea1c8-22c3-4943-9a8b-d1491dac294d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: AAB, predicted: AAA\n",
      "token: AAC, predicted: AAA\n",
      "token: AAE, predicted: AAA\n",
      "token: AAH, predicted: AAA\n",
      "token: AAK, predicted: AAA\n",
      "token: AAM, predicted: AAA\n",
      "token: AAO, predicted: AAA\n",
      "token: AAP, predicted: AAA\n",
      "token: AAT, predicted: AAA\n",
      "token: AAX, predicted: AAA\n",
      "token: AAY, predicted: AAA\n",
      "token: ABB, predicted: ABA\n",
      "token: ABC, predicted: ABA\n",
      "token: ABE, predicted: ABA\n",
      "token: ABH, predicted: ABA\n",
      "token: ABK, predicted: ABA\n",
      "token: ABM, predicted: ABA\n",
      "token: ABO, predicted: ABA\n",
      "token: ABP, predicted: ABA\n",
      "token: ABT, predicted: ABA\n",
      "token: ABX, predicted: ABA\n",
      "token: ABY, predicted: ABA\n",
      "token: ACB, predicted: ACA\n",
      "token: ACC, predicted: ACA\n",
      "token: ACE, predicted: ACA\n",
      "token: ACH, predicted: ACA\n",
      "token: ACK, predicted: ACA\n",
      "token: ACM, predicted: ACA\n",
      "token: ACO, predicted: ACA\n",
      "token: ACP, predicted: ACA\n",
      "token: ACT, predicted: ACA\n",
      "token: ACX, predicted: ACA\n",
      "token: ACY, predicted: ACA\n",
      "token: AEB, predicted: AEA\n",
      "token: AEC, predicted: AEA\n",
      "token: AEE, predicted: AEA\n",
      "token: AEH, predicted: AEA\n",
      "token: AEK, predicted: AEA\n",
      "token: AEM, predicted: AEA\n",
      "token: AEO, predicted: AEA\n",
      "token: AEP, predicted: AEA\n",
      "token: AET, predicted: AEA\n",
      "token: AEX, predicted: AEA\n",
      "token: AEY, predicted: AEA\n",
      "token: AHB, predicted: AHA\n",
      "token: AHC, predicted: AHA\n",
      "token: AHE, predicted: AHA\n",
      "token: AHH, predicted: AHA\n",
      "token: AHK, predicted: AHA\n",
      "token: AHM, predicted: AHA\n",
      "token: AHO, predicted: AHA\n",
      "token: AHP, predicted: AHA\n",
      "token: AHT, predicted: AHA\n",
      "token: AHX, predicted: AHA\n",
      "token: AHY, predicted: AHA\n",
      "token: AKB, predicted: AKA\n",
      "token: AKC, predicted: AKA\n",
      "token: AKE, predicted: AKA\n",
      "token: AKH, predicted: AKA\n",
      "token: AKK, predicted: AKA\n",
      "token: AKM, predicted: AKA\n",
      "token: AKO, predicted: AKA\n",
      "token: AKP, predicted: AKA\n",
      "token: AKT, predicted: AKA\n",
      "token: AKX, predicted: AKA\n",
      "token: AKY, predicted: AKA\n",
      "token: AMB, predicted: AMA\n",
      "token: AMC, predicted: AMA\n",
      "token: AME, predicted: AMA\n",
      "token: AMH, predicted: AMA\n",
      "token: AMK, predicted: AMA\n",
      "token: AMM, predicted: AMA\n",
      "token: AMO, predicted: AMA\n",
      "token: AMP, predicted: AMA\n",
      "token: AMT, predicted: AMA\n",
      "token: AMX, predicted: AMA\n",
      "token: AMY, predicted: AMA\n",
      "token: AOB, predicted: AOA\n",
      "token: AOC, predicted: AOA\n",
      "token: AOE, predicted: AOA\n",
      "token: AOH, predicted: AOA\n",
      "token: AOK, predicted: AOA\n",
      "token: AOM, predicted: AOA\n",
      "token: AOO, predicted: AOA\n",
      "token: AOP, predicted: AOA\n",
      "token: AOT, predicted: AOA\n",
      "token: AOX, predicted: AOA\n",
      "token: AOY, predicted: AOA\n",
      "token: APB, predicted: APA\n",
      "token: APC, predicted: APA\n",
      "token: APE, predicted: APA\n",
      "token: APH, predicted: APA\n",
      "token: APK, predicted: APA\n",
      "token: APM, predicted: APA\n",
      "token: APO, predicted: APA\n",
      "token: APP, predicted: APA\n",
      "token: APT, predicted: APA\n",
      "token: APX, predicted: APA\n",
      "token: APY, predicted: APA\n",
      "token: ATB, predicted: ATA\n",
      "token: ATC, predicted: ATA\n",
      "token: ATE, predicted: ATA\n",
      "token: ATH, predicted: ATA\n",
      "token: ATK, predicted: ATA\n",
      "token: ATM, predicted: ATA\n",
      "token: ATO, predicted: ATA\n",
      "token: ATP, predicted: ATA\n",
      "token: ATT, predicted: ATA\n",
      "token: ATX, predicted: ATA\n",
      "token: ATY, predicted: ATA\n",
      "token: AXB, predicted: AXA\n",
      "token: AXC, predicted: AXA\n",
      "token: AXE, predicted: AXA\n",
      "token: AXH, predicted: AXA\n",
      "token: AXK, predicted: AXA\n",
      "token: AXM, predicted: AXA\n",
      "token: AXO, predicted: AXA\n",
      "token: AXP, predicted: AXA\n",
      "token: AXT, predicted: AXA\n",
      "token: AXX, predicted: AXA\n",
      "token: AXY, predicted: AXA\n",
      "token: AYB, predicted: AYA\n",
      "token: AYC, predicted: AYA\n",
      "token: AYE, predicted: AYA\n",
      "token: AYH, predicted: AYA\n",
      "token: AYK, predicted: AYA\n",
      "token: AYM, predicted: AYA\n",
      "token: AYO, predicted: AYA\n",
      "token: AYP, predicted: AYA\n",
      "token: AYT, predicted: AYA\n",
      "token: AYX, predicted: AYA\n",
      "token: AYY, predicted: AYA\n",
      "token: BAB, predicted: BAA\n",
      "token: BAC, predicted: BAA\n",
      "token: BAE, predicted: BAA\n",
      "token: BAH, predicted: BAA\n",
      "token: BAK, predicted: BAA\n",
      "token: BAM, predicted: BAA\n",
      "token: BAO, predicted: BAA\n",
      "token: BAP, predicted: BAA\n",
      "token: BAT, predicted: BAA\n",
      "token: BAX, predicted: BAA\n",
      "token: BAY, predicted: BAA\n",
      "token: BBB, predicted: BBA\n",
      "token: BBC, predicted: BBA\n",
      "token: BBE, predicted: BBA\n",
      "token: BBH, predicted: BBA\n",
      "token: BBK, predicted: BBA\n",
      "token: BBM, predicted: BBA\n",
      "token: BBO, predicted: BBA\n",
      "token: BBP, predicted: BBA\n",
      "token: BBT, predicted: BBA\n",
      "token: BBX, predicted: BBA\n",
      "token: BBY, predicted: BBA\n",
      "token: BCB, predicted: BCA\n",
      "token: BCC, predicted: BCA\n",
      "token: BCE, predicted: BCA\n",
      "token: BCH, predicted: BCA\n",
      "token: BCK, predicted: BCA\n",
      "token: BCM, predicted: BCA\n",
      "token: BCO, predicted: BCA\n",
      "token: BCP, predicted: BCA\n",
      "token: BCT, predicted: BCA\n",
      "token: BCX, predicted: BCA\n",
      "token: BCY, predicted: BCA\n",
      "token: BEB, predicted: BEA\n",
      "token: BEC, predicted: BEA\n",
      "token: BEE, predicted: BEA\n",
      "token: BEH, predicted: BEA\n",
      "token: BEK, predicted: BEA\n",
      "token: BEM, predicted: BEA\n",
      "token: BEO, predicted: BEA\n",
      "token: BEP, predicted: BEA\n",
      "token: BET, predicted: BEA\n",
      "token: BEX, predicted: BEA\n",
      "token: BEY, predicted: BEA\n",
      "token: BHB, predicted: BHA\n",
      "token: BHC, predicted: BHA\n",
      "token: BHE, predicted: BHA\n",
      "token: BHH, predicted: BHA\n",
      "token: BHK, predicted: BHA\n",
      "token: BHM, predicted: BHA\n",
      "token: BHO, predicted: BHA\n",
      "token: BHP, predicted: BHA\n",
      "token: BHT, predicted: BHA\n",
      "token: BHX, predicted: BHA\n",
      "token: BHY, predicted: BHA\n",
      "token: BKB, predicted: BKA\n",
      "token: BKC, predicted: BKA\n",
      "token: BKE, predicted: BKA\n",
      "token: BKH, predicted: BKA\n",
      "token: BKK, predicted: BKA\n",
      "token: BKM, predicted: BKA\n",
      "token: BKO, predicted: BKA\n",
      "token: BKP, predicted: BKA\n",
      "token: BKT, predicted: BKA\n",
      "token: BKX, predicted: BKA\n",
      "token: BKY, predicted: BKA\n",
      "token: BMB, predicted: BMA\n",
      "token: BMC, predicted: BMA\n",
      "token: BME, predicted: BMA\n",
      "token: BMH, predicted: BMA\n",
      "token: BMK, predicted: BMA\n",
      "token: BMM, predicted: BMA\n",
      "token: BMO, predicted: BMA\n",
      "token: BMP, predicted: BMA\n",
      "token: BMT, predicted: BMA\n",
      "token: BMX, predicted: BMA\n",
      "token: BMY, predicted: BMA\n",
      "token: BOB, predicted: BOA\n",
      "token: BOC, predicted: BOA\n",
      "token: BOE, predicted: BOA\n",
      "token: BOH, predicted: BOA\n",
      "token: BOK, predicted: BOA\n",
      "token: BOM, predicted: BOA\n",
      "token: BOO, predicted: BOA\n",
      "token: BOP, predicted: BOA\n",
      "token: BOT, predicted: BOA\n",
      "token: BOX, predicted: BOA\n",
      "token: BOY, predicted: BOA\n",
      "token: BPB, predicted: BPA\n",
      "token: BPC, predicted: BPA\n",
      "token: BPE, predicted: BPA\n",
      "token: BPH, predicted: BPA\n",
      "token: BPK, predicted: BPA\n",
      "token: BPM, predicted: BPA\n",
      "token: BPO, predicted: BPA\n",
      "token: BPP, predicted: BPA\n",
      "token: BPT, predicted: BPA\n",
      "token: BPX, predicted: BPA\n",
      "token: BPY, predicted: BPA\n",
      "token: BTB, predicted: BTA\n",
      "token: BTC, predicted: BTA\n",
      "token: BTE, predicted: BTA\n",
      "token: BTH, predicted: BTA\n",
      "token: BTK, predicted: BTA\n",
      "token: BTM, predicted: BTA\n",
      "token: BTO, predicted: BTA\n",
      "token: BTP, predicted: BTA\n",
      "token: BTT, predicted: BTA\n",
      "token: BTX, predicted: BTA\n",
      "token: BTY, predicted: BTA\n",
      "token: BXB, predicted: BXA\n",
      "token: BXC, predicted: BXA\n",
      "token: BXE, predicted: BXA\n",
      "token: BXH, predicted: BXA\n",
      "token: BXK, predicted: BXA\n",
      "token: BXM, predicted: BXA\n",
      "token: BXO, predicted: BXA\n",
      "token: BXP, predicted: BXA\n",
      "token: BXT, predicted: BXA\n",
      "token: BXX, predicted: BXA\n",
      "token: BXY, predicted: BXA\n",
      "token: BYB, predicted: BYA\n",
      "token: BYC, predicted: BYA\n",
      "token: BYE, predicted: BYA\n",
      "token: BYH, predicted: BYA\n",
      "token: BYK, predicted: BYA\n",
      "token: BYM, predicted: BYA\n",
      "token: BYO, predicted: BYA\n",
      "token: BYP, predicted: BYA\n",
      "token: BYT, predicted: BYA\n",
      "token: BYX, predicted: BYA\n",
      "token: BYY, predicted: BYA\n",
      "token: CAB, predicted: CAA\n",
      "token: CAC, predicted: CAA\n",
      "token: CAE, predicted: CAA\n",
      "token: CAH, predicted: CAA\n",
      "token: CAK, predicted: CAA\n",
      "token: CAM, predicted: CAA\n",
      "token: CAO, predicted: CAA\n",
      "token: CAP, predicted: CAA\n",
      "token: CAT, predicted: CAA\n",
      "token: CAX, predicted: CAA\n",
      "token: CAY, predicted: CAA\n",
      "token: CBB, predicted: CBA\n",
      "token: CBC, predicted: CBA\n",
      "token: CBE, predicted: CBA\n",
      "token: CBH, predicted: CBA\n",
      "token: CBK, predicted: CBA\n",
      "token: CBM, predicted: CBA\n",
      "token: CBO, predicted: CBA\n",
      "token: CBP, predicted: CBA\n",
      "token: CBT, predicted: CBA\n",
      "token: CBX, predicted: CBA\n",
      "token: CBY, predicted: CBA\n",
      "token: CCB, predicted: CCA\n",
      "token: CCC, predicted: CCA\n",
      "token: CCE, predicted: CCA\n",
      "token: CCH, predicted: CCA\n",
      "token: CCK, predicted: CCA\n",
      "token: CCM, predicted: CCA\n",
      "token: CCO, predicted: CCA\n",
      "token: CCP, predicted: CCA\n",
      "token: CCT, predicted: CCA\n",
      "token: CCX, predicted: CCA\n",
      "token: CCY, predicted: CCA\n",
      "token: CEB, predicted: CEA\n",
      "token: CEC, predicted: CEA\n",
      "token: CEE, predicted: CEA\n",
      "token: CEH, predicted: CEA\n",
      "token: CEK, predicted: CEA\n",
      "token: CEM, predicted: CEA\n",
      "token: CEO, predicted: CEA\n",
      "token: CEP, predicted: CEA\n",
      "token: CET, predicted: CEA\n",
      "token: CEX, predicted: CEA\n",
      "token: CEY, predicted: CEA\n",
      "token: CHB, predicted: CHA\n",
      "token: CHC, predicted: CHA\n",
      "token: CHE, predicted: CHA\n",
      "token: CHH, predicted: CHA\n",
      "token: CHK, predicted: CHA\n",
      "token: CHM, predicted: CHA\n",
      "token: CHO, predicted: CHA\n",
      "token: CHP, predicted: CHA\n",
      "token: CHT, predicted: CHA\n",
      "token: CHX, predicted: CHA\n",
      "token: CHY, predicted: CHA\n",
      "token: CKB, predicted: CKA\n",
      "token: CKC, predicted: CKA\n",
      "token: CKE, predicted: CKA\n",
      "token: CKH, predicted: CKA\n",
      "token: CKK, predicted: CKA\n",
      "token: CKM, predicted: CKA\n",
      "token: CKO, predicted: CKA\n",
      "token: CKP, predicted: CKA\n",
      "token: CKT, predicted: CKA\n",
      "token: CKX, predicted: CKA\n",
      "token: CKY, predicted: CKA\n",
      "token: CMB, predicted: CMA\n",
      "token: CMC, predicted: CMA\n",
      "token: CME, predicted: CMA\n",
      "token: CMH, predicted: CMA\n",
      "token: CMK, predicted: CMA\n",
      "token: CMM, predicted: CMA\n",
      "token: CMO, predicted: CMA\n",
      "token: CMP, predicted: CMA\n",
      "token: CMT, predicted: CMA\n",
      "token: CMX, predicted: CMA\n",
      "token: CMY, predicted: CMA\n",
      "token: COB, predicted: COA\n",
      "token: COC, predicted: COA\n",
      "token: COE, predicted: COA\n",
      "token: COH, predicted: COA\n",
      "token: COK, predicted: COA\n",
      "token: COM, predicted: COA\n",
      "token: COO, predicted: COA\n",
      "token: COP, predicted: COA\n",
      "token: COT, predicted: COA\n",
      "token: COX, predicted: COA\n",
      "token: COY, predicted: COA\n",
      "token: CPB, predicted: CPA\n",
      "token: CPC, predicted: CPA\n",
      "token: CPE, predicted: CPA\n",
      "token: CPH, predicted: CPA\n",
      "token: CPK, predicted: CPA\n",
      "token: CPM, predicted: CPA\n",
      "token: CPO, predicted: CPA\n",
      "token: CPP, predicted: CPA\n",
      "token: CPT, predicted: CPA\n",
      "token: CPX, predicted: CPA\n",
      "token: CPY, predicted: CPA\n",
      "token: CTB, predicted: CTA\n",
      "token: CTC, predicted: CTA\n",
      "token: CTE, predicted: CTA\n",
      "token: CTH, predicted: CTA\n",
      "token: CTK, predicted: CTA\n",
      "token: CTM, predicted: CTA\n",
      "token: CTO, predicted: CTA\n",
      "token: CTP, predicted: CTA\n",
      "token: CTT, predicted: CTA\n",
      "token: CTX, predicted: CTA\n",
      "token: CTY, predicted: CTA\n",
      "token: CXB, predicted: CXA\n",
      "token: CXC, predicted: CXA\n",
      "token: CXE, predicted: CXA\n",
      "token: CXH, predicted: CXA\n",
      "token: CXK, predicted: CXA\n",
      "token: CXM, predicted: CXA\n",
      "token: CXO, predicted: CXA\n",
      "token: CXP, predicted: CXA\n",
      "token: CXT, predicted: CXA\n",
      "token: CXX, predicted: CXA\n",
      "token: CXY, predicted: CXA\n",
      "token: CYB, predicted: CYA\n",
      "token: CYC, predicted: CYA\n",
      "token: CYE, predicted: CYA\n",
      "token: CYH, predicted: CYA\n",
      "token: CYK, predicted: CYA\n",
      "token: CYM, predicted: CYA\n",
      "token: CYO, predicted: CYA\n",
      "token: CYP, predicted: CYA\n",
      "token: CYT, predicted: CYA\n",
      "token: CYX, predicted: CYA\n",
      "token: CYY, predicted: CYA\n",
      "token: EAB, predicted: EAA\n",
      "token: EAC, predicted: EAA\n",
      "token: EAE, predicted: EAA\n",
      "token: EAH, predicted: EAA\n",
      "token: EAK, predicted: EAA\n",
      "token: EAM, predicted: EAA\n",
      "token: EAO, predicted: EAA\n",
      "token: EAP, predicted: EAA\n",
      "token: EAT, predicted: EAA\n",
      "token: EAX, predicted: EAA\n",
      "token: EAY, predicted: EAA\n",
      "token: EBB, predicted: EBA\n",
      "token: EBC, predicted: EBA\n",
      "token: EBE, predicted: EBA\n",
      "token: EBH, predicted: EBA\n",
      "token: EBK, predicted: EBA\n",
      "token: EBM, predicted: EBA\n",
      "token: EBO, predicted: EBA\n",
      "token: EBP, predicted: EBA\n",
      "token: EBT, predicted: EBA\n",
      "token: EBX, predicted: EBA\n",
      "token: EBY, predicted: EBA\n",
      "token: ECB, predicted: ECA\n",
      "token: ECC, predicted: ECA\n",
      "token: ECE, predicted: ECA\n",
      "token: ECH, predicted: ECA\n",
      "token: ECK, predicted: ECA\n",
      "token: ECM, predicted: ECA\n",
      "token: ECO, predicted: ECA\n",
      "token: ECP, predicted: ECA\n",
      "token: ECT, predicted: ECA\n",
      "token: ECX, predicted: ECA\n",
      "token: ECY, predicted: ECA\n",
      "token: EEB, predicted: EEA\n",
      "token: EEC, predicted: EEA\n",
      "token: EEE, predicted: EEA\n",
      "token: EEH, predicted: EEA\n",
      "token: EEK, predicted: EEA\n",
      "token: EEM, predicted: EEA\n",
      "token: EEO, predicted: EEA\n",
      "token: EEP, predicted: EEA\n",
      "token: EET, predicted: EEA\n",
      "token: EEX, predicted: EEA\n",
      "token: EEY, predicted: EEA\n",
      "token: EHB, predicted: EHA\n",
      "token: EHC, predicted: EHA\n",
      "token: EHE, predicted: EHA\n",
      "token: EHH, predicted: EHA\n",
      "token: EHK, predicted: EHA\n",
      "token: EHM, predicted: EHA\n",
      "token: EHO, predicted: EHA\n",
      "token: EHP, predicted: EHA\n",
      "token: EHT, predicted: EHA\n",
      "token: EHX, predicted: EHA\n",
      "token: EHY, predicted: EHA\n",
      "token: EKB, predicted: EKA\n",
      "token: EKC, predicted: EKA\n",
      "token: EKE, predicted: EKA\n",
      "token: EKH, predicted: EKA\n",
      "token: EKK, predicted: EKA\n",
      "token: EKM, predicted: EKA\n",
      "token: EKO, predicted: EKA\n",
      "token: EKP, predicted: EKA\n",
      "token: EKT, predicted: EKA\n",
      "token: EKX, predicted: EKA\n",
      "token: EKY, predicted: EKA\n",
      "token: EMB, predicted: EMA\n",
      "token: EMC, predicted: EMA\n",
      "token: EME, predicted: EMA\n",
      "token: EMH, predicted: EMA\n",
      "token: EMK, predicted: EMA\n",
      "token: EMM, predicted: EMA\n",
      "token: EMO, predicted: EMA\n",
      "token: EMP, predicted: EMA\n",
      "token: EMT, predicted: EMA\n",
      "token: EMX, predicted: EMA\n",
      "token: EMY, predicted: EMA\n",
      "token: EOB, predicted: EOA\n",
      "token: EOC, predicted: EOA\n",
      "token: EOE, predicted: EOA\n",
      "token: EOH, predicted: EOA\n",
      "token: EOK, predicted: EOA\n",
      "token: EOM, predicted: EOA\n",
      "token: EOO, predicted: EOA\n",
      "token: EOP, predicted: EOA\n",
      "token: EOT, predicted: EOA\n",
      "token: EOX, predicted: EOA\n",
      "token: EOY, predicted: EOA\n",
      "token: EPB, predicted: EPA\n",
      "token: EPC, predicted: EPA\n",
      "token: EPE, predicted: EPA\n",
      "token: EPH, predicted: EPA\n",
      "token: EPK, predicted: EPA\n",
      "token: EPM, predicted: EPA\n",
      "token: EPO, predicted: EPA\n",
      "token: EPP, predicted: EPA\n",
      "token: EPT, predicted: EPA\n",
      "token: EPX, predicted: EPA\n",
      "token: EPY, predicted: EPA\n",
      "token: ETB, predicted: ETA\n",
      "token: ETC, predicted: ETA\n",
      "token: ETE, predicted: ETA\n",
      "token: ETH, predicted: ETA\n",
      "token: ETK, predicted: ETA\n",
      "token: ETM, predicted: ETA\n",
      "token: ETO, predicted: ETA\n",
      "token: ETP, predicted: ETA\n",
      "token: ETT, predicted: ETA\n",
      "token: ETX, predicted: ETA\n",
      "token: ETY, predicted: ETA\n",
      "token: EXB, predicted: EXA\n",
      "token: EXC, predicted: EXA\n",
      "token: EXE, predicted: EXA\n",
      "token: EXH, predicted: EXA\n",
      "token: EXK, predicted: EXA\n",
      "token: EXM, predicted: EXA\n",
      "token: EXO, predicted: EXA\n",
      "token: EXP, predicted: EXA\n",
      "token: EXT, predicted: EXA\n",
      "token: EXX, predicted: EXA\n",
      "token: EXY, predicted: EXA\n",
      "token: EYB, predicted: EYA\n",
      "token: EYC, predicted: EYA\n",
      "token: EYE, predicted: EYA\n",
      "token: EYH, predicted: EYA\n",
      "token: EYK, predicted: EYA\n",
      "token: EYM, predicted: EYA\n",
      "token: EYO, predicted: EYA\n",
      "token: EYP, predicted: EYA\n",
      "token: EYT, predicted: EYA\n",
      "token: EYX, predicted: EYA\n",
      "token: EYY, predicted: EYA\n",
      "token: HAB, predicted: HAA\n",
      "token: HAC, predicted: HAA\n",
      "token: HAE, predicted: HAA\n",
      "token: HAH, predicted: HAA\n",
      "token: HAK, predicted: HAA\n",
      "token: HAM, predicted: HAA\n",
      "token: HAO, predicted: HAA\n",
      "token: HAP, predicted: HAA\n",
      "token: HAT, predicted: HAA\n",
      "token: HAX, predicted: HAA\n",
      "token: HAY, predicted: HAA\n",
      "token: HBB, predicted: HBA\n",
      "token: HBC, predicted: HBA\n",
      "token: HBE, predicted: HBA\n",
      "token: HBH, predicted: HBA\n",
      "token: HBK, predicted: HBA\n",
      "token: HBM, predicted: HBA\n",
      "token: HBO, predicted: HBA\n",
      "token: HBP, predicted: HBA\n",
      "token: HBT, predicted: HBA\n",
      "token: HBX, predicted: HBA\n",
      "token: HBY, predicted: HBA\n",
      "token: HCB, predicted: HCA\n",
      "token: HCC, predicted: HCA\n",
      "token: HCE, predicted: HCA\n",
      "token: HCH, predicted: HCA\n",
      "token: HCK, predicted: HCA\n",
      "token: HCM, predicted: HCA\n",
      "token: HCO, predicted: HCA\n",
      "token: HCP, predicted: HCA\n",
      "token: HCT, predicted: HCA\n",
      "token: HCX, predicted: HCA\n",
      "token: HCY, predicted: HCA\n",
      "token: HEB, predicted: HEA\n",
      "token: HEC, predicted: HEA\n",
      "token: HEE, predicted: HEA\n",
      "token: HEH, predicted: HEA\n",
      "token: HEK, predicted: HEA\n",
      "token: HEM, predicted: HEA\n",
      "token: HEO, predicted: HEA\n",
      "token: HEP, predicted: HEA\n",
      "token: HET, predicted: HEA\n",
      "token: HEX, predicted: HEA\n",
      "token: HEY, predicted: HEA\n",
      "token: HHB, predicted: HHA\n",
      "token: HHC, predicted: HHA\n",
      "token: HHE, predicted: HHA\n",
      "token: HHH, predicted: HHA\n",
      "token: HHK, predicted: HHA\n",
      "token: HHM, predicted: HHA\n",
      "token: HHO, predicted: HHA\n",
      "token: HHP, predicted: HHA\n",
      "token: HHT, predicted: HHA\n",
      "token: HHX, predicted: HHA\n",
      "token: HHY, predicted: HHA\n",
      "token: HKB, predicted: HKA\n",
      "token: HKC, predicted: HKA\n",
      "token: HKE, predicted: HKA\n",
      "token: HKH, predicted: HKA\n",
      "token: HKK, predicted: HKA\n",
      "token: HKM, predicted: HKA\n",
      "token: HKO, predicted: HKA\n",
      "token: HKP, predicted: HKA\n",
      "token: HKT, predicted: HKA\n",
      "token: HKX, predicted: HKA\n",
      "token: HKY, predicted: HKA\n",
      "token: HMB, predicted: HMA\n",
      "token: HMC, predicted: HMA\n",
      "token: HME, predicted: HMA\n",
      "token: HMH, predicted: HMA\n",
      "token: HMK, predicted: HMA\n",
      "token: HMM, predicted: HMA\n",
      "token: HMO, predicted: HMA\n",
      "token: HMP, predicted: HMA\n",
      "token: HMT, predicted: HMA\n",
      "token: HMX, predicted: HMA\n",
      "token: HMY, predicted: HMA\n",
      "token: HOB, predicted: HOA\n",
      "token: HOC, predicted: HOA\n",
      "token: HOE, predicted: HOA\n",
      "token: HOH, predicted: HOA\n",
      "token: HOK, predicted: HOA\n",
      "token: HOM, predicted: HOA\n",
      "token: HOO, predicted: HOA\n",
      "token: HOP, predicted: HOA\n",
      "token: HOT, predicted: HOA\n",
      "token: HOX, predicted: HOA\n",
      "token: HOY, predicted: HOA\n",
      "token: HPB, predicted: HPA\n",
      "token: HPC, predicted: HPA\n",
      "token: HPE, predicted: HPA\n",
      "token: HPH, predicted: HPA\n",
      "token: HPK, predicted: HPA\n",
      "token: HPM, predicted: HPA\n",
      "token: HPO, predicted: HPA\n",
      "token: HPP, predicted: HPA\n",
      "token: HPT, predicted: HPA\n",
      "token: HPX, predicted: HPA\n",
      "token: HPY, predicted: HPA\n",
      "token: HTB, predicted: HTA\n",
      "token: HTC, predicted: HTA\n",
      "token: HTE, predicted: HTA\n",
      "token: HTH, predicted: HTA\n",
      "token: HTK, predicted: HTA\n",
      "token: HTM, predicted: HTA\n",
      "token: HTO, predicted: HTA\n",
      "token: HTP, predicted: HTA\n",
      "token: HTT, predicted: HTA\n",
      "token: HTX, predicted: HTA\n",
      "token: HTY, predicted: HTA\n",
      "token: HXB, predicted: HXA\n",
      "token: HXC, predicted: HXA\n",
      "token: HXE, predicted: HXA\n",
      "token: HXH, predicted: HXA\n",
      "token: HXK, predicted: HXA\n",
      "token: HXM, predicted: HXA\n",
      "token: HXO, predicted: HXA\n",
      "token: HXP, predicted: HXA\n",
      "token: HXT, predicted: HXA\n",
      "token: HXX, predicted: HXA\n",
      "token: HXY, predicted: HXA\n",
      "token: HYB, predicted: HYA\n",
      "token: HYC, predicted: HYA\n",
      "token: HYE, predicted: HYA\n",
      "token: HYH, predicted: HYA\n",
      "token: HYK, predicted: HYA\n",
      "token: HYM, predicted: HYA\n",
      "token: HYO, predicted: HYA\n",
      "token: HYP, predicted: HYA\n",
      "token: HYT, predicted: HYA\n",
      "token: HYX, predicted: HYA\n",
      "token: HYY, predicted: HYA\n",
      "token: KAB, predicted: KAA\n",
      "token: KAC, predicted: KAA\n",
      "token: KAE, predicted: KAA\n",
      "token: KAH, predicted: KAA\n",
      "token: KAK, predicted: KAA\n",
      "token: KAM, predicted: KAA\n",
      "token: KAO, predicted: KAA\n",
      "token: KAP, predicted: KAA\n",
      "token: KAT, predicted: KAA\n",
      "token: KAX, predicted: KAA\n",
      "token: KAY, predicted: KAA\n",
      "token: KBB, predicted: KBA\n",
      "token: KBC, predicted: KBA\n",
      "token: KBE, predicted: KBA\n",
      "token: KBH, predicted: KBA\n",
      "token: KBK, predicted: KBA\n",
      "token: KBM, predicted: KBA\n",
      "token: KBO, predicted: KBA\n",
      "token: KBP, predicted: KBA\n",
      "token: KBT, predicted: KBA\n",
      "token: KBX, predicted: KBA\n",
      "token: KBY, predicted: KBA\n",
      "token: KCB, predicted: KCA\n",
      "token: KCC, predicted: KCA\n",
      "token: KCE, predicted: KCA\n",
      "token: KCH, predicted: KCA\n",
      "token: KCK, predicted: KCA\n",
      "token: KCM, predicted: KCA\n",
      "token: KCO, predicted: KCA\n",
      "token: KCP, predicted: KCA\n",
      "token: KCT, predicted: KCA\n",
      "token: KCX, predicted: KCA\n",
      "token: KCY, predicted: KCA\n",
      "token: KEB, predicted: KEA\n",
      "token: KEC, predicted: KEA\n",
      "token: KEE, predicted: KEA\n",
      "token: KEH, predicted: KEA\n",
      "token: KEK, predicted: KEA\n",
      "token: KEM, predicted: KEA\n",
      "token: KEO, predicted: KEA\n",
      "token: KEP, predicted: KEA\n",
      "token: KET, predicted: KEA\n",
      "token: KEX, predicted: KEA\n",
      "token: KEY, predicted: KEA\n",
      "token: KHB, predicted: KHA\n",
      "token: KHC, predicted: KHA\n",
      "token: KHE, predicted: KHA\n",
      "token: KHH, predicted: KHA\n",
      "token: KHK, predicted: KHA\n",
      "token: KHM, predicted: KHA\n",
      "token: KHO, predicted: KHA\n",
      "token: KHP, predicted: KHA\n",
      "token: KHT, predicted: KHA\n",
      "token: KHX, predicted: KHA\n",
      "token: KHY, predicted: KHA\n",
      "token: KKB, predicted: KKA\n",
      "token: KKC, predicted: KKA\n",
      "token: KKE, predicted: KKA\n",
      "token: KKH, predicted: KKA\n",
      "token: KKK, predicted: KKA\n",
      "token: KKM, predicted: KKA\n",
      "token: KKO, predicted: KKA\n",
      "token: KKP, predicted: KKA\n",
      "token: KKT, predicted: KKA\n",
      "token: KKX, predicted: KKA\n",
      "token: KKY, predicted: KKA\n",
      "token: KMB, predicted: KMA\n",
      "token: KMC, predicted: KMA\n",
      "token: KME, predicted: KMA\n",
      "token: KMH, predicted: KMA\n",
      "token: KMK, predicted: KMA\n",
      "token: KMM, predicted: KMA\n",
      "token: KMO, predicted: KMA\n",
      "token: KMP, predicted: KMA\n",
      "token: KMT, predicted: KMA\n",
      "token: KMX, predicted: KMA\n",
      "token: KMY, predicted: KMA\n",
      "token: KOB, predicted: KOA\n",
      "token: KOC, predicted: KOA\n",
      "token: KOE, predicted: KOA\n",
      "token: KOH, predicted: KOA\n",
      "token: KOK, predicted: KOA\n",
      "token: KOM, predicted: KOA\n",
      "token: KOO, predicted: KOA\n",
      "token: KOP, predicted: KOA\n",
      "token: KOT, predicted: KOA\n",
      "token: KOX, predicted: KOA\n",
      "token: KOY, predicted: KOA\n",
      "token: KPB, predicted: KPA\n",
      "token: KPC, predicted: KPA\n",
      "token: KPE, predicted: KPA\n",
      "token: KPH, predicted: KPA\n",
      "token: KPK, predicted: KPA\n",
      "token: KPM, predicted: KPA\n",
      "token: KPO, predicted: KPA\n",
      "token: KPP, predicted: KPA\n",
      "token: KPT, predicted: KPA\n",
      "token: KPX, predicted: KPA\n",
      "token: KPY, predicted: KPA\n",
      "token: KTB, predicted: KTA\n",
      "token: KTC, predicted: KTA\n",
      "token: KTE, predicted: KTA\n",
      "token: KTH, predicted: KTA\n",
      "token: KTK, predicted: KTA\n",
      "token: KTM, predicted: KTA\n",
      "token: KTO, predicted: KTA\n",
      "token: KTP, predicted: KTA\n",
      "token: KTT, predicted: KTA\n",
      "token: KTX, predicted: KTA\n",
      "token: KTY, predicted: KTA\n",
      "token: KXB, predicted: KXA\n",
      "token: KXC, predicted: KXA\n",
      "token: KXE, predicted: KXA\n",
      "token: KXH, predicted: KXA\n",
      "token: KXK, predicted: KXA\n",
      "token: KXM, predicted: KXA\n",
      "token: KXO, predicted: KXA\n",
      "token: KXP, predicted: KXA\n",
      "token: KXT, predicted: KXA\n",
      "token: KXX, predicted: KXA\n",
      "token: KXY, predicted: KXA\n",
      "token: KYB, predicted: KYA\n",
      "token: KYC, predicted: KYA\n",
      "token: KYE, predicted: KYA\n",
      "token: KYH, predicted: KYA\n",
      "token: KYK, predicted: KYA\n",
      "token: KYM, predicted: KYA\n",
      "token: KYO, predicted: KYA\n",
      "token: KYP, predicted: KYA\n",
      "token: KYT, predicted: KYA\n",
      "token: KYX, predicted: KYA\n",
      "token: KYY, predicted: KYA\n",
      "token: MAB, predicted: MAA\n",
      "token: MAC, predicted: MAA\n",
      "token: MAE, predicted: MAA\n",
      "token: MAH, predicted: MAA\n",
      "token: MAK, predicted: MAA\n",
      "token: MAM, predicted: MAA\n",
      "token: MAO, predicted: MAA\n",
      "token: MAP, predicted: MAA\n",
      "token: MAT, predicted: MAA\n",
      "token: MAX, predicted: MAA\n",
      "token: MAY, predicted: MAA\n",
      "token: MBB, predicted: MBA\n",
      "token: MBC, predicted: MBA\n",
      "token: MBE, predicted: MBA\n",
      "token: MBH, predicted: MBA\n",
      "token: MBK, predicted: MBA\n",
      "token: MBM, predicted: MBA\n",
      "token: MBO, predicted: MBA\n",
      "token: MBP, predicted: MBA\n",
      "token: MBT, predicted: MBA\n",
      "token: MBX, predicted: MBA\n",
      "token: MBY, predicted: MBA\n",
      "token: MCB, predicted: MCA\n",
      "token: MCC, predicted: MCA\n",
      "token: MCE, predicted: MCA\n",
      "token: MCH, predicted: MCA\n",
      "token: MCK, predicted: MCA\n",
      "token: MCM, predicted: MCA\n",
      "token: MCO, predicted: MCA\n",
      "token: MCP, predicted: MCA\n",
      "token: MCT, predicted: MCA\n",
      "token: MCX, predicted: MCA\n",
      "token: MCY, predicted: MCA\n",
      "token: MEB, predicted: MEA\n",
      "token: MEC, predicted: MEA\n",
      "token: MEE, predicted: MEA\n",
      "token: MEH, predicted: MEA\n",
      "token: MEK, predicted: MEA\n",
      "token: MEM, predicted: MEA\n",
      "token: MEO, predicted: MEA\n",
      "token: MEP, predicted: MEA\n",
      "token: MET, predicted: MEA\n",
      "token: MEX, predicted: MEA\n",
      "token: MEY, predicted: MEA\n",
      "token: MHB, predicted: MHA\n",
      "token: MHC, predicted: MHA\n",
      "token: MHE, predicted: MHA\n",
      "token: MHH, predicted: MHA\n",
      "token: MHK, predicted: MHA\n",
      "token: MHM, predicted: MHA\n",
      "token: MHO, predicted: MHA\n",
      "token: MHP, predicted: MHA\n",
      "token: MHT, predicted: MHA\n",
      "token: MHX, predicted: MHA\n",
      "token: MHY, predicted: MHA\n",
      "token: MKB, predicted: MKA\n",
      "token: MKC, predicted: MKA\n",
      "token: MKE, predicted: MKA\n",
      "token: MKH, predicted: MKA\n",
      "token: MKK, predicted: MKA\n",
      "token: MKM, predicted: MKA\n",
      "token: MKO, predicted: MKA\n",
      "token: MKP, predicted: MKA\n",
      "token: MKT, predicted: MKA\n",
      "token: MKX, predicted: MKA\n",
      "token: MKY, predicted: MKA\n",
      "token: MMB, predicted: MMA\n",
      "token: MMC, predicted: MMA\n",
      "token: MME, predicted: MMA\n",
      "token: MMH, predicted: MMA\n",
      "token: MMK, predicted: MMA\n",
      "token: MMM, predicted: MMA\n",
      "token: MMO, predicted: MMA\n",
      "token: MMP, predicted: MMA\n",
      "token: MMT, predicted: MMA\n",
      "token: MMX, predicted: MMA\n",
      "token: MMY, predicted: MMA\n",
      "token: MOB, predicted: MOA\n",
      "token: MOC, predicted: MOA\n",
      "token: MOE, predicted: MOA\n",
      "token: MOH, predicted: MOA\n",
      "token: MOK, predicted: MOA\n",
      "token: MOM, predicted: MOA\n",
      "token: MOO, predicted: MOA\n",
      "token: MOP, predicted: MOA\n",
      "token: MOT, predicted: MOA\n",
      "token: MOX, predicted: MOA\n",
      "token: MOY, predicted: MOA\n",
      "token: MPB, predicted: MPA\n",
      "token: MPC, predicted: MPA\n",
      "token: MPE, predicted: MPA\n",
      "token: MPH, predicted: MPA\n",
      "token: MPK, predicted: MPA\n",
      "token: MPM, predicted: MPA\n",
      "token: MPO, predicted: MPA\n",
      "token: MPP, predicted: MPA\n",
      "token: MPT, predicted: MPA\n",
      "token: MPX, predicted: MPA\n",
      "token: MPY, predicted: MPA\n",
      "token: MTB, predicted: MTA\n",
      "token: MTC, predicted: MTA\n",
      "token: MTE, predicted: MTA\n",
      "token: MTH, predicted: MTA\n",
      "token: MTK, predicted: MTA\n",
      "token: MTM, predicted: MTA\n",
      "token: MTO, predicted: MTA\n",
      "token: MTP, predicted: MTA\n",
      "token: MTT, predicted: MTA\n",
      "token: MTX, predicted: MTA\n",
      "token: MTY, predicted: MTA\n",
      "token: MXB, predicted: MXA\n",
      "token: MXC, predicted: MXA\n",
      "token: MXE, predicted: MXA\n",
      "token: MXH, predicted: MXA\n",
      "token: MXK, predicted: MXA\n",
      "token: MXM, predicted: MXA\n",
      "token: MXO, predicted: MXA\n",
      "token: MXP, predicted: MXA\n",
      "token: MXT, predicted: MXA\n",
      "token: MXX, predicted: MXA\n",
      "token: MXY, predicted: MXA\n",
      "token: MYB, predicted: MYA\n",
      "token: MYC, predicted: MYA\n",
      "token: MYE, predicted: MYA\n",
      "token: MYH, predicted: MYA\n",
      "token: MYK, predicted: MYA\n",
      "token: MYM, predicted: MYA\n",
      "token: MYO, predicted: MYA\n",
      "token: MYP, predicted: MYA\n",
      "token: MYT, predicted: MYA\n",
      "token: MYX, predicted: MYA\n",
      "token: MYY, predicted: MYA\n",
      "token: OAB, predicted: OAA\n",
      "token: OAC, predicted: OAA\n",
      "token: OAE, predicted: OAA\n",
      "token: OAH, predicted: OAA\n",
      "token: OAK, predicted: OAA\n",
      "token: OAM, predicted: OAA\n",
      "token: OAO, predicted: OAA\n",
      "token: OAP, predicted: OAA\n",
      "token: OAT, predicted: OAA\n",
      "token: OAX, predicted: OAA\n",
      "token: OAY, predicted: OAA\n",
      "token: OBB, predicted: OBA\n",
      "token: OBC, predicted: OBA\n",
      "token: OBE, predicted: OBA\n",
      "token: OBH, predicted: OBA\n",
      "token: OBK, predicted: OBA\n",
      "token: OBM, predicted: OBA\n",
      "token: OBO, predicted: OBA\n",
      "token: OBP, predicted: OBA\n",
      "token: OBT, predicted: OBA\n",
      "token: OBX, predicted: OBA\n",
      "token: OBY, predicted: OBA\n",
      "token: OCB, predicted: OCA\n",
      "token: OCC, predicted: OCA\n",
      "token: OCE, predicted: OCA\n",
      "token: OCH, predicted: OCA\n",
      "token: OCK, predicted: OCA\n",
      "token: OCM, predicted: OCA\n",
      "token: OCO, predicted: OCA\n",
      "token: OCP, predicted: OCA\n",
      "token: OCT, predicted: OCA\n",
      "token: OCX, predicted: OCA\n",
      "token: OCY, predicted: OCA\n",
      "token: OEB, predicted: OEA\n",
      "token: OEC, predicted: OEA\n",
      "token: OEE, predicted: OEA\n",
      "token: OEH, predicted: OEA\n",
      "token: OEK, predicted: OEA\n",
      "token: OEM, predicted: OEA\n",
      "token: OEO, predicted: OEA\n",
      "token: OEP, predicted: OEA\n",
      "token: OET, predicted: OEA\n",
      "token: OEX, predicted: OEA\n",
      "token: OEY, predicted: OEA\n",
      "token: OHB, predicted: OHA\n",
      "token: OHC, predicted: OHA\n",
      "token: OHE, predicted: OHA\n",
      "token: OHH, predicted: OHA\n",
      "token: OHK, predicted: OHA\n",
      "token: OHM, predicted: OHA\n",
      "token: OHO, predicted: OHA\n",
      "token: OHP, predicted: OHA\n",
      "token: OHT, predicted: OHA\n",
      "token: OHX, predicted: OHA\n",
      "token: OHY, predicted: OHA\n",
      "token: OKB, predicted: OKA\n",
      "token: OKC, predicted: OKA\n",
      "token: OKE, predicted: OKA\n",
      "token: OKH, predicted: OKA\n",
      "token: OKK, predicted: OKA\n",
      "token: OKM, predicted: OKA\n",
      "token: OKO, predicted: OKA\n",
      "token: OKP, predicted: OKA\n",
      "token: OKT, predicted: OKA\n",
      "token: OKX, predicted: OKA\n",
      "token: OKY, predicted: OKA\n",
      "token: OMB, predicted: OMA\n",
      "token: OMC, predicted: OMA\n",
      "token: OME, predicted: OMA\n",
      "token: OMH, predicted: OMA\n",
      "token: OMK, predicted: OMA\n",
      "token: OMM, predicted: OMA\n",
      "token: OMO, predicted: OMA\n",
      "token: OMP, predicted: OMA\n",
      "token: OMT, predicted: OMA\n",
      "token: OMX, predicted: OMA\n",
      "token: OMY, predicted: OMA\n",
      "token: OOB, predicted: OOA\n",
      "token: OOC, predicted: OOA\n",
      "token: OOE, predicted: OOA\n",
      "token: OOH, predicted: OOA\n",
      "token: OOK, predicted: OOA\n",
      "token: OOM, predicted: OOA\n",
      "token: OOO, predicted: OOA\n",
      "token: OOP, predicted: OOA\n",
      "token: OOT, predicted: OOA\n",
      "token: OOX, predicted: OOA\n",
      "token: OOY, predicted: OOA\n",
      "token: OPB, predicted: OPA\n",
      "token: OPC, predicted: OPA\n",
      "token: OPE, predicted: OPA\n",
      "token: OPH, predicted: OPA\n",
      "token: OPK, predicted: OPA\n",
      "token: OPM, predicted: OPA\n",
      "token: OPO, predicted: OPA\n",
      "token: OPP, predicted: OPA\n",
      "token: OPT, predicted: OPA\n",
      "token: OPX, predicted: OPA\n",
      "token: OPY, predicted: OPA\n",
      "token: OTB, predicted: OTA\n",
      "token: OTC, predicted: OTA\n",
      "token: OTE, predicted: OTA\n",
      "token: OTH, predicted: OTA\n",
      "token: OTK, predicted: OTA\n",
      "token: OTM, predicted: OTA\n",
      "token: OTO, predicted: OTA\n",
      "token: OTP, predicted: OTA\n",
      "token: OTT, predicted: OTA\n",
      "token: OTX, predicted: OTA\n",
      "token: OTY, predicted: OTA\n",
      "token: OXB, predicted: OXA\n",
      "token: OXC, predicted: OXA\n",
      "token: OXE, predicted: OXA\n",
      "token: OXH, predicted: OXA\n",
      "token: OXK, predicted: OXA\n",
      "token: OXM, predicted: OXA\n",
      "token: OXO, predicted: OXA\n",
      "token: OXP, predicted: OXA\n",
      "token: OXT, predicted: OXA\n",
      "token: OXX, predicted: OXA\n",
      "token: OXY, predicted: OXA\n",
      "token: OYB, predicted: OYA\n",
      "token: OYC, predicted: OYA\n",
      "token: OYE, predicted: OYA\n",
      "token: OYH, predicted: OYA\n",
      "token: OYK, predicted: OYA\n",
      "token: OYM, predicted: OYA\n",
      "token: OYO, predicted: OYA\n",
      "token: OYP, predicted: OYA\n",
      "token: OYT, predicted: OYA\n",
      "token: OYX, predicted: OYA\n",
      "token: OYY, predicted: OYA\n",
      "token: PAB, predicted: PAA\n",
      "token: PAC, predicted: PAA\n",
      "token: PAE, predicted: PAA\n",
      "token: PAH, predicted: PAA\n",
      "token: PAK, predicted: PAA\n",
      "token: PAM, predicted: PAA\n",
      "token: PAO, predicted: PAA\n",
      "token: PAP, predicted: PAA\n",
      "token: PAT, predicted: PAA\n",
      "token: PAX, predicted: PAA\n",
      "token: PAY, predicted: PAA\n",
      "token: PBB, predicted: PBA\n",
      "token: PBC, predicted: PBA\n",
      "token: PBE, predicted: PBA\n",
      "token: PBH, predicted: PBA\n",
      "token: PBK, predicted: PBA\n",
      "token: PBM, predicted: PBA\n",
      "token: PBO, predicted: PBA\n",
      "token: PBP, predicted: PBA\n",
      "token: PBT, predicted: PBA\n",
      "token: PBX, predicted: PBA\n",
      "token: PBY, predicted: PBA\n",
      "token: PCB, predicted: PCA\n",
      "token: PCC, predicted: PCA\n",
      "token: PCE, predicted: PCA\n",
      "token: PCH, predicted: PCA\n",
      "token: PCK, predicted: PCA\n",
      "token: PCM, predicted: PCA\n",
      "token: PCO, predicted: PCA\n",
      "token: PCP, predicted: PCA\n",
      "token: PCT, predicted: PCA\n",
      "token: PCX, predicted: PCA\n",
      "token: PCY, predicted: PCA\n",
      "token: PEB, predicted: PEA\n",
      "token: PEC, predicted: PEA\n",
      "token: PEE, predicted: PEA\n",
      "token: PEH, predicted: PEA\n",
      "token: PEK, predicted: PEA\n",
      "token: PEM, predicted: PEA\n",
      "token: PEO, predicted: PEA\n",
      "token: PEP, predicted: PEA\n",
      "token: PET, predicted: PEA\n",
      "token: PEX, predicted: PEA\n",
      "token: PEY, predicted: PEA\n",
      "token: PHB, predicted: PHA\n",
      "token: PHC, predicted: PHA\n",
      "token: PHE, predicted: PHA\n",
      "token: PHH, predicted: PHA\n",
      "token: PHK, predicted: PHA\n",
      "token: PHM, predicted: PHA\n",
      "token: PHO, predicted: PHA\n",
      "token: PHP, predicted: PHA\n",
      "token: PHT, predicted: PHA\n",
      "token: PHX, predicted: PHA\n",
      "token: PHY, predicted: PHA\n",
      "token: PKB, predicted: PKA\n",
      "token: PKC, predicted: PKA\n",
      "token: PKE, predicted: PKA\n",
      "token: PKH, predicted: PKA\n",
      "token: PKK, predicted: PKA\n",
      "token: PKM, predicted: PKA\n",
      "token: PKO, predicted: PKA\n",
      "token: PKP, predicted: PKA\n",
      "token: PKT, predicted: PKA\n",
      "token: PKX, predicted: PKA\n",
      "token: PKY, predicted: PKA\n",
      "token: PMB, predicted: PMA\n",
      "token: PMC, predicted: PMA\n",
      "token: PME, predicted: PMA\n",
      "token: PMH, predicted: PMA\n",
      "token: PMK, predicted: PMA\n",
      "token: PMM, predicted: PMA\n",
      "token: PMO, predicted: PMA\n",
      "token: PMP, predicted: PMA\n",
      "token: PMT, predicted: PMA\n",
      "token: PMX, predicted: PMA\n",
      "token: PMY, predicted: PMA\n",
      "token: POB, predicted: POA\n",
      "token: POC, predicted: POA\n",
      "token: POE, predicted: POA\n",
      "token: POH, predicted: POA\n",
      "token: POK, predicted: POA\n",
      "token: POM, predicted: POA\n",
      "token: POO, predicted: POA\n",
      "token: POP, predicted: POA\n",
      "token: POT, predicted: POA\n",
      "token: POX, predicted: POA\n",
      "token: POY, predicted: POA\n",
      "token: PPB, predicted: PPA\n",
      "token: PPC, predicted: PPA\n",
      "token: PPE, predicted: PPA\n",
      "token: PPH, predicted: PPA\n",
      "token: PPK, predicted: PPA\n",
      "token: PPM, predicted: PPA\n",
      "token: PPO, predicted: PPA\n",
      "token: PPP, predicted: PPA\n",
      "token: PPT, predicted: PPA\n",
      "token: PPX, predicted: PPA\n",
      "token: PPY, predicted: PPA\n",
      "token: PTB, predicted: PTA\n",
      "token: PTC, predicted: PTA\n",
      "token: PTE, predicted: PTA\n",
      "token: PTH, predicted: PTA\n",
      "token: PTK, predicted: PTA\n",
      "token: PTM, predicted: PTA\n",
      "token: PTO, predicted: PTA\n",
      "token: PTP, predicted: PTA\n",
      "token: PTT, predicted: PTA\n",
      "token: PTX, predicted: PTA\n",
      "token: PTY, predicted: PTA\n",
      "token: PXB, predicted: PXA\n",
      "token: PXC, predicted: PXA\n",
      "token: PXE, predicted: PXA\n",
      "token: PXH, predicted: PXA\n",
      "token: PXK, predicted: PXA\n",
      "token: PXM, predicted: PXA\n",
      "token: PXO, predicted: PXA\n",
      "token: PXP, predicted: PXA\n",
      "token: PXT, predicted: PXA\n",
      "token: PXX, predicted: PXA\n",
      "token: PXY, predicted: PXA\n",
      "token: PYB, predicted: PYA\n",
      "token: PYC, predicted: PYA\n",
      "token: PYE, predicted: PYA\n",
      "token: PYH, predicted: PYA\n",
      "token: PYK, predicted: PYA\n",
      "token: PYM, predicted: PYA\n",
      "token: PYO, predicted: PYA\n",
      "token: PYP, predicted: PYA\n",
      "token: PYT, predicted: PYA\n",
      "token: PYX, predicted: PYA\n",
      "token: PYY, predicted: PYA\n",
      "token: TAB, predicted: TAA\n",
      "token: TAC, predicted: TAA\n",
      "token: TAE, predicted: TAA\n",
      "token: TAH, predicted: TAA\n",
      "token: TAK, predicted: TAA\n",
      "token: TAM, predicted: TAA\n",
      "token: TAO, predicted: TAA\n",
      "token: TAP, predicted: TAA\n",
      "token: TAT, predicted: TAA\n",
      "token: TAX, predicted: TAA\n",
      "token: TAY, predicted: TAA\n",
      "token: TBB, predicted: TBA\n",
      "token: TBC, predicted: TBA\n",
      "token: TBE, predicted: TBA\n",
      "token: TBH, predicted: TBA\n",
      "token: TBK, predicted: TBA\n",
      "token: TBM, predicted: TBA\n",
      "token: TBO, predicted: TBA\n",
      "token: TBP, predicted: TBA\n",
      "token: TBT, predicted: TBA\n",
      "token: TBX, predicted: TBA\n",
      "token: TBY, predicted: TBA\n",
      "token: TCB, predicted: TCA\n",
      "token: TCC, predicted: TCA\n",
      "token: TCE, predicted: TCA\n",
      "token: TCH, predicted: TCA\n",
      "token: TCK, predicted: TCA\n",
      "token: TCM, predicted: TCA\n",
      "token: TCO, predicted: TCA\n",
      "token: TCP, predicted: TCA\n",
      "token: TCT, predicted: TCA\n",
      "token: TCX, predicted: TCA\n",
      "token: TCY, predicted: TCA\n",
      "token: TEB, predicted: TEA\n",
      "token: TEC, predicted: TEA\n",
      "token: TEE, predicted: TEA\n",
      "token: TEH, predicted: TEA\n",
      "token: TEK, predicted: TEA\n",
      "token: TEM, predicted: TEA\n",
      "token: TEO, predicted: TEA\n",
      "token: TEP, predicted: TEA\n",
      "token: TET, predicted: TEA\n",
      "token: TEX, predicted: TEA\n",
      "token: TEY, predicted: TEA\n",
      "token: THB, predicted: THA\n",
      "token: THC, predicted: THA\n",
      "token: THE, predicted: THA\n",
      "token: THH, predicted: THA\n",
      "token: THK, predicted: THA\n",
      "token: THM, predicted: THA\n",
      "token: THO, predicted: THA\n",
      "token: THP, predicted: THA\n",
      "token: THT, predicted: THA\n",
      "token: THX, predicted: THA\n",
      "token: THY, predicted: THA\n",
      "token: TKB, predicted: TKA\n",
      "token: TKC, predicted: TKA\n",
      "token: TKE, predicted: TKA\n",
      "token: TKH, predicted: TKA\n",
      "token: TKK, predicted: TKA\n",
      "token: TKM, predicted: TKA\n",
      "token: TKO, predicted: TKA\n",
      "token: TKP, predicted: TKA\n",
      "token: TKT, predicted: TKA\n",
      "token: TKX, predicted: TKA\n",
      "token: TKY, predicted: TKA\n",
      "token: TMB, predicted: TMA\n",
      "token: TMC, predicted: TMA\n",
      "token: TME, predicted: TMA\n",
      "token: TMH, predicted: TMA\n",
      "token: TMK, predicted: TMA\n",
      "token: TMM, predicted: TMA\n",
      "token: TMO, predicted: TMA\n",
      "token: TMP, predicted: TMA\n",
      "token: TMT, predicted: TMA\n",
      "token: TMX, predicted: TMA\n",
      "token: TMY, predicted: TMA\n",
      "token: TOB, predicted: TOA\n",
      "token: TOC, predicted: TOA\n",
      "token: TOE, predicted: TOA\n",
      "token: TOH, predicted: TOA\n",
      "token: TOK, predicted: TOA\n",
      "token: TOM, predicted: TOA\n",
      "token: TOO, predicted: TOA\n",
      "token: TOP, predicted: TOA\n",
      "token: TOT, predicted: TOA\n",
      "token: TOX, predicted: TOA\n",
      "token: TOY, predicted: TOA\n",
      "token: TPB, predicted: TPA\n",
      "token: TPC, predicted: TPA\n",
      "token: TPE, predicted: TPA\n",
      "token: TPH, predicted: TPA\n",
      "token: TPK, predicted: TPA\n",
      "token: TPM, predicted: TPA\n",
      "token: TPO, predicted: TPA\n",
      "token: TPP, predicted: TPA\n",
      "token: TPT, predicted: TPA\n",
      "token: TPX, predicted: TPA\n",
      "token: TPY, predicted: TPA\n",
      "token: TTB, predicted: TTA\n",
      "token: TTC, predicted: TTA\n",
      "token: TTE, predicted: TTA\n",
      "token: TTH, predicted: TTA\n",
      "token: TTK, predicted: TTA\n",
      "token: TTM, predicted: TTA\n",
      "token: TTO, predicted: TTA\n",
      "token: TTP, predicted: TTA\n",
      "token: TTT, predicted: TTA\n",
      "token: TTX, predicted: TTA\n",
      "token: TTY, predicted: TTA\n",
      "token: TXB, predicted: TXA\n",
      "token: TXC, predicted: TXA\n",
      "token: TXE, predicted: TXA\n",
      "token: TXH, predicted: TXA\n",
      "token: TXK, predicted: TXA\n",
      "token: TXM, predicted: TXA\n",
      "token: TXO, predicted: TXA\n",
      "token: TXP, predicted: TXA\n",
      "token: TXT, predicted: TXA\n",
      "token: TXX, predicted: TXA\n",
      "token: TXY, predicted: TXA\n",
      "token: TYB, predicted: TYA\n",
      "token: TYC, predicted: TYA\n",
      "token: TYE, predicted: TYA\n",
      "token: TYH, predicted: TYA\n",
      "token: TYK, predicted: TYA\n",
      "token: TYM, predicted: TYA\n",
      "token: TYO, predicted: TYA\n",
      "token: TYP, predicted: TYA\n",
      "token: TYT, predicted: TYA\n",
      "token: TYX, predicted: TYA\n",
      "token: TYY, predicted: TYA\n",
      "token: XAB, predicted: XAA\n",
      "token: XAC, predicted: XAA\n",
      "token: XAE, predicted: XAA\n",
      "token: XAH, predicted: XAA\n",
      "token: XAK, predicted: XAA\n",
      "token: XAM, predicted: XAA\n",
      "token: XAO, predicted: XAA\n",
      "token: XAP, predicted: XAA\n",
      "token: XAT, predicted: XAA\n",
      "token: XAX, predicted: XAA\n",
      "token: XAY, predicted: XAA\n",
      "token: XBB, predicted: XBA\n",
      "token: XBC, predicted: XBA\n",
      "token: XBE, predicted: XBA\n",
      "token: XBH, predicted: XBA\n",
      "token: XBK, predicted: XBA\n",
      "token: XBM, predicted: XBA\n",
      "token: XBO, predicted: XBA\n",
      "token: XBP, predicted: XBA\n",
      "token: XBT, predicted: XBA\n",
      "token: XBX, predicted: XBA\n",
      "token: XBY, predicted: XBA\n",
      "token: XCB, predicted: XCA\n",
      "token: XCC, predicted: XCA\n",
      "token: XCE, predicted: XCA\n",
      "token: XCH, predicted: XCA\n",
      "token: XCK, predicted: XCA\n",
      "token: XCM, predicted: XCA\n",
      "token: XCO, predicted: XCA\n",
      "token: XCP, predicted: XCA\n",
      "token: XCT, predicted: XCA\n",
      "token: XCX, predicted: XCA\n",
      "token: XCY, predicted: XCA\n",
      "token: XEB, predicted: XEA\n",
      "token: XEC, predicted: XEA\n",
      "token: XEE, predicted: XEA\n",
      "token: XEH, predicted: XEA\n",
      "token: XEK, predicted: XEA\n",
      "token: XEM, predicted: XEA\n",
      "token: XEO, predicted: XEA\n",
      "token: XEP, predicted: XEA\n",
      "token: XET, predicted: XEA\n",
      "token: XEX, predicted: XEA\n",
      "token: XEY, predicted: XEA\n",
      "token: XHB, predicted: XHA\n",
      "token: XHC, predicted: XHA\n",
      "token: XHE, predicted: XHA\n",
      "token: XHH, predicted: XHA\n",
      "token: XHK, predicted: XHA\n",
      "token: XHM, predicted: XHA\n",
      "token: XHO, predicted: XHA\n",
      "token: XHP, predicted: XHA\n",
      "token: XHT, predicted: XHA\n",
      "token: XHX, predicted: XHA\n",
      "token: XHY, predicted: XHA\n",
      "token: XKB, predicted: XKA\n",
      "token: XKC, predicted: XKA\n",
      "token: XKE, predicted: XKA\n",
      "token: XKH, predicted: XKA\n",
      "token: XKK, predicted: XKA\n",
      "token: XKM, predicted: XKA\n",
      "token: XKO, predicted: XKA\n",
      "token: XKP, predicted: XKA\n",
      "token: XKT, predicted: XKA\n",
      "token: XKX, predicted: XKA\n",
      "token: XKY, predicted: XKA\n",
      "token: XMB, predicted: XMA\n",
      "token: XMC, predicted: XMA\n",
      "token: XME, predicted: XMA\n",
      "token: XMH, predicted: XMA\n",
      "token: XMK, predicted: XMA\n",
      "token: XMM, predicted: XMA\n",
      "token: XMO, predicted: XMA\n",
      "token: XMP, predicted: XMA\n",
      "token: XMT, predicted: XMA\n",
      "token: XMX, predicted: XMA\n",
      "token: XMY, predicted: XMA\n",
      "token: XOB, predicted: XOA\n",
      "token: XOC, predicted: XOA\n",
      "token: XOE, predicted: XOA\n",
      "token: XOH, predicted: XOA\n",
      "token: XOK, predicted: XOA\n",
      "token: XOM, predicted: XOA\n",
      "token: XOO, predicted: XOA\n",
      "token: XOP, predicted: XOA\n",
      "token: XOT, predicted: XOA\n",
      "token: XOX, predicted: XOA\n",
      "token: XOY, predicted: XOA\n",
      "token: XPB, predicted: XPA\n",
      "token: XPC, predicted: XPA\n",
      "token: XPE, predicted: XPA\n",
      "token: XPH, predicted: XPA\n",
      "token: XPK, predicted: XPA\n",
      "token: XPM, predicted: XPA\n",
      "token: XPO, predicted: XPA\n",
      "token: XPP, predicted: XPA\n",
      "token: XPT, predicted: XPA\n",
      "token: XPX, predicted: XPA\n",
      "token: XPY, predicted: XPA\n",
      "token: XTB, predicted: XTA\n",
      "token: XTC, predicted: XTA\n",
      "token: XTE, predicted: XTA\n",
      "token: XTH, predicted: XTA\n",
      "token: XTK, predicted: XTA\n",
      "token: XTM, predicted: XTA\n",
      "token: XTO, predicted: XTA\n",
      "token: XTP, predicted: XTA\n",
      "token: XTT, predicted: XTA\n",
      "token: XTX, predicted: XTA\n",
      "token: XTY, predicted: XTA\n",
      "token: XXB, predicted: XXA\n",
      "token: XXC, predicted: XXA\n",
      "token: XXE, predicted: XXA\n",
      "token: XXH, predicted: XXA\n",
      "token: XXK, predicted: XXA\n",
      "token: XXM, predicted: XXA\n",
      "token: XXO, predicted: XXA\n",
      "token: XXP, predicted: XXA\n",
      "token: XXT, predicted: XXA\n",
      "token: XXX, predicted: XXA\n",
      "token: XXY, predicted: XXA\n",
      "token: XYB, predicted: XYA\n",
      "token: XYC, predicted: XYA\n",
      "token: XYE, predicted: XYA\n",
      "token: XYH, predicted: XYA\n",
      "token: XYK, predicted: XYA\n",
      "token: XYM, predicted: XYA\n",
      "token: XYO, predicted: XYA\n",
      "token: XYP, predicted: XYA\n",
      "token: XYT, predicted: XYA\n",
      "token: XYX, predicted: XYA\n",
      "token: XYY, predicted: XYA\n",
      "token: YAB, predicted: YAA\n",
      "token: YAC, predicted: YAA\n",
      "token: YAE, predicted: YAA\n",
      "token: YAH, predicted: YAA\n",
      "token: YAK, predicted: YAA\n",
      "token: YAM, predicted: YAA\n",
      "token: YAO, predicted: YAA\n",
      "token: YAP, predicted: YAA\n",
      "token: YAT, predicted: YAA\n",
      "token: YAX, predicted: YAA\n",
      "token: YAY, predicted: YAA\n",
      "token: YBB, predicted: YBA\n",
      "token: YBC, predicted: YBA\n",
      "token: YBE, predicted: YBA\n",
      "token: YBH, predicted: YBA\n",
      "token: YBK, predicted: YBA\n",
      "token: YBM, predicted: YBA\n",
      "token: YBO, predicted: YBA\n",
      "token: YBP, predicted: YBA\n",
      "token: YBT, predicted: YBA\n",
      "token: YBX, predicted: YBA\n",
      "token: YBY, predicted: YBA\n",
      "token: YCB, predicted: YCA\n",
      "token: YCC, predicted: YCA\n",
      "token: YCE, predicted: YCA\n",
      "token: YCH, predicted: YCA\n",
      "token: YCK, predicted: YCA\n",
      "token: YCM, predicted: YCA\n",
      "token: YCO, predicted: YCA\n",
      "token: YCP, predicted: YCA\n",
      "token: YCT, predicted: YCA\n",
      "token: YCX, predicted: YCA\n",
      "token: YCY, predicted: YCA\n",
      "token: YEB, predicted: YEA\n",
      "token: YEC, predicted: YEA\n",
      "token: YEE, predicted: YEA\n",
      "token: YEH, predicted: YEA\n",
      "token: YEK, predicted: YEA\n",
      "token: YEM, predicted: YEA\n",
      "token: YEO, predicted: YEA\n",
      "token: YEP, predicted: YEA\n",
      "token: YET, predicted: YEA\n",
      "token: YEX, predicted: YEA\n",
      "token: YEY, predicted: YEA\n",
      "token: YHB, predicted: YHA\n",
      "token: YHC, predicted: YHA\n",
      "token: YHE, predicted: YHA\n",
      "token: YHH, predicted: YHA\n",
      "token: YHK, predicted: YHA\n",
      "token: YHM, predicted: YHA\n",
      "token: YHO, predicted: YHA\n",
      "token: YHP, predicted: YHA\n",
      "token: YHT, predicted: YHA\n",
      "token: YHX, predicted: YHA\n",
      "token: YHY, predicted: YHA\n",
      "token: YKB, predicted: YKA\n",
      "token: YKC, predicted: YKA\n",
      "token: YKE, predicted: YKA\n",
      "token: YKH, predicted: YKA\n",
      "token: YKK, predicted: YKA\n",
      "token: YKM, predicted: YKA\n",
      "token: YKO, predicted: YKA\n",
      "token: YKP, predicted: YKA\n",
      "token: YKT, predicted: YKA\n",
      "token: YKX, predicted: YKA\n",
      "token: YKY, predicted: YKA\n",
      "token: YMB, predicted: YMA\n",
      "token: YMC, predicted: YMA\n",
      "token: YME, predicted: YMA\n",
      "token: YMH, predicted: YMA\n",
      "token: YMK, predicted: YMA\n",
      "token: YMM, predicted: YMA\n",
      "token: YMO, predicted: YMA\n",
      "token: YMP, predicted: YMA\n",
      "token: YMT, predicted: YMA\n",
      "token: YMX, predicted: YMA\n",
      "token: YMY, predicted: YMA\n",
      "token: YOB, predicted: YOA\n",
      "token: YOC, predicted: YOA\n",
      "token: YOE, predicted: YOA\n",
      "token: YOH, predicted: YOA\n",
      "token: YOK, predicted: YOA\n",
      "token: YOM, predicted: YOA\n",
      "token: YOO, predicted: YOA\n",
      "token: YOP, predicted: YOA\n",
      "token: YOT, predicted: YOA\n",
      "token: YOX, predicted: YOA\n",
      "token: YOY, predicted: YOA\n",
      "token: YPB, predicted: YPA\n",
      "token: YPC, predicted: YPA\n",
      "token: YPE, predicted: YPA\n",
      "token: YPH, predicted: YPA\n",
      "token: YPK, predicted: YPA\n",
      "token: YPM, predicted: YPA\n",
      "token: YPO, predicted: YPA\n",
      "token: YPP, predicted: YPA\n",
      "token: YPT, predicted: YPA\n",
      "token: YPX, predicted: YPA\n",
      "token: YPY, predicted: YPA\n",
      "token: YTB, predicted: YTA\n",
      "token: YTC, predicted: YTA\n",
      "token: YTE, predicted: YTA\n",
      "token: YTH, predicted: YTA\n",
      "token: YTK, predicted: YTA\n",
      "token: YTM, predicted: YTA\n",
      "token: YTO, predicted: YTA\n",
      "token: YTP, predicted: YTA\n",
      "token: YTT, predicted: YTA\n",
      "token: YTX, predicted: YTA\n",
      "token: YTY, predicted: YTA\n",
      "token: YXB, predicted: YXA\n",
      "token: YXC, predicted: YXA\n",
      "token: YXE, predicted: YXA\n",
      "token: YXH, predicted: YXA\n",
      "token: YXK, predicted: YXA\n",
      "token: YXM, predicted: YXA\n",
      "token: YXO, predicted: YXA\n",
      "token: YXP, predicted: YXA\n",
      "token: YXT, predicted: YXA\n",
      "token: YXX, predicted: YXA\n",
      "token: YXY, predicted: YXA\n",
      "token: YYB, predicted: YYA\n",
      "token: YYC, predicted: YYA\n",
      "token: YYE, predicted: YYA\n",
      "token: YYH, predicted: YYA\n",
      "token: YYK, predicted: YYA\n",
      "token: YYM, predicted: YYA\n",
      "token: YYO, predicted: YYA\n",
      "token: YYP, predicted: YYA\n",
      "token: YYT, predicted: YYA\n",
      "token: YYX, predicted: YYA\n",
      "token: YYY, predicted: YYA\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "idx2char = {v: k for k, v in char2idx.items()}\n",
    "wrong_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for token in unique_series_sorted:\n",
    "        input_tensor = torch.tensor([[char2idx[char] for char in token]], dtype=torch.float32)\n",
    "\n",
    "        output = model(input_tensor)\n",
    "        predicted_idxs = torch.round(output).squeeze(0).detach().numpy()\n",
    "        predicted_token = ''.join(idx2char[i] for i in predicted_idxs)\n",
    "\n",
    "        if predicted_token != token:\n",
    "            print(f'token: {token}, predicted: {predicted_token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "75d896fd-c2cd-49ec-a309-a850ab555199",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x1 and 3x8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m true_idx \u001b[38;5;241m=\u001b[39m series2idx[token]\n\u001b[1;32m      9\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[true_idx]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)  \u001b[38;5;66;03m# shape: [1, 1]\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m output \u001b[38;5;241m=\u001b[39m model(input_tensor)\n\u001b[1;32m     12\u001b[0m predicted_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mround(output)\u001b[38;5;241m.\u001b[39mitem())  \u001b[38;5;66;03m# round to nearest index\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Clamp to valid index range\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[92], line 32\u001b[0m, in \u001b[0;36mAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 32\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(out)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x1 and 3x8)"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "wrong_predictions = []\n",
    "\n",
    "idx2series = {v: k for k, v in series2idx.items()}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for token in unique_series_sorted:\n",
    "        true_idx = series2idx[token]\n",
    "        input_tensor = torch.tensor([[true_idx]], dtype=torch.float32)  # shape: [1, 1]\n",
    "\n",
    "        output = model(input_tensor)\n",
    "        predicted_idx = int(torch.round(output).item())  # round to nearest index\n",
    "\n",
    "        # Clamp to valid index range\n",
    "        predicted_idx = max(0, min(predicted_idx, len(idx2series) - 1))\n",
    "\n",
    "        if predicted_idx != true_idx:\n",
    "            predicted_token = idx2series[predicted_idx]\n",
    "            wrong_predictions.append((token, predicted_token))\n",
    "\n",
    "# Show results\n",
    "print(f\"Total wrong predictions: {len(wrong_predictions)} / {len(unique_series_sorted)}\\n\")\n",
    "for true_token, predicted_token in wrong_predictions:\n",
    "    print(f\"{true_token} âžœ {predicted_token}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "e3abde91-d5a7-4e8d-9c32-ad47c49f0a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    input_tensor = torch.tensor([[series2idx['AAC']]], dtype=torch.float32)\n",
    "    \n",
    "    out = model(input_tensor)\n",
    "    print(torch.round(out).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "25ce96d2-126c-4bbf-8843-c31dfa83126c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9967]])"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
